{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/group-nai-shomu/00/blob/main/11%E5%9F%BA%E7%A4%8E.ipynb","timestamp":1682401647647}],"collapsed_sections":["lgDoUdSfVBGQ","PjWIMiPcVEJk","QedU-gffVIRg","jAWYBz5MVYrr","VrdcfiD5VhO6","tCuh92x88A3k"],"toc_visible":true,"authorship_tag":"ABX9TyPrMvdv/VzuInCE1QiybdyC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"Ak93hq4rkDAA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <font color=\"Silver\">Python│slice\n","https://numpy.org/devdocs/user/absolute_beginners.html\n","\n","<img src=\"https://numpy.org/doc/stable/_images/np_indexing.png\" width=\"800\">\n","<img src=\"https://numpy.org/doc/stable/_images/np_matrix_indexing.png\" width=\"800\">"],"metadata":{"id":"Kv0GbORajwp0"}},{"cell_type":"markdown","source":["# <font color=\"Silver\">Python│range\n"," - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://techbeamers.com/python-range-function/)</font></font><br>\n","<img src=\"https://techbeamers.com/wp-content/uploads/2019/05/Python-range-function-explained.png\" width=\"320\">"],"metadata":{"id":"8wH0tdfKMWyY"}},{"cell_type":"markdown","source":["# <font color=\"Silver\">Python│np.random\n","rand() <font color=\"silver\"> # 0〜1の乱数を1個生成</font><br>\n","rand(100) <font color=\"silver\"> # 0〜1の乱数を100個生成</font><br>\n","rand(10,10) <font color=\"silver\"> # 0〜1の乱数で 10x10 の行列を生成</font><br>\n","self.mask = np.random.rand(x.shape) > self. dropout_ratio <font color=\"silver\"># Dropout</font><br>\n","<br>\n","randn() <font color=\"silver\"># 標準正規分布 (平均0, 標準偏差1)</font><br>\n","randn(10) <font color=\"silver\"># 標準正規分布を10個生成</font><br>\n","randn(10,10) <font color=\"silver\"># 標準正規分布による 10x10 の行列</font><br>\n","<br>\n","randint(100)) <font color=\"silver\">#  0〜99 の整数を1個生成</font><br>\n","randint(30,70)) <font color=\"silver\"># 30〜69 の整数を1個生成</font><br>\n","randint(0,100,20)) <font color=\"silver\">#  0〜99 の整数を20個生成</font><br>\n","randint(0,100,(5,5))) <font color=\"silver\">#  0〜99 の整数で5x5の行列を生成</font><br>\n","<br>\n","np.random.permutation(5) <font color=\"silver\"># array([3, 1, 2, 4, 0])</font><br>\n","np.random.choice(a, size=None, replace=True, p=None)<br>\n","np.random.choice(5, 3) <font color=\"silver\"> # array([3, 3, 2])</font><br>\n","<font color=\"silver\">重複を許すサンプリング（復元抽出）３つ抽出、それぞれの範囲は[0, 5)</font><br>\n","np.random.random_sample() <font color=\"silver\"># 0.1322630747589819 連続一様分布から0.0以上1.0未満の乱数配列を生成</font><br> \n","np.random.random_sample(3) <font color=\"silver\"># array([0.8892175 , 0.86287021, 0.15078942])</font><br>\n","<br>\n","np.unique(y, return_inverse=True)<br>\n","<font color=\"silver\">labels, y_labels = np.unique(y, return_inverse=True)<br>\n","y = [\"good\",\"good\",\"good\",\"good\",\"bad\",\"bad\"]<br>\n","labels  # array(['bad', 'good'], dtype='<U4')<br>\n","y_labels  # array([1, 1, 1, 1, 0, 0])</font><br><br>\n","l, v = np.linalg.eig(cov)<br>\n","<font color=\"silver\">l：covの固有値。l.shape：(次元数,)<br>\n","v：covの固有ベクトル。v.shape：(次元数, 次元数)<br>\n","lの第0次元とvの第1次元は固有ベクトルの本数を意味する<br>\n","入力データとvの各列の内積を取ることで、新たな変数が生成される<br>"],"metadata":{"id":"wtL7aXVcjkRu"}},{"cell_type":"markdown","source":["# <font color=\"Silver\">Python│torch.squeeze\n","torch.unsqueeze(input, dim)<br>\n","torch.squeeze(input, dim=None)<br>\n","\n","Tensor.squeeze()は「テンソル中の要素数1の次元を削除する」、 Tensor.unsqueeze(d: int)は「テンソルのd階目に要素数1の次元を挿入する」 \n","\n"],"metadata":{"id":"_9UXEO-jAD5v"}},{"cell_type":"markdown","source":["# <font color=\"silver\">表現 [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!6356&cid=b0f01606242a6ed3&CT=1672154080396&OR=ItemsView)</font>"],"metadata":{"id":"Pu3z7dBDqnuq"}},{"cell_type":"markdown","source":["# <font color=\"silver\">表現│万能近似定理 \n","<font color=\"silver\"> \n","万能近似定理, Universal Approximation Theorem</font>\n","> <font color=\"Blue\">近似可能性</font><br>\n","> 　ネットワークが十分な数の隠れユニットを持つ場合、線形の出力層とシグモイド関数のような「押しつぶす」活性化関数をもつ隠れ層が少なくとも1つ含まれる順伝播型ネットワークは、どんなボレル可測関数でも任意の精度で近似できる。<br>\n","> 　つまり、ノードを増やしていくと、ニューラルネットワークの表現力はどんどん上がっていき、学習データをほぼ完全に説明できるニューラルネットワークが実現できる、ということ。<br>\n","> <font color=\"Blue\">学習性は述べていない</font><br>\n","> 　学習データをほぼ完全に説明できるモデルが実現できたとしても、そのモデルの汎化性能が必ずしも高いわけではないことに注意<br><br>\n","> ボレル可測関数：<font color=\"silver\"> R/の有界で閉じた部分集合上の任意の連続関数。"],"metadata":{"id":"yck-1J0_JIt4"}},{"cell_type":"markdown","source":["# <font color=\"silver\">表現│表現学習\n","<font color=\"silver\">表現学習とは、</font>\n","><font color=\"Blue\">$\\tiny{\\rm Link}$ [<font color=\"Blue\">…</font>](https://blog.recruit.co.jp/data/articles/ssl_vision_01/#%E8%A1%A8%E7%8F%BE%E5%AD%A6%E7%BF%92)  [<font color=\"Blue\">…</font>](https://qiita.com/saliton/items/2f7b1bfb451df75a286f#%EF%BC%92%E8%A1%A8%E7%8F%BE%E5%AD%A6%E7%BF%92representation-learning)"],"metadata":{"id":"YwaasZEGIX_z"}},{"cell_type":"markdown","source":["# <font color=\"silver\">表現│自己教師あり学習と対照学習\n","<font color=\"silver\">自己教師あり学習, SSL, Self-Supervised Learning, 対照学習, Contrastive Learning</font><br>\n","><font color=\"Blue\">$\\tiny{\\rm Link}$ [<font color=\"Blue\">…</font>](https://qiita.com/saliton/items/2f7b1bfb451df75a286f#%EF%BC%93%E8%87%AA%E5%B7%B1%E6%95%99%E5%B8%AB%E3%81%82%E3%82%8A%E5%AD%A6%E7%BF%92self-supervised-learning)  [<font color=\"Blue\">…</font>](https://blog.recruit.co.jp/data/articles/ssl_vision_01/#%E8%87%AA%E5%B7%B1%E6%95%99%E5%B8%AB%E3%81%82%E3%82%8A%E5%AD%A6%E7%BF%92)</font><br>"],"metadata":{"id":"YJRERJlcQQrA"}},{"cell_type":"markdown","source":["# <font color=\"silver\">表現│半教師あり学習\n","<font color=\"silver\"> 半教師あり学習, SSL, Semi-Supervised Learning</font><br>\n","><font color=\"Blue\">$\\tiny{\\rm Link}$ [<font color=\"Blue\">…</font>](https://note.com/akira_tosei/n/n3ff2ad70562b) [<font color=\"Blue\">…</font>](https://www.skillupai.com/blog/tech/semi-supervised-learning/)</font><br><br>\n","> <font color=\"Blue\">Consistency Regularization</font><br>\n","> <font color=\"Blue\">Entropy Minimization</font><br><br>\n",">  　ラベルなしデータに対する出力が一貫性を持つようにネットワークを学習する。一貫性とは，同一の画像に対してノイズの付与や幾何学変換の適用などによって出力が変化しないことを表す。<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/11_cnn_pytorch/13_semi_supervised_learning.ipynb#scrollTo=tPyzEOSCUTI1)</font></font><br>\n","<img src=\"https://dl.dropboxusercontent.com/s/40zfvwq2eksud9k/CR.png\" width=\"640\"><br>\n","> <img src=\"https://assets.st-note.com/production/uploads/images/35466070/picture_pc_26f00561548e3d13a24c567ce9955f8d.png?width=800\" width=\"480\"><br><br>\n","> <img src=\"https://camo.qiitausercontent.com/0b473d134bd4b20941e1985612c23ce906ce90b6/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3832313135382f33333430363836322d373137362d616231342d663332652d6565343065363462396239612e706e67\" width=\"640\">"],"metadata":{"id":"gAYwZ35gIYXD"}},{"cell_type":"markdown","source":["# <font color=\"silver\">表現│弱教師あり学習\n","<font color=\"silver\"> 弱教師あり学習, Weakly Supervised Learning</font><br>"],"metadata":{"id":"ekRuXovNrTqv"}},{"cell_type":"markdown","source":["# <font color=\"silver\">表現│転移学習 \n"],"metadata":{"id":"fat9XWjZTkVb"}},{"cell_type":"markdown","source":["# <font color=\"silver\">表現│模倣学習</font>"],"metadata":{"id":"opGKYROlSbE9"}},{"cell_type":"markdown","source":["# <font color=\"silver\">表現│マルチタスク学習</font>\n","> - <font color=\"silver\">Description</font><br>\n"," - マルチタスク学習\n","   - タスク間で特徴量を共有しながら学習を進める方法のことであり、 これによって、タスク間に共通するドメイン特有の情報を活用することがで き 汎化性能向上が期待される\n","   - 特徴量をタスク間で共有するため、 ラベル付きデータが少ないタスクの場合 でも、他方のタスクでラベル付きデータが十分に得られるのであれば、高い性能を示すモデルを獲得できる可能性がある。 \n","   - Fast R-CNNは、分類とバウンディングボックス回帰の2つのタスクを同時に学 習しているため、マルチタスク学習と考えることができる。 <br>\n","   - 正則化の効果が期待できる。\n","   - タスクを解くために有用な情報が類似していると仮定した上で, 複数のタスクを同時に解くことで汎化性能の向上を狙う\n","   - 転移学習・ファインチューニングではモデルの一部を再利用するが、マルチタスク学習ではモデルの一部がタスク間で共有される。\n","   - あるひとつのタスクから別のタスクに向かって一方向に情報の転移を行う転移学習とは異なり,マルチタスク学習は複数のタスク間で相互に情報を共有することが可能である.\n","   - いくつかのタスクから生じる事例を貯めることで汎化性能を向上させられる"],"metadata":{"id":"wR-D7NbhXibd"}},{"cell_type":"markdown","source":["# <font color=\"silver\">表現│教師あり事前学習</font>"],"metadata":{"id":"lulQLJBFUgya"}},{"cell_type":"markdown","source":["# <font color=\"silver\">表現│N-shot Learning </font>\n","> - GPT-2 や GPT-3 は、ファインチューニングを必要としない学習が可能（One-Shot Learning, Zero-Shot Learning, Few-Shot Learning）\n","> - 学習データを一つも示さずどんなタスクを解くかを示すことでモデルにタスクを解かせることをZero-Shot Learningという\n","> - 翻訳したいペアを与えずに翻訳する言語をまたいだ翻訳をゼロショット学習"],"metadata":{"id":"UvIX1nvTShP2"}},{"cell_type":"markdown","source":["# <font color=\"Silver\">表現│メタ学習 \n","<font color=\"Silver\">メタ学習, Meta-Learning</font><br>\n","><font color=\"Blue\">$\\tiny{\\rm Link}$ [<font color=\"Blue\">…</font>](https://recruit.gmo.jp/engineer/jisedai/blog/meta-learning/) [<font color=\"Blue\">…</font>](https://drive.google.com/drive/folders/1wHf2oaMWUfN6WtpMLAyhiUOvou_L-R8p)"],"metadata":{"id":"Ped6MbDBYdIM"}},{"cell_type":"markdown","source":["# <font color=\"Silver\">表現│XAI\n"," <font color=\"Silver\"> Explainable AI, XAI, 説明可能な AI</font><br>\n","> <font color=\"Blue\">局所的な説明</font><br>\n","> Local explanation, Prediction explanation <br>\n","> モデルの中身ではなく、データ毎の予測の根拠を知りたい<br>\n","> あるひとつのデータに着目し、そのデータの予測結果に寄与する特徴量やその予測結果に関連が深い学習データなどを提示する方法のこと → LIME, SHAP<br>\n","<br>\n","> <font color=\"Blue\">大域的な説明</font><br>\n","> Global explanation, Model explanation<br>\n","> モデルの中身やデータ全体の傾向を解釈したい<br>\n","> ニューラルネットワークなどの複雑なモデルを決定木や線形モデルなどの解釈性の良いモデルで近似し、モデルの予測過程を提示する方法のこと → 一般化線形回帰モデル・決定木・Feature Importance・Partial Dependence・感応度分析など<br>\n"],"metadata":{"id":"jA-vUD2nTWFa"}},{"cell_type":"markdown","source":["# <font color=\"silver\">表現│パブニックの原理</font>"],"metadata":{"id":"hT34Q9lWb-cC"}},{"cell_type":"markdown","source":["# <font color=\"silver\">表現│appendix, 比較</font>\n","> <font color=\"Blue\">半教師あり学習と自己教師あり学習</font><br>\n",">  　半教師あり学習も自己教師あり学習も、ラベル付きデータが十分用意できない環境でも学習できる手法。ただし、半教師あり学習は少量のラベル付きデータが必要である一方、自己教師あり学習はデータの基礎構造の不足を補う予測学習であり、教師データが必須ではない。<br>"],"metadata":{"id":"af3fP5H-sj5M"}},{"cell_type":"markdown","source":["# <font color=\"silver\">線形代数 [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!5726&cid=b0f01606242a6ed3&CT=1670903086431&OR=ItemsView)</font>\n","\n"],"metadata":{"id":"-xYj4hIqlhUd"}},{"cell_type":"markdown","source":["# <font color=\"silver\">線形代数│固有値分解</font>\n",">- 半正定値行列の固有値は非負である<br><br>\n","> - 対角化･固有値分解･特異値分解<br><br>\n","$P^{-1}AP=Λ \\qquad A=PΛP^{-1}\\qquad A=UΣV^{T}$<br><br>\n","> - 固有値の求め方<br><br>\n","$ \\begin{align}det({A}-\\lambda{I})&=0\\\\\\\\\n","\\left[ \\begin{array}{cc} 1 & 4 \\\\ 2 & 3 \\end{array} \\right]-\\lambda \\left[ \\begin{array}{cc}  1 & 0 \\\\ 0 & 1 \\end{array} \\right]&=0\\\\\\\\\n","\\left[ \\begin{array}{cc} 1-\\lambda & 4 \\\\ 2 & 3-\\lambda \\end{array} \\right]&=0\\\\\\\\\n","(1-\\lambda)(3-\\lambda)-4×2 &=0\\\\\\\\\n","\\end{align}$<br>\n","> - 固有ベクトルの求め方<br><br>\n","$ \\begin{align}\\left[ \\begin{array}{cc} 1 & 4 \\\\ 2 & 3 \\end{array} \\right]\\left[ \\begin{array}{cc}  x \\\\ y \\end{array} \\right]&=5 \\left[ \\begin{array}{cc}  x \\\\ y \\end{array} \\right]\\end{align}$<br><br>\n","$ \\begin{cases} 1x+4y = -x \\\\ 2x+3y = -y \\\\ \\end{cases} \\rightarrow \\begin{cases} 2x+4y = 0 \\\\ 2x+4y = 0 \\\\ \\end{cases}$<br><br>\n",">   - <font color=\"silver\">Description</font><br>\n",">    -   固有値・固有ベクトルの定義<br>\n",">      -   $n$次正方行列${A}$に対し、<br>\n","スカラー$λ$と零ベクトルでない$n$項ベクトル${u}≠0$が、<br>\n","${A}{u}=\\lambda {u}$<br>\n","を満たすとき$λ$を${A}$の固有値といい、<br>\n","${u}$を${A}$の固有値$λ$に関する固有ベクトルという<br><br>\n",">    -   固有値分解の定義<br>\n",">      -   $n$次正方行列${A}$が固有値$λ$と固有ベクトル${u}$を持つとき、<br><br>\n","${A}={P}{\\Lambda}{P}^{-1}$<br><br>\n","と変形すること固有値分解という<br>\n","ただし、<br><br>\n","${\\Lambda}=\\begin{pmatrix} \\lambda_{1} & 0 & \\dots & 0 \\\\0 & \\lambda_{2} & \\dots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\0 & 0 & \\dots & \\lambda_{n} \\end{pmatrix}\\quad{P}=({u}_1{u}_2\\dots{u}_n)$<br><br>\n","である<br><br>\n",">    -   特異値・特異ベクトルの定義<br>\n",">      -   任意の零行列ではない$m×n$行列$\\pmb{A}$に対して、<br>\n","$\\pmb{A}\\pmb{v}=\\sigma\\pmb{u}$<br>\n","$\\pmb{A}^\\top\\pmb{u}=\\sigma\\pmb{v}$<br>\n","を満たすような正の数$σ$を特異値<br>\n","$m$次元ベクトル$\\pmb{u}$を左特異ベクトル<br>\n","$n$次元ベクトル$\\pmb{v}$を右特異ベクトルという<br><br>\n",">    -   特異値分解の定義<br>\n",">      -  $m×n$行列$\\pmb{A}(m≤n)$が<br>\n","特異値$σ$と<br>\n","左特異ベクトル$\\pmb{u}$と<br>\n","右特異ベクトル$\\pmb{v}$を持つとき、<br>\n","$\\pmb{A}=\\pmb{U}\\pmb{\\Sigma}\\pmb{V}^\\top$<br>\n","と変形すること特異値分解という<br>\n","ただし、<br><br>\n","${\\pmb{\\Sigma}=(diag(σ1,σ2,...,σm)|0m×(n-m))\\\\\n","\\;\\;=\\left(\\begin{array}{cccc|ccc}\n","σ_{1} & 0 & \\dots & 0 & 0 & \\dots & 0\\\\\n","0 & σ_{2} & \\dots & 0 & 0 & \\dots & 0\\\\\n","\\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n","0 & 0 & \\dots & σ_{m} & 0 & \\dots & 0\n","\\end{array}\\right)\n","}\\quad$<br><br>\n","の対角行列、<br><br>\n","${\\pmb{U}=(\\pmb{u}_{1}\\pmb{u}_{2}…\\pmb{u}_{m})}\\quad$<br><br>\n","の直交行列、<br><br>\n","${\\pmb{V}^\\top=\\begin{pmatrix}\\pmb{v}_{1}\\\\\\pmb{v}_{2}\\\\\\vdots\\\\\\pmb{v}_{n}\\end{pmatrix}}\\quad$<br><br>\n","の直交行列の転置行列、<br>\n","である。<br><br>\n","<img src=\"https://camo.qiitausercontent.com/aea25f6cb0dcc6ca021e54a3d75ddd7e5c6c44d0/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3434383530332f61663636663561662d356236362d656463332d626434372d3564306139353864643062362e706e67\" width=\"320\">"],"metadata":{"id":"E8yi_g7FgdXN"}},{"cell_type":"markdown","source":["# <font color=\"silver\">線形代数│固有値分解の例題</font>\n","> - 例題<br>\n",">   - $ {A} = \\begin{bmatrix} 1 & 4 \\\\ 2 & 3  \\end{bmatrix} $\n","<br><br>\n",">   - 固有値$\\lambda$を特性方程式$det({A}-\\lambda{I})=0$で求める</font><br><br>\n","$ \\begin{align}det({A}-\\lambda{I})&=0\\\\\\\\\n","\\left[ \\begin{array}{cc} 1 & 4 \\\\ 2 & 3 \\end{array} \\right]-\\lambda \\left[ \\begin{array}{cc}  1 & 0 \\\\ 0 & 1 \\end{array} \\right]&=0\\\\\\\\\n","\\left[ \\begin{array}{cc} 1-\\lambda & 4 \\\\ 2 & 3-\\lambda \\end{array} \\right]&=0\\\\\\\\\n","(1-\\lambda)(3-\\lambda)-4×2 &=0\\\\\\\\\n","3-4\\lambda+\\lambda^2 - 8 &=0\\\\\\\\\n","\\lambda^2-4\\lambda-5 &= 0\\\\\\\\\n","(\\lambda+1)(\\lambda-5) &=0\n","\\end{align}$\n","<br><br>\n","$ {\\Lambda} = \\begin{bmatrix} 5 & 0 \\\\ 0 & -1  \\end{bmatrix}$<br><br>\n",">   -固有値$\\lambda=5$の時の固有ベクトルを求める</font><br><br>\n","$ \\begin{align}{A}{x}&=\\lambda {x}\\\\\\\\\n","\\left[ \\begin{array}{cc} 1 & 4 \\\\ 2 & 3 \\end{array} \\right]\\left[ \\begin{array}{cc}  x \\\\ y \\end{array} \\right]&=5 \\left[ \\begin{array}{cc}  x \\\\ y \\end{array} \\right]\\\\\\\\\n","\\end{align}$<br><br>\n","$ \\begin{cases} 1x+4y = 5x \\\\ 2x+3y = 5y \\\\ \\end{cases} \\rightarrow \\begin{cases} -4x+4y = 0 \\\\ 2x-2y = 0 \\\\ \\end{cases}$<br><br>\n","$ \\begin{bmatrix} x \\\\ y\\end{bmatrix}=\\begin{bmatrix} 1 \\\\ 1\\end{bmatrix}\\begin{bmatrix} x \\\\ y\\end{bmatrix}$<br><br>\n",">   - 固有値$\\lambda=-1$の時の固有ベクトルを求める</font><br><br>\n","$ \\begin{align}{A}{x}&=\\lambda {x}\\\\\\\\\n","\\left[ \\begin{array}{cc} 1 & 4 \\\\ 2 & 3 \\end{array} \\right]\\left[ \\begin{array}{cc}  x \\\\ y \\end{array} \\right]&=-1 \\left[ \\begin{array}{cc}  x \\\\ y \\end{array} \\right]\\\\\\\\\n","\\end{align}$<br><br>\n","$ \\begin{cases} 1x+4y = -x \\\\ 2x+3y = -y \\\\ \\end{cases} \\rightarrow \\begin{cases} 2x+4y = 0 \\\\ 2x+4y = 0 \\\\ \\end{cases}$<br><br>\n","$ \\begin{bmatrix} x \\\\ y\\end{bmatrix}=\\begin{bmatrix} 1 \\\\ -2\\end{bmatrix}\\begin{bmatrix} x \\\\ y\\end{bmatrix}$<br><br>\n",">   - 固有値分解をする</font>\n","$ {A} = \\begin{bmatrix} 1 & 4 \\\\ 2 & 3  \\end{bmatrix}$<br><br>\n","$ {\\Lambda} = \\begin{bmatrix} 5 & 0 \\\\ 0 & -1  \\end{bmatrix}$<br><br>\n","$ {P}={ \\begin{bmatrix} 1 & 1\\\\ 1 & -2 \\end{bmatrix}}$<br><br>\n","$ {A}={P}{\\Lambda}{P}^{-1}$<br><br>\n","$ \\begin{eqnarray} { \\begin{bmatrix} 1 & 4 \\\\ 2 & 3 \\end{bmatrix}} = \n","{ \\begin{bmatrix} 1 & -\\frac{1}{\\sqrt{2}}\\\\ 0 & \\frac{1}{\\sqrt{2}} \\end{bmatrix}} \n","{ \\begin{bmatrix} 5 & 0 \\\\ 0 & -1 \\end{bmatrix}} \n","{ \\begin{bmatrix} 1 & 1 \\\\ 0 & \\sqrt{2} \\end{bmatrix}} \\end{eqnarray}$"],"metadata":{"id":"fmip72NdPKb5"}},{"cell_type":"markdown","source":["# <font color=\"silver\">線形代数│特性方程式の例題</font>\n","> - $ {A} = \\begin{bmatrix} 1 & 4 \\\\ 2 & 3  \\end{bmatrix} $\n","<br><br>\n","$ \\begin{align}\\left[ \\begin{array}{cc} 1 & 4 \\\\ 2 & 3 \\end{array} \\right]-\\lambda \\left[ \\begin{array}{cc}  1 & 0 \\\\ 0 & 1 \\end{array} \\right]&=0\\\\\\\\\n","\\left[ \\begin{array}{cc} 1-\\lambda & 4 \\\\ 2 & 3-\\lambda \\end{array} \\right]&=0\\\\\\\\\n","(1-\\lambda)(3-\\lambda)-4×2 &=0\\\\\n","3-4\\lambda+\\lambda^2 - 8 &=0\\\\\n","\\lambda^2-4\\lambda-5 &= 0\\\\\n","(\\lambda+1)(\\lambda-5) &=0\n","\\end{align}$\n","<br><br>\n","$ {\\Lambda} = \\begin{bmatrix} 5 & 0 \\\\ 0 & -1  \\end{bmatrix}$<br><br><br>\n","> - $ {A} = \\begin{bmatrix} 3 & 4 \\\\ 4 & 3  \\end{bmatrix} $\n","<br><br>\n","$ \\begin{align}\\left[ \\begin{array}{cc} 3 & 4 \\\\ 4 & 3 \\end{array} \\right]-\\lambda \\left[ \\begin{array}{cc}  1 & 0 \\\\ 0 & 1 \\end{array} \\right]&=0\\\\\\\\\n","\\left[ \\begin{array}{cc} 3-\\lambda & 4 \\\\ 4 & 3-\\lambda \\end{array} \\right]&=0\\\\\\\\\n","(3-\\lambda)(3-\\lambda)-4×4 &=0\\\\\n","9-6\\lambda+\\lambda^2 - 16 &=0\\\\\n","\\lambda^2-6\\lambda-7 &= 0\\\\\n","(\\lambda+1)(\\lambda-7) &=0\n","\\end{align}$\n","<br><br>\n","$ {\\Lambda} = \\begin{bmatrix} 7 & 0 \\\\ 0 & -1  \\end{bmatrix}$<br><br>"],"metadata":{"id":"VWeqkFeHxGm7"}},{"cell_type":"markdown","source":["# <font color=\"silver\">線形代数│固有ベクトルの例題</font>\n",">   - 固有値$\\lambda=5$の時の固有ベクトルを求める</font><br><br>\n","$ \\begin{align}\\left[ \\begin{array}{cc} 1 & 4 \\\\ 2 & 3 \\end{array} \\right]\\left[ \\begin{array}{cc}  x \\\\ y \\end{array} \\right]&=5 \\left[ \\begin{array}{cc}  x \\\\ y \\end{array} \\right]\\end{align}$<br><br>\n","$ \\begin{cases} 1x+4y = 5x \\\\ 2x+3y = 5y \\\\ \\end{cases} \\rightarrow \\begin{cases} -4x+4y = 0 \\\\ 2x-2y = 0 \\\\ \\end{cases}$<br><br>\n","$ \\begin{bmatrix} x \\\\ y\\end{bmatrix}=\\begin{bmatrix} 1 \\\\ 1\\end{bmatrix}\\begin{bmatrix} x \\\\ y\\end{bmatrix}$<br><br>\n",">   - 固有値$\\lambda=-1$の時の固有ベクトルを求める</font><br><br>\n","$ \\begin{align}\\left[ \\begin{array}{cc} 1 & 4 \\\\ 2 & 3 \\end{array} \\right]\\left[ \\begin{array}{cc}  x \\\\ y \\end{array} \\right]&=-1 \\left[ \\begin{array}{cc}  x \\\\ y \\end{array} \\right]\\end{align}$<br><br>\n","$ \\begin{cases} 1x+4y = -x \\\\ 2x+3y = -y \\\\ \\end{cases} \\rightarrow \\begin{cases} 2x+4y = 0 \\\\ 2x+4y = 0 \\\\ \\end{cases}$<br><br>\n","$ \\begin{bmatrix} x \\\\ y\\end{bmatrix}=\\begin{bmatrix} 1 \\\\ -2\\end{bmatrix}\\begin{bmatrix} x \\\\ y\\end{bmatrix}$<br><br>\n",">   - 固有値$\\lambda=7$の時の固有ベクトルを求める</font><br><br>\n","$ \\begin{align}\\left[ \\begin{array}{cc} 3 & 4 \\\\ 4 & 3 \\end{array} \\right]\\left[ \\begin{array}{cc}  x \\\\ y \\end{array} \\right]&=7 \\left[ \\begin{array}{cc}  x \\\\ y \\end{array} \\right]\\end{align}$<br><br>\n","$ \\begin{cases} 3x+4y = 7x \\\\ 4x+3y = 7y \\\\ \\end{cases} \\rightarrow \\begin{cases} -4x+4y = 0 \\\\ 4x-4y = 0 \\\\ \\end{cases}$<br><br>\n","$ \\begin{bmatrix} x \\\\ y\\end{bmatrix}=\\begin{bmatrix} 1 \\\\ 1\\end{bmatrix}\\begin{bmatrix} x \\\\ y\\end{bmatrix}$<br><br>\n",">   - 固有値$\\lambda=-1$の時の固有ベクトルを求める</font><br><br>\n","$ \\begin{align}\\left[ \\begin{array}{cc} 3 & 4 \\\\ 4 & 3 \\end{array} \\right]\\left[ \\begin{array}{cc}  x \\\\ y \\end{array} \\right]&=-1 \\left[ \\begin{array}{cc}  x \\\\ y \\end{array} \\right]\\end{align}$<br><br>\n","$ \\begin{cases} 3x+4y = -1x \\\\ 4x+3y = -1y \\\\ \\end{cases} \\rightarrow \\begin{cases} 4x+4y = 0 \\\\ 4x+4y = 0 \\\\ \\end{cases}$<br><br>\n","$ \\begin{bmatrix} x \\\\ y\\end{bmatrix}=\\begin{bmatrix} 1 \\\\ -1\\end{bmatrix}\\begin{bmatrix} x \\\\ y\\end{bmatrix}$<br><br>"],"metadata":{"id":"xK3LxlP1zaNo"}},{"cell_type":"markdown","source":["# <font color=\"silver\">線形代数│ベクトルの距離</font>\n","> - <font color=\"silver\">Description</font><br>\n",">  -   ユークリッド距離・マンハッタン距離・ミンコフスキー距離・チェビシェフ距離<br><br>\n",">    -   $ {d(\\pmb{x}, \\pmb{y}) = \\sqrt{\\sum_{i=1}^N (x_i – y_i)^2}} = \\sqrt{\\sum_{i=1}^N (x_i – y_i)^\\top(x_i – y_i)}$\n","<br><br>\n","$ d(\\pmb{x}, \\pmb{y}) = \\sum_{i=1}^N |x_i – y_i|$\n","<br><br>\n","$ d(\\pmb{x}, \\pmb{y}) = \\left(\\sum_{k=1}^n \\left| x_i - y_i \\right|^p \\right)^{\\frac 1 p}$\n","<br><br>\n","$ d(\\pmb{x}, \\pmb{y}) = \\mathop{\\rm max}\\limits_{i}|x_i -y_i|$\n","<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://iq.opengenus.org/euclidean-vs-manhattan-vs-chebyshev-distance/)</font><br>\n","<img src=\"https://iq.opengenus.org/content/images/2018/12/distance.jpg\" height=\"160\"><br><br>\n",">  -   マハラノビス距離</font>\n",">    -   データ集合の相関関係を考慮し、データ集合からの外れ具合・異質さを評価する距離</font><br>\n",">    -   正規分布の共分散の形にあわせて算出する距離<br>\n",">    -   同一確率分布に従うと仮定される2つのベクトルの類似度の指標<br>\n","<br>\n","$ {D(\\pmb{x}, \\pmb{y}) = \\displaystyle\\sqrt{ (x – y)^\\top \\displaystyle\\sum^{-1} (x – y) }}$<br><br>\n",">  -   ノルム<br>\n",">    -   $ \\|\\pmb{x}\\|_1 = \\displaystyle\\sum_{i=1}^{N}|x_i| \\quad $または$ \\quad \\|\\pmb{x}\\|_1=\\pmb{x}\\cdot\\pmb{x}$\n","<br><br>\n","$ {\\|\\pmb{x}\\|_2=\\left(\\displaystyle\\sum_{i=1}^{N}|x_i|^2\\right)^{1/2}}=\\sqrt{\\displaystyle\\sum_{i=1}^{N}x_i^2}$\n","<br><br>\n","$ {\\|\\pmb{x}\\|_\\infty = \\displaystyle\\max_i |x_i|}$\n","<br><br>\n","$ {\\|\\pmb{x}\\|_p=\\left(\\displaystyle\\sum_{i=1}^{N}|x_i|^p\\right)^{1/p}}$"],"metadata":{"id":"e_M-03QFQ3tJ"}},{"cell_type":"markdown","source":["# <font color=\"silver\">確率分布 [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!5733&cid=b0f01606242a6ed3&CT=1670905537863&OR=ItemsView)</font>"],"metadata":{"id":"Go_lBuHPu7H4"}},{"cell_type":"markdown","source":["# <font color=\"silver\">確率分布│期待値分散</font>\n","> <font color=\"silver\">母集団の期待値分散</font> <br><br>\n","$\\mu=\\displaystyle\\frac{1}{N} \\sum_{i=1}^{N} x_{i}\\qquad\\sigma^{2}=\\displaystyle\\frac{1}{N} \\sum_{i=1}^{N}\\left(x_{i}-\\mu\\right)^{2}$<br><br>\n","<font color=\"silver\">標本の期待値分散</font> <br><br>\n","$\\bar{x}=\\displaystyle\\frac{1}{n} \\sum_{i=1}^{n} x_{i}\\qquad s^{2}=\\displaystyle\\frac{1}{n} \\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}$<br><br>\n","<font color=\"silver\">確率分布の期待値</font> <br><br>\n","$E[X]=\\displaystyle\\sum_{x} x p(x)\\qquad V[X]=\\displaystyle\\sum_{x}(x-E[X])^{2} p(x)$<br><br>\n","<font color=\"silver\">分散の性質</font><br><br>\n","$\\mathbb{V}[X]=\\mathbb{E}[(X-\\mathbb{E}[X])^2]$\n","<br><br>\n","$\\mathbb{V}[X]=\\mathbb{E}[X^2]-(\\mathbb{E}[X])^2$\n","<br><br>\n","<font color=\"silver\">共分散</font><br><br>\n","$\\sigma_{xy}=\\cfrac{1}{n}\\displaystyle\\sum_{i=1}^n{(x_i-\\bar{x})(y_i-\\bar{y})}\\qquad \\sigma_{xy}=\\cfrac{1}{n}\\displaystyle\\sum_{i=1}^nx_iy_i-\\bar{x}\\bar{y}$</font>\n","<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qctoranomaki.com/sqc/statistics/covariance/)</font><br>\n","<img src=\"https://qctoranomaki.com/wp-content/uploads/2021/11/f4a1b0aed5dc02442c433030ff24c031-5-768x626.jpg\" height=\"320\">\n","<img src=\"https://qctoranomaki.com/wp-content/uploads/2021/11/51e8fc29fa61692e70bc4c925b602ad0-2.jpg\" height=\"320\">\n","<br><br>\n","$ \\begin{align}\n","S\n","= \\begin{pmatrix}\n","s_{1}^2 & s_{12} & \\cdots & s_{1d} \\\\\n","s_{12} & s_{2}^2 & \\cdots & s_{2d} \\\\\n","\\vdots \\\\\n","s_{1d} & s_{2d}  & \\cdots & s_{d}^2\n","\\end{pmatrix}\n","\\end{align}$<br><br>\n","<font color=\"silver\">相関係数</font><br><br>\n","$r=\\cfrac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^n(y_i-\\bar{y})^2}} \\qquad  ρ_{xy} = \\cfrac{\\sigma_{xy}}{\\sigma_{x}\\sigma_{y}}$<br><br>\n","<font color=\"silver\">条件付き期待値</font><br><br>\n","$ \\mathbb{E}[X|Y=y] = \\displaystyle\\sum_{i=1} x_iP(X=x_i|Y=y)$\n","<br><br>\n","$ \\mathbb{E}[X] = \\displaystyle\\sum_{i=1} \\mathbb{E}[X|Y=y]P(Y=y_i)$<br><br>\n","<font color=\"silver\">条件付き分散</font><br><br>\n","$ \\mathbb{V}[X|Y]=\\mathbb{E}[X^{2}|Y]-\\mathbb{E}[X|Y]^{2}$\n","<br><br>\n","$ \\mathbb{V}[X] = \\mathbb{E}\\left[\\mathbb{V}\\left[ X|Y \\right] \\right]+\\mathbb{V}\\left[ \\mathbb{E}\\left[ X|Y \\right] \\right]$\n","<br><br>$ \\begin{align}\n","\\mathbb{V}[X]&=\\displaystyle\\sum_{i=1}^n{(x_i-\\mu)}^2\\mathrm{P} (X=x_i)\\\\\\\\\n","&=\\mathbb{E}[(X-\\mathbb{E}[X])^2]\\\\\\\\\n","&=\\mathbb{E}[X^2]-(\\mathbb{E}[X])^2\n","\\end{align}$\n"],"metadata":{"id":"02O9ilyoRh7h"}},{"cell_type":"markdown","source":["# <font color=\"silver\">確率分布│正規分布</font>\n",">- <font color=\"silver\">期待値分散</font><br>\n","$\\mathcal{N}(x | \\mu, \\sigma^2)$ <font color=\"silver\">一変量正規分布, </font><br>\n","$\\mu$ <font color=\"silver\">期待値</font><br>\n","$\\sigma^2$ <font color=\"silver\">分散</font><br><br>\n","- <font color=\"silver\">確率関数</font><br>\n","$f(x | \\mu, \\sigma^2) =\\cfrac{1}{\\sqrt {2\\pi \\sigma^2}}\\exp{\\biggl (}-{\\cfrac {(x-\\mu)^2}{2\\sigma^{2} }}{\\biggr )}$</font><br><br>\n","$f(x | \\mu, \\sigma^2) = \\cfrac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2\\sigma^2}(x - \\mu)^2}$<br><br>\n","- <font color=\"silver\">尤度関数</font><br><br>\n","$\\displaystyle L(\\mu,\\sigma^2|x) = \\prod_{i=1}^n{\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{\\left(-\\frac{(x_i-\\mu)^2}{2\\sigma^2} \\right)}}$<br><br>\n","- <font color=\"silver\">対数尤度関数</font><br><br>\n","$\\displaystyle l(\\mu,\\sigma^2|x) = -n \\log \\sqrt{2\\pi\\sigma^2} - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i-\\mu)^2$<br><br>\n","$\\displaystyle l(\\mu,\\sigma^2|x)=-\\frac{1}{2}n\\log 2\\pi-\\frac{1}{2}n\\log \\sigma^{2}-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(x_{i}-\\mu)^{2}$<br><br>\n","- <font color=\"silver\">最尤推定量</font><br><br>\n","$\\displaystyle{\\hat{\\mu}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}} \\qquad \\displaystyle{\\hat{\\sigma}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}(x_{i}-\\mu)^{2}}$"],"metadata":{"id":"ptpyh_wzOYD1"}},{"cell_type":"markdown","source":["# <font color=\"silver\">確率分布│ベルヌーイ分布</font>\n",">- <font color=\"silver\">期待値分散</font><br>\n"," $\\mathrm{Ber}(x | p)$ <font color=\"silver\">ベルヌーイ分布</font><br>\n","$p$ <font color=\"silver\">期待値</font><br>\n","$p(1-p)$ <font color=\"silver\">分散</font><br><br>\n","- <font color=\"silver\">確率関数</font><br><br>\n","$f(x | p)=p^{x}(1-p)^{1-x}\\qquad p \\in [0,1] \\qquad x \\in \\{0,1\\}$<br><br>\n","- <font color=\"silver\">尤度関数</font><br><br>\n","$L(p|x) =  \\displaystyle\\prod_{i=1}^n p^{x_i}(1-p)^{1-x_i}$<br><br>\n","- <font color=\"silver\">対数尤度関数</font><br><br>\n","$\\displaystyle l(p|x)=\\sum_{i=1}^{n}\\left\\{x_{i}\\log p+(1-x_{i})\\log(1-p)\\right\\}$<br><br>\n","- <font color=\"silver\">最尤推定量</font><br><br>\n","$\\displaystyle{\\hat{p}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}}$"],"metadata":{"id":"_1MUf2ERSY1O"}},{"cell_type":"markdown","source":["# <font color=\"silver\">確率分布│カテゴリカル分布</font>\n",">- <font color=\"silver\">期待値分散</font><br>\n"," $\\mathrm{Cat}(\\pmb{x}|\\pmb{p})$<font color=\"silver\">ベルヌーイ分布</font><br>\n","$p_k$<font color=\"silver\">期待値</font><br>\n","$p_k(1-p_k)$<font color=\"silver\">分散</font><br><br>\n","- <font color=\"silver\">確率関数</font><br>\n","$\\mathrm{Cat}(\\pmb{x}|\\pmb{p})= \\displaystyle\\prod_{k=1}^K {p_k}^{x_k}$<br><br>\n","$\\displaystyle\\sum\\limits_{k=1}^K p_k = 1 \\qquad 0 \\le p_k \\le 1 \\qquad \\pmb{x} = (x_1, \\dots, x_K) \\qquad \\ x_k \\in \\{0, 1\\} \\ \\text{where} \\displaystyle\\sum\\limits_{k=1}^K x_k = 1$<br><br>\n","- <font color=\"silver\">尤度関数</font><br><br>\n","$\\displaystyle L(p_{j})=\\prod_{i=1}^{n}\\prod_{j=1}^{k}p_{j}^{x_{ij}}$<br><br>\n","- <font color=\"silver\">負の対数尤度関数</font><br><br>\n","$\\displaystyle -\\log L(p_{j})=-\\sum_{i=1}^{n}\\sum_{j=1}^{k}x_{ij}\\log p_{j}$<br><br>\n","- <font color=\"silver\">最尤推定量</font><br><br>\n","$\\displaystyle{\\hat{p}_{j}=\\frac{1}{n}\\sum_{i=1}^{n}x_{ij}}$"],"metadata":{"id":"3qIYzZD7KzGb"}},{"cell_type":"markdown","source":["# <font color=\"silver\">確率分布│二項分布</font>\n",">$\\mathrm{Bin}(x | p,n) = {}_{n}\\mathrm{C}_{x}\\;p^x(1-p)^{n-x}＝\\cfrac{n!}{x!(n-x)!}p^x(1-p)^{n-x}$\n","<br><br>\n","$\\mathbb{E}_{X \\sim \\text{Bin}}[X=x_j]=np$\n","<br><br>\n","$\\mathbb{V}_{X \\sim \\text{Bin}}[X=x_j]=np(1-p)$\n","<br><br>\n","$p \\in [0,1] ,\\quad  x=0,1,\\cdots,n$"],"metadata":{"id":"lLVe6dL6Sd31"}},{"cell_type":"markdown","source":["# <font color=\"silver\">確率分布│多項分布</font>\n",">$\\mathrm{Mult}(\\pmb{x}|\\pmb{p},n) = \\displaystyle\\cfrac{n!}{x_1! \\, \\dots \\, x_K!} \\prod\\limits_{k=1}^K p_k^{x_k}$\n","<br><br>\n","$\\mathbb{E}_{X \\sim \\text{Mult}}[X=x_k]=np_k \\qquad \\mathbb{V}_{X \\sim \\text{Mult}}[X=x_k]=np_k(1-p_k)$\n","<br><br>\n","$\\displaystyle\\sum\\limits_{k=1}^K p_k = 1, \\quad 0 \\le p_k \\le 1,\\quad x_k \\in \\{ 0, 1, \\dots, N \\} \\ \\text{where} \\ \\displaystyle\\sum_{k=1}^K x_k = N$"],"metadata":{"id":"QmsldcQ3Sqsb"}},{"cell_type":"markdown","source":["# <font color=\"silver\">確率分布│最小二乗法テンプレ</font>\n","> - <font color=\"silver\">Description</font><br><br>\n"," - 最尤推定と最小２乗法<br><br>\n","   - 回帰問題のためのモデルを考える。<br><br>\n","$y=f(\\pmb{x}:\\pmb{θ})+\\epsilon$<br><br>\n","   - $\\epsilon$ は、一変量正規分布 $N(0, \\sigma^2)$ に従う確率変数である<br><br>\n","$\\epsilon_i=y_i-f(\\pmb{x}_i:\\pmb{θ})$<br><br>\n","$ f(\\epsilon | 0, \\sigma^2)  =\\cfrac{1}{\\sqrt {2\\pi \\sigma^2}}\\exp{\\biggl (}-{\\cfrac {\\epsilon^2}{2\\sigma^{2} }}{\\biggr )}$<br><br>\n","$\\displaystyle -\\log L(p \\mid \\mathcal{D})=\\frac{1}{2\\sigma^2}\\sum_{i=1}^{N} (y_i-f(\\pmb{x}_i:\\pmb{θ}))^2 + N\\log\\sqrt {2\\pi \\sigma^2}$<br><br>\n","   - 最小二乗法の目的関数<br><br>\n","$\\displaystyle\\pmb{θ}^*, σ^* =\\mathop{\\rm argmax}\\limits_{\\pmb{θ},σ}\\frac{1}{2\\sigma^2}\\sum_{i=1}^{N} (y_i-f(\\pmb{x}_i:\\pmb{θ}))^2 + N\\log\\sqrt {2\\pi \\sigma^2}$<br><br>\n","   - $\\pmb{θ}$を解くために$θ$を固定したときの目的関数<br><br>\n","$\\displaystyle\\pmb{θ}^* =\\mathop{\\rm argmax}\\limits_{\\pmb{θ}}\\frac{1}{2}\\sum_{i=1}^{N} (y_i-f(\\pmb{x}_i:\\pmb{θ}))^2$<br><br>\n","   - 一変量正規分布 $N(\\mu, 1)$ に従う確率変数である<br><br>\n","$f(x | \\mu, 0) =\\cfrac{1}{\\sqrt {2\\pi}}\\exp{\\biggl (}-{\\cfrac {(x-\\mu)^2}{2}}{\\biggr )}$\n"],"metadata":{"id":"DPtlu3sfFX-k"}},{"cell_type":"markdown","source":["# <font color=\"silver\">確率分布│appendix, 例題</font>\n",">※ コインを5回投げるとき、表が2回出る確率</font>\n","<br><br>\n","<font color=\"silver\">$\\displaystyle\\cfrac{5!}{2!3!}\\left(\\frac{1}{2}\\right)^2\\left(\\frac{1}{2}\\right)^3 = 10 \\times \\frac{1}{2^5} = \\frac{5}{16}$</font>\n","<br><br>\n","※ 赤玉2個、白玉1個が入っている袋から、玉を1個取り出すことを5回行うとき、赤玉が4回出る確率</font>\n","<br><br>\n","<font color=\"silver\">$\\displaystyle\\cfrac{5!}{4!1!}\\left(\\frac{2}{3}\\right)^4\\left(\\frac{1}{3}\\right) = 5 \\times \\frac{2^4}{3^5} = \\frac{80}{243}$</font>\n","<br><br>\n","※ サイコロを4回投げるとき、1の目が2回、4の目が1回、5の目が1回出る確率</font>\n","<br><br>\n","<font color=\"silver\">$\\displaystyle\\frac{4!}{2!\\,0!\\,0!\\,1!\\,1!\\,0!}\\left(\\frac{1}{6}\\right)^2\\left(\\frac{1}{6}\\right)\\left(\\frac{1}{6}\\right) = 12 \\times \\frac{1}{6^4} = \\frac{1}{108}$</font>\n","<br><br>\n","※ 赤玉4個、青玉3個、白玉1個が入っている袋から、玉を1個取り出すことを4回行うとき、赤玉が1回、青玉が2回、白玉が1回出る確率</font>\n","<br><br>\n","<font color=\"silver\">$\\displaystyle\\frac{4!}{1!\\,2!\\,1!}\\left(\\frac{4}{8}\\right)\\left(\\frac{3}{8}\\right)^2\\left(\\frac{1}{8}\\right) = 12 \\times \\frac{4 \\times 3^2}{8^4} = \\frac{27}{256}$</font>"],"metadata":{"id":"KqKpGetGSzny"}},{"cell_type":"markdown","source":["# <font color=\"silver\">ベイズ [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!5739&cid=b0f01606242a6ed3&CT=1670906803004&OR=ItemsView)</font>"],"metadata":{"id":"WUuMsqZRzgLU"}},{"cell_type":"markdown","source":["# <font color=\"silver\">ベイズ│ベイズの定理</font>\n","> - 事象の確率・余事象の確率<br>\n"," - $P(A)=\\cfrac{n(A)}{n(U)}, \\quad P(\\overline{A})=1-P(A), \\quad 0 \\leq P(A) \\leq 1$<br><br>\n","> - 加法定理<br>\n"," - $P(A \\cup B) = P(A) + P(B) – P(A \\cap B)$<br><br>\n","> - 乗法定理, 同時確率<br>\n"," - $P(A \\cap B)=P(A,B)=P(A) \\times P(B)=P(A\\mid B)P(B)=P(B\\mid A)P(A)$<br><br>\n","> - 周辺確率<br>\n"," - $P(A)=\\displaystyle\\sum_{B}P(A, B)= \\displaystyle\\sum_{B}P(A\\mid B)P(B)$<br><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F582794%2F6d2be4f5-892d-cd55-db41-82897302bc3b.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=b54b5f1cb263bffad207f74920bd9a70\" width=\"320\"><br><br>\n","> - ベイズの定理<br>\n"," - ベイズ推定では、ベイズの定理により、実データの観測を通してモデルパラメータ $θ$ より確からしい確率分布を推定する。<br>\n"," - 事前分布を一様分布とした場合、最尤法による $θ$ の推定値と(式1)において事後確率を最大にする $θ$ は一致する。<br>\n"," - 事前確率$\\displaystyle P(A)$は、一様分布のような比較的高いエントロピーを持つ分布が使われる<br>\n"," - 規格化定数$\\displaystyle P(B)$とは、左辺の積分が1になることを保証する規格化定数<br><br>\n","$\\begin{eqnarray} \\overset{\\small (事後確率)}{P(A \\mid B)} = \\dfrac{ \\overset{\\small (尤度関数)}{P(B \\mid A)} \\overset{\\small (事前確率)}{P(A)} } { \\underset{(周辺尤度・エビデンス・規格化定数)}{P(B)} } \\end{eqnarray}$<br>\n","<br><br>\n","<font color=\"black\">$\\displaystyle P(A \\mid B)=\\cfrac{P(A \\cap B)}{P(B)}=\\cfrac{P(A\\mid B)P(B)}{P(B)}=\\cfrac{P(B \\mid A)P(A)}{P(B)}$<br><br>\n","<font color=\"black\">$\\displaystyle P(A \\mid B)=\\frac{P(B \\mid A)P(A)}{P(B \\mid A)P(A) + P(B \\mid \\bar{A})P(\\bar{A})}$<br><br>\n","> - ベイズ更新<br>\n"," - $P(A\\mid B, C) = \\cfrac{P(C \\mid A)P(A \\mid B)}{P(C)}$\n"],"metadata":{"id":"RDbmwJ76ioYS"}},{"cell_type":"markdown","source":["# <font color=\"silver\">ベイズ│最尤推定</font>\n","> - <font color=\"silver\">Description</font><br>\n"," - 最尤推定</font><br>\n","   - $p(\\pmb{x}\\mid \\pmb{\\theta})$によって元データ$p(\\pmb{x})$布を表現する方法論<br>\n","   - 事象$x$が真の確率分布$p(\\pmb{x})$に従って生起していると仮定したとき、真の確率分布$p(\\pmb{x})$は分からないので、<font color=\"blue\">$p(\\pmb{x}\\mid \\pmb{\\theta})$で近似</font>することができる$\\pmb{\\theta}$を推定する方法<br>\n","   - 最尤推定は<font color=\"blue\">KLダイバージェンスの最小化問題</font><br>。KLダイバージェンスの最小化は交差エントロピーの最小化と対応化できる。<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://stats.biopapyrus.jp/bayesian-statistics/bayesian-estimation.html)</font><br>\n","<img src=\"https://stats.biopapyrus.jp/media/baysianest_maxlikelihoodest_01.png\" width=\"480\"><br>\n","<img src=\"https://stats.biopapyrus.jp/media/baysianest_maxlikelihoodest_05.png\" width=\"480\"><br>\n"," - 尤度関数<br>\n","   - 確率変数$X$がパラメータ$\\pmb{\\theta}$によって決定される確率分布$p(\\pmb{x}\\mid \\pmb{\\theta})$に従うとする。全データに対する尤度は、確率の積で、これを最大にするようにパラメータ$\\pmb{\\theta}$を学習する。<br><br>\n","$\\begin{align}\n","\\mathcal{L}(\\pmb{\\theta}\\mid\\pmb{x})\n","&= f(\\pmb{x}\\mid\\pmb{\\theta})\\\\\n","&= f(x_1,x_2,\\cdots,x_N;\\pmb{\\theta})\\quad\\\\\n","&= \\displaystyle\\prod_{i=1}^{N} f(x_{i}\\mid\\pmb{\\theta})\n","\\end{align}$\n","<br><br>\n"," - 対数尤度関数</font><br>\n","   - 全データに対する尤度の対数をとり、積を和に変換することでアンダーフローを回避する。<br><br>\n","$\\begin{align}\n","\\log\\mathcal{L}(\\pmb{\\theta};\\pmb{x})\n","&= \\log\\displaystyle\\prod_{i=1}^{N} f(x_{i};\\pmb{\\theta})\\\\\n","&= \\displaystyle\\sum_{i=1}^{N} \\log f(x_{i};\\pmb{\\theta})\n","\\end{align}$<br><br>\n"," - 最尤推定</font><br>\n","   - 最尤推定は尤度関数又は対数尤度関数を最大にするパラメータ$\\pmb{\\theta}$を求める問題。学習に用いられる最適化アルゴリズムは最小化を目的に実装されていることが多いため、負にすることで最大化問題を最小化に変換する。<br><br>\n","$\\begin{align}\n","\\hat{\\pmb{\\theta}}\n","&=\\mathop{\\rm argmax}\\limits_{\\pmb{\\theta}}\\mathcal{L}(\\pmb{\\theta};\\pmb{x})\\\\\n","&=\\mathop{\\rm argmax}\\limits_{\\pmb{\\theta}}\\displaystyle\\prod_{i=1}^{N} f(x_{i};\\pmb{\\theta})\\\\\n","&=\\mathop{\\rm argmax}\\limits_{\\pmb{\\theta}}\\log\\displaystyle\\prod_{i=1}^{N} f(x_{i};\\pmb{\\theta})\\\\\n","&=\\mathop{\\rm argmax}\\limits_{\\pmb{\\theta}}\\displaystyle\\sum_{i=1}^{N} \\log f(x_{i};\\pmb{\\theta})\\\\\n","&=\\mathop{\\rm argmin}\\limits_{\\pmb{\\theta}}-\\displaystyle\\sum_{i=1}^{N} \\log f(x_{i};\\pmb{\\theta})\\\\\n","\\end{align}$<br><br>"],"metadata":{"id":"BtBBnqifRFK9"}},{"cell_type":"markdown","source":["# <font color=\"silver\">ベイズ│識別モデルと生成モデル</font>\n","> - <font color=\"silver\">Description</font><br><br>\n"," $\\mathcal{C}_{\\rm pred}=\\mathop{\\rm argmax}\\limits_{\\mathcal{C}}p(\\mathcal{C} \\mid x)$<br><br>\n"," - 識別モデル<br><br>\n"," $p(\\mathcal{C} \\mid x)$ をニューラルネットワークにより直接モデリングして $\\mathop{\\rm argmax}\\limits_{\\mathcal{C}}p(\\mathcal{C} \\mid x)$ を解くアプローチ<br><br>\n"," - 生成モデル<br><br>\n"," $\\displaystyle P(\\mathcal{C} \\mid x)=\\cfrac{ P(x \\mid \\mathcal{C})P(\\mathcal{C})}{P(x)}=\\cfrac{P(x , \\mathcal{C})}{P(x)}$<br><br>\n"," であることから、$P(x \\mid \\mathcal{C})$と$P(\\mathcal{C})$をそれぞれモデリングするか、同時確率 $P(x , \\mathcal{C})$ をモデリングして、$\\mathop{\\rm argmax}\\limits_{\\mathcal{C}}p(\\mathcal{C} \\mid x)$ を解くアプローチ<br><br>\n"," 生成モデルに基づくアプローチで得たモデルを用いて<br><br>\n","$P(x)=\\displaystyle\\sum_{\\mathcal{C}}P(x \\mid \\mathcal{C})P(\\mathcal{C})$<br><br>\n","と周辺化することで、入力 $x$ が従う分布 $p(x)$ を求めることが可能であるため、予測のみならず欠損値の補完やデータ拡張への応用が期待でできる。<br><br>\n"," - 規格化定数<br><br>\n"," 規格化定数$P(x)$は積分によって算出できるが、様に計算することができないため、マルコフ連鎖モンテカルロ法等によって近似する。<br><br>\n","$P({x})= \\displaystyle\\int P({x}\\mid \\mathcal{C})P(\\mathcal{C}) d\\mathcal{C}$<br>"],"metadata":{"id":"NkVtTE7OlDpY"}},{"cell_type":"markdown","source":["# <font color=\"silver\">ベイズ│ナイーブベイズ</font>\n","> - ナイーブベイズ<br><br>\n","$\\begin{align}\n","P(C_k | x_1,x_2,...,x_N)\n","&=\\cfrac{ P(C_k)P(x_1, x_2,...,x_N|C_k) }{P(x_1, x_2,...,x_N) }\\\\\n","&\\propto P(C_k) P(x_1, x_2,...,x_N|C_k) \\\\\n","&= P(C_k)\\prod_{n=1}^{N} P(x_n|C_k)\n","\\end{align}$\n","<br><br>\n","$\\hat{y} = \\mathop{\\rm argmax}\\limits_{y \\in \\mathcal{Y}} \n","\\left( P(C_k)\\displaystyle\\prod_{n=1}^{N} P(x_n|C_k) \\right),\\quad\\hat{y} \\in{ \\{1,...,K \\}}$\n","<br><br>\n","$x_1, x_2,...,x_N$：<font color=\"silver\">$N$個の入力</font><br>\n","$C_k$：<font color=\"silver\">予測するクラス$\\{1,...,K \\}$</font><br><br>\n","> - 文書分類<br><br>\n","$\\begin{align}\n","P(cat|doc) &= \\frac{P(cat)P(doc|cat)}{P(doc)}\\propto P(cat)P(doc|cat)\\\\\n","&= P(cat)\\prod_{i=0}^k P(word_k|cat)\\\\\n","&= \\log\\left(P(cat)\\prod_{i=0}^k P(word_k|cat) \\right)\\\\\n","&= \\log P(cat) + \\log\\prod_{i=0}^k P(word_k|cat)\\\\\n","&= \\log P(cat) + \\sum_{i=0}^k \\log P(word_k|cat)\n","\\end{align}$<br><br>\n","> - 実装, scikit-learn<br><br>\n","<font color=\"silver\">clf = GaussianNB(<br>\n","$\\quad$priors=None,<br>\n","$\\quad$var_smoothing=1e-09<br>\n","$\\quad$)<br>\n","clf.class_count_<br>\n","clf.class_prior_<br>\n","clf.classes_<br>\n","clf.epsilon_<br>\n","clf.n_features_in_<br>\n","clf.feature_names_in_<br>\n","clf.sigma_<br>\n","clf.var_<br>\n","clf.theta_<br>\n","clf = MultinomialNB(<br>\n","$\\quad$alpha=1.0,<br>\n","$\\quad$fit_prior=True,<br>\n","$\\quad$class_prior=None<br>\n","$\\quad$)<br>\n","clf.class_count_<br>\n","clf.class_log_prior_<br>\n","clf.classes_<br>\n","clf.feature_count_<br>\n","clf.feature_log_prob_<br>\n","clf.n_features_<br>\n","clf.n_features_in_<br>\n","clf.feature_names_in_<br>\n","clf = BernoulliNB(<br>\n","$\\quad$alpha=1.0, <font color=\"black\">ラプラススムージングパラメータ, ゼロ頻度問題対策</font><br>\n","$\\quad$binarize=0.0, <font color=\"black\">バイナリ化閾値, 閾値を超える値を1、それ以外を0として扱う</font><br>\n","$\\quad$fit_prior=True,<br>\n","$\\quad$lass_prior=None<br>\n","$\\quad$)<br>\n","clf.class_count_<br>\n","clf.class_log_prior_<br>\n","clf.classes_<br>\n","clf.feature_count_<br>\n","clf.feature_log_prob_<br>\n","clf.n_features_<br>\n","clf.n_features_in_<br>\n","clf.feature_names_in_"],"metadata":{"id":"d3nApjGKiRyY"}},{"cell_type":"markdown","source":["# <font color=\"silver\">ベイズ│appendix, ベイズ推定例題</font>\n","> 　過去の調査結果から、全てのメールのうち10%が迷惑メールであることが分かっています。また、迷惑メールの中で「広告」という単語の含まれる確率が30%、一般メールの中では5%であることが分かっています。無作為に選んだメールに「広告」という単語が含まれていた場合、このメールが迷惑メールである確率はいくつでしょうか？</font><br>\n","<font color=\"silver\">$\\scriptsize \\cfrac{0.3\\times0.1}{0.3\\times0.1＋0.05×0.9}=0.4$</font><br>\n","> 　1万人に1人の割合で罹患する病気があったとします。この病気の陽性/陰性を判定する検査において、誤判定する割合が1%であったとします。Aさんが陽性と判定されたとき、本当に病気にかかっている確率はいくつでしょうか？</font><br>\n","<font color=\"silver\">$\\scriptsize \\cfrac{0.99\\times0.0001}{0.99\\times0.0001＋0.01×0.9999}=0.0098$</font><br>\n","> 　とある製品は、工場A、工場B、工場Cの3つの工場で作られています。生産数量の比率は、それぞれ50%、30%、20%です。また、それぞれの工場における不良の発生率は、1%、2%、5%という調査結果が得られています。ここで、とある不良品がどの工場で製造されたのか不明な場合において、工場Cで生産されたものである確率はいくつでしょうか？</font><br>\n","<font color=\"silver\">$\\scriptsize \\cfrac{0.05\\times0.2}{0.01×0.5 ＋ 0.02×0.3 ＋ 0.05\\times0.2}=0.0476$</font><br>\n","> 　工場A,B,C,Dで生産される製品があり、ある日不良品が発見された。 各工場の生産割合が30%, 40%, 20%, 10%、不良品率が5%, 6%, 4%, 10%であるとしたとき、不良品が生産された可能性が高い工場はどれでしょうか？<br>\n","<font color=\"silver\">$\\scriptsize \\cfrac{？\\times？}{0.05×0.3 ＋ 0.06×0.4 ＋ 0.04\\times0.2＋ 0.10\\times0.1}$</font><br>\n","> 　 疾患 X に罹患している確率が 0.001 であるとする。検査Yは、疾患 X に罹患している人に適用した場合に確率 0.95 で陽性を示し、疾患 X に感染していない人に適用した場合に確率 0.05 で陽性を示すことが知られている。Z氏 に対して検査薬 Y を適用したところ、陽性を示した。Z氏 が疾患 X に罹患している確率はいくつでしょうか？<br>\n","<font color=\"silver\">$\\scriptsize \\cfrac{0.95\\times0.001}{0.95\\times0.001＋0.05×0.999}=0.019$</font><br>\n","> 　母集団に属する人が疾患Xに罹患している確率を0.010 とする。 簡易検 査薬Yは、疾患Xに感染している人に適用した場合に確率 0.90で陽性を 示し、疾患Xに感染していない人に適用した場合に確率0.10で陽性を示 すことが知られている。 母集団に属する人のうち、ある1名に対して 簡易検査薬Yを適用したところ、 陽性を示した。 このとき、 zが疾患Xに 罹患している確率。<br>\n","<font color=\"silver\">$\\scriptsize \\cfrac{0.90\\times0.010}{0.99\\times0.010＋0.10×0.990}=0.083$</font>"],"metadata":{"id":"WTfHAb0uizxX"}},{"cell_type":"markdown","source":["# <font color=\"silver\">情報理論 [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!5746&cid=b0f01606242a6ed3&CT=1670907934354&OR=ItemsView)</font>"],"metadata":{"id":"jd7KFFxm4iYd"}},{"cell_type":"markdown","source":["# <font color=\"silver\">情報理論│基本情報量</font>\n","> - Information<br><br>\n","$ I(x) = -\\log P(x)$<br><br>\n"," -   <font color=\"silver\">Description</font><br><br>\n","$I(x) = \\log\\cfrac{1}{p(x)} = -\\log p(x)$<br><br>\n","<img src=\"https://camo.qiitausercontent.com/4294035b14106783a536dd1344dc0e3d0664559d/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f323531353532372f30313766346164342d613436652d363362622d626336372d6132613038623431303332632e706e67\" width=\"200\">"],"metadata":{"id":"Cp6QeEMDU9cj"}},{"cell_type":"markdown","source":["# <font color=\"silver\">情報理論│平均情報量</font>\n","> - Entropy<br><br>\n","$ H(X) = -\\displaystyle\\sum_{x} P(x) logP(x)$<br><br>\n"," -   <font color=\"silver\">Description</font><br><br>\n","・平均情報量は結果の予測がしにくいときに大きく、予測がしやすいときに小さくなる。</font><br>\n","・あるできごとの発生確率が全て等しいときに平均情報量は最大となる。h(p)はp=0.5のとき、最大値1を取る</font><br>\n","・例えば、コインの表が出る確率が0と1に近づくとき0に近くなり、0.5で最大値1をとる。</font>\n","<br><br>\n",">$ H(p(x))  =  \\mathbb{E}_{p(x)}[I(x)] = \\mathbb{E}_{p(x)}[-\\log p(x)]$<br><br>\n","$ H(X) = \\displaystyle\\sum_{x} I(x) P(x)$<br><br>\n","$ H(X) = -\\displaystyle\\sum_{x} P(x) logP(x)$\n","<br><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2515527%2F47f415f6-1859-f987-69c1-cdc6d9c6b01c.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=d77b5fe90ae5641fe30ebfc0a9a789aa\" width=\"200\">\n","\n","\n"],"metadata":{"id":"lgDoUdSfVBGQ"}},{"cell_type":"markdown","source":["# <font color=\"silver\">情報理論│結合エントロピー</font>\n",">$H(X,Y)  =  - \\displaystyle\\sum_x \\sum_y P(x,y) \\log_2 P(x,y)$"],"metadata":{"id":"PjWIMiPcVEJk"}},{"cell_type":"markdown","source":["# <font color=\"silver\">情報理論│条件エントロピー</font>\n",">$H(X|Y) = - \\displaystyle\\sum_y p(y) \\sum_x p(x|y) \\log_2 p(x|y) = - \\sum_x \\sum_y p(x,y) \\log_2 p(x|y) $"],"metadata":{"id":"QedU-gffVIRg"}},{"cell_type":"markdown","source":["# <font color=\"silver\">情報理論│相互情報量</font>\n",">※ 2 つの情報が互いにどれだけ影響し合っているかを表すもの。</font><br>\n","※ 「平均情報量」と「条件付きエントロピー」の差</font><br><br>\n","$I(X;Y) = \\displaystyle\\sum_x \\sum_y p(x,y) \\log_2 \\left( \\cfrac{p(x,y)}{p(x) p(y)}   \\right)$\n","<br><br>\n","$\\begin{eqnarray}\n","I(X;Y) & = & H(X) - H(X|Y) \\\\\\\\\n","& = & H(Y) - H(Y|X) \\\\\\\\\n","& = & H(X) + H(Y) - H(X,Y)\n","\\end{eqnarray}$"],"metadata":{"id":"_cdR1q28VT5q"}},{"cell_type":"markdown","source":["# <font color=\"silver\">情報理論│バイナリクロスエントロピー</font>\n",">$\\begin{align}\n","H(P,Q) = - \\sum_{i}^{2} y_i \\log \\hat{y_i} &= - (y_1 \\log \\hat{y_1} + y_2 \\log \\hat{y_2} ) \\\\\n","&= - (y_1 \\log \\hat{y_1} + (1-y_1) \\log (1-\\hat{y_1}) )\n","\\end{align}$\n"],"metadata":{"id":"W0Z1Cg6T_Nby"}},{"cell_type":"markdown","source":["# <font color=\"silver\">情報理論│クロスエントロピー</font>\n","> - Cross-Entropy<br><br>\n","$\\displaystyle H(P,Q) = -\\sum_{x \\sim P} P(x) \\log Q(x)$<br><br>\n"," -   <font color=\"silver\">Description</font><br><br>\n","$H(p(x), q(x))  =  \\mathbb{E}_{p(x)}[-\\log q(x)]$<br><br>"],"metadata":{"id":"jAWYBz5MVYrr"}},{"cell_type":"markdown","source":["# <font color=\"silver\">情報理論│KLダイバージェンス</font>\n","> - KL Divergence<br><br>\n","$ D_{KL}(P\\|Q) = \\displaystyle\\sum_{x} P(x) \\log\\cfrac{P(x)}{Q(x)}$\n","<br><br>\n","$ D_{KL}(P\\|Q) = -\\displaystyle\\sum_{x} P(x) \\log\\cfrac{Q(x)}{P(x)}$\n","<br><br>\n"," -   <font color=\"silver\">Description</font><br><br>\n","・$KL(p||q) \\neq KL(q||p)$であり$P$と$Q$が非対称<br>\n","・$P$と$Q$が非対称のため$P(x)$がゼロにが近い状況で$Q(x)$が無視される<br>\n","・$p(x)$を調整して、$p(x)$と$q(x)$の誤差（カルバックライブラー情報量）を最小化するためには、第２項（クロスエントロピー）を最小化することと同じ<br>\n","・最尤推定とKLダイバージェンスの最小化は等価<br><br>\n"," $\\begin{eqnarray} KL(p(x)||q(x)) & = & H(p(x), q(x)) - H(p(x))\\\\ &=& \\mathbb{E}_{p(x)}[-\\log q(x)] - \\mathbb{E}_{p(x)}[-\\log p(x)]\\\\ &=& \\mathbb{E}_{p(x)}[-\\log q(x) + \\log p(x)]\\\\ &=& \\mathbb{E}_{p(x)}\\left[\\log\\left(\\frac{p(x)}{q(x)}\\right)\\right]\\end{eqnarray}$<br><br>\n","（KL Divergence）=（Cross-Entropy）-（Entropy）</font><br><br>\n","${\\begin{align}\n","D_{KL}(P||Q) &= \\sum_{\\Omega\\in x}P(x)\\log \\frac{P(x)}{Q(x)} \\\\ &= \\sum_{\\Omega\\in x}P(x)(\\log P(x)-\\log Q(x)) \\\\ &= \\sum_{\\Omega\\in E}(P(x) \\log P(x)-P(x) \\log Q(x)) \\\\ &= H(P, Q) - H(P)\\\\ &= H(P, Q) - H(P, P)\n","\\end{align}\n","}$<br><br>\n","${\\begin{align}\n","D_{\\mathrm{KL}}(p||q) =& \\sum_{x\\in \\Omega} p(x)\\log_{2}\\frac{p(x)}{q(x)}\\\\\n"," =& \\sum_{x\\in \\Omega} p(x)(\\log_{2}p(x)-\\log_{2}q(x))\\\\\n"," =& \\sum_{x\\in \\Omega} p(x)\\log_{2}p(x)-\\sum_{x\\in \\Omega} p(x)\\log_{2}q(x)\\\\\n"," =& - \\left(-\\sum_{x\\in \\Omega} p(x)\\log_{2}p(x)\\right) -\\sum_{x\\in \\Omega} p(x)\\log_{2}q(x)\\\\\n"," =& - H(p) + H(p,q)\\\\\n","=& H(p,q) - H(p)\\\\\n","\\end{align}\n","}$<br><br>\n","$H(p)$は定数\n"],"metadata":{"id":"VrdcfiD5VhO6"}},{"cell_type":"markdown","source":["# <font color=\"silver\">情報理論│JSダイバージェンス</font>\n",">※ $D_{KL}(P\\|M)$と$D_{KL}(Q\\|M)$との平均、対象で交換可能</font><br>\n","※ 真の分布と学習モデルの分布が重ならないとき、勾配消失問題が生じる</font><br><br>\n","$M(x) = \\cfrac{P(x)+Q(x)}{2}$\n","<br><br>\n","$ D_{JS}(P\\|Q) = \\cfrac{1}{2}(D_{KL}(P\\|M) + D_{KL}(Q\\|M))$"],"metadata":{"id":"EIWGbMktVlIW"}},{"cell_type":"markdown","source":["# <font color=\"silver\">正則化 [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!6753&cid=b0f01606242a6ed3&CT=1674416126219&OR=ItemsView)</font>"],"metadata":{"id":"5mFbPAMoPu-E"}},{"cell_type":"markdown","source":["# <font color=\"silver\">正則化│バイアスバイリアンス\n","\n","><font color=\"Blue\">$\\quad$E$\\qquad$=$\\quad$バイアス + バイリアンス$\\quad$+$\\qquad$ノイズ</font><br>\n","><font color=\"Blue\">$\\quad$$\\qquad$$\\qquad$$\\qquad$予測値と理想値$\\qquad$+$\\qquad$理想値と実データ</font><br>\n","<br>\n","> $\\displaystyle E\\left( L\\right) =\\int \\left\\{ y\\left( x\\right) -h\\left( x\\right) \\right\\} ^{2}p\\left( x\\right) dx + \\int \\int \\left\\{ h\\left( x\\right) -t\\right\\} ^{2}p\\left( x,t\\right) dxdy$\n","<br><br>\n","損失関数<br>\n","第１項：<font color=\"silver\">予測値と理想値の差 → バイアス＋バイリアンス</font><br>\n","第２項：<font color=\"silver\">理想値と実データの差 → ノイズ</font><br>\n","$y(x)$：<font color=\"silver\">予測モデル</font><br>\n","$h(x)$：<font color=\"silver\">理想的な予測モデル</font><br>\n","$t$：<font color=\"silver\">目的変数（実データの値）</font>\n","<br><br>\n","><font color=\"Blue\">$\\qquad$バイアス$\\qquad$$\\qquad$$\\qquad$$\\qquad$+$\\qquad$$\\qquad$バイリアンス</font><br>\n","><font color=\"Blue\">$\\qquad$予測値の期待値と理想値$\\qquad$+$\\qquad$$\\qquad$予測値と予測値の期待値</font><br><br>\n","$\\displaystyle \\int \\left\\{ E_{D}\\left[ y\\left( x;D\\right) \\right] -h\\left( x\\right) \\right\\} ^{2}p(x)dx +  \\int E_{D}\\left[ \\left\\{ y\\left( x;D\\right) -E_{D},\\left[ y\\left( x;D\\right) \\right] \\right\\} ^{2}\\right] p\\left( x\\right) dx$\n","<br><br>\n","損失関数の第１項<br>\n","第１項：<font color=\"silver\">予測値の平均が理想値からどれくらい離れているか → バイアス</font><br>\n","第２項：<font color=\"silver\">特定のデータが予測値全体からどれくらい離れているか → バイリアンス</font><br>\n","<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://nisshingeppo.com/ai/whats-bias-variance/)<br></font>\n","<img src=\"https://nisshingeppo.com/ai/wp-content/uploads/2021/07/%E3%83%90%E3%82%A4%E3%82%A2%E3%82%B9%E3%81%A8%E3%81%AF-768x432.jpg\" height=\"240\"><br>\n","<img src=\"https://nisshingeppo.com/ai/wp-content/uploads/2021/07/%E3%83%90%E3%83%AA%E3%82%A2%E3%83%B3%E3%82%B9%E3%81%A8%E3%81%AF.jpg\" height=\"240\"><br>\n","<img src=\"https://nisshingeppo.com/ai/wp-content/uploads/2021/07/%E3%83%8E%E3%82%A4%E3%82%BA%E3%81%A8%E3%81%AF.jpg\" height=\"200\"><br>\n","<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://wimper-1996.hatenablog.com/entry/2020/02/22/220558)</font></font><br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/w/wimper_1996/20200221/20200221233936.png\" width=\"320\">\n"],"metadata":{"id":"w-9r35AiP3gC"}},{"cell_type":"markdown","source":["# <font color=\"silver\">正則化│L1正則化･L2正則化\n","> - <font color=\"silver\">Description</font><br>\n"," - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://cochineal19.hatenablog.com/entry/2021/05/16/133121)</font></font><br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/c/cochineal19/20210516/20210516134227.png\" width=\"640\"><br><br>\n"," - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://wimper-1996.hatenablog.com/entry/2020/02/22/220558)</font></font><br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/w/wimper_1996/20200222/20200222215249.png\" width=\"320\"><br><br>\n"," - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/c60evaporator/items/784f0640004be4eefc51)</font></font><br>\n","<img src=\"https://camo.qiitausercontent.com/3b56c556a042f4e4889309dad0f8199da935a9d9/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3631303136372f35653935363561352d623933662d383962332d346238392d6161373139373437356533652e706e67\" width=\"480\"><br><br> \n"," - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/K_Noguchi/items/3f5cf527d6f6d46767fb)</font></font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fi.imgur.com%2FsH5KKDK.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=b05abd701be6a19b3f597364fba032da\" width=\"480\"><br><br> \n"],"metadata":{"id":"bn1b8HHBP_8J"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Loss [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!5755&cid=b0f01606242a6ed3&CT=1670913850645&OR=ItemsView)</font>"],"metadata":{"id":"Bu8RREFKcZRw"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Loss│MAE･MSE･RMSE･R2</font>\n","><font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://www.datatechnotes.com/2019/02/regression-model-accuracy-mae-mse-rmse.html)</font></font><br>\n","<img src=\"https://1.bp.blogspot.com/-kL42RjXdOEc/XMELxXVMe3I/AAAAAAAABRw/mx2RoIheodwWj0CPAqg9chwXJmpOyPyJQCLcBGAs/s400/Loss_Functions.PNG\" width=\"480\"><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://www.datatechnotes.com/2019/02/regression-model-accuracy-mae-mse-rmse.html)</font></font><br>\n","<img src=\"https://4.bp.blogspot.com/-wG7IbjTfE6k/XGUvqm7TCVI/AAAAAAAAAZU/vpH1kuKTIooKTcVlnm1EVRCXLVZM9cPNgCLcBGAs/s1600/formula-MAE-MSE-RMSE-RSquared.JPG\" width=\"320\"><br><br>\n","  - MAE<br>\n","    - 外れ値に寛容<br>\n","    - 損失関数の微分が常に一定になるという問題<br><br>\n","$\\textrm{MAE} = \\displaystyle\\cfrac{1}{N}\\sum_{i=1}^{N}|y_{i}-\\hat{y}_{i}|$\n","<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://heartbeat.comet.ml/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0)</font></font><br>\n","<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*8BQhdKu1nk-tAAbOR17qGg.png\" width=\"320\"><br><br>\n","  - MSE<br>\n","    - MAEがデータの中央値寄りなのに対し、MSEは平均値に近い<br><br>\n","$\\textrm{MSE} = \\displaystyle\\cfrac{1}{N}\\sum_{i=1}^{N}(y_{i}-\\hat{y}_{i})^{2}$<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://miro.medium.com/v2/resize:fit:720/format:webp/1*EqTaoCB1NmJnsRYEezSACA.png)</font></font><br>\n","<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*EqTaoCB1NmJnsRYEezSACA.png\" width=\"320\"><br><br>\n","  - Huber Loss<br>\n","    - 損失が大きいとMAEに似た機能をし、損失が小さいとMSEの機能になる。<br>\n","    - MSEとMAEの切り替わりは𝛿で設定する<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://miro.medium.com/v2/resize:fit:720/format:webp/1*EqTaoCB1NmJnsRYEezSACA.png)</font></font><br>\n","<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*0eoiZGyddDqltzzjoyfRzA.png\" width=\"480\"><br><br>\n","<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*jxidxadWSMLvwLDZz2mycg.png\" width=\"320\"><br><br>\n","  - Smooth L1 Loss <br><br>\n","$L_1^\\text{smooth}(x) = \\begin{cases}\n","    0.5 x^2             & \\text{if } \\vert x \\vert < 1\\\\\n","    \\vert x \\vert - 0.5 & \\text{otherwise}\n","\\end{cases}$<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://lilianweng.github.io/posts/2017-10-15-word-embedding/)</font></font><br>\n","<img src=\"https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/l1-smooth.png\" width=\"160\"><br><br>\n"," - RMSE<br><br>\n","$\\displaystyle\\textrm{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(y_{i}-\\hat{y}_{i})^{2}}$<br><br>\n"," - R2<br><br>\n","$ \\textrm{R}^{2} = 1 - \\cfrac{\\sum(y_{i}-\\hat{y_{i}})^{2}}{\\sum(y_{i}-\\bar{y_{i}})^{2}}$<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://cor.tokyo/terminology/74/)</font></font><br>\n","- 予測データの分散が教師データの分散に相対して、何割占めているかを表す。<br>\n","- 精度依存する評価と違い、分散が低くても、精度が高くない場合があり得る。<br>\n","- 回帰分析の当てはまりの良さ<br>\n","- 目的変数のスケールに依存せず評価可能<br>\n","- 0から1の値をとる<br>\n","<img src=\"https://cor.tokyo/wp-content/uploads/2019/04/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2019-04-18-23.22.06.png\" width=\"320\"><br>\n","><font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/ironball/items/6acb3546312f4c65ec54)</font></font><br>\n","<img src=\"https://camo.qiitausercontent.com/4b2a8b68fcf74c56350ef368cdbc7b5e4ef73f2e/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3438383537372f35333564336439302d343035302d313862392d316263372d3465373066346666623233322e706e67\" width=\"480\"><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://www.xiaowenying.com/machine-learning/2019/11/18/svm.html)</font><br>\n","<img src=\"https://www.xiaowenying.com/assets/post_img/svm/7824BB24CA8355AFBD5B9BF42521096D.jpg\" width=\"320\"><br>"],"metadata":{"id":"N-ggvSFDuw5n"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Loss│logloss</font>\n","> - <font color=\"silver\">Description</font><br>\n",">  - logloss\n",">    - 0から1の値をとる<br>\n",">    - 予測した確率分布と正解となる確率分布がどのくらい同じかを表す<br>\n",">    - 正しく予測できているときに小さくなる<br><br>\n","$\\begin{align}  \\textrm{logloss} &= -\\frac{1}{N}\\sum_{i=1}^{N}(y_{i}\\log p_{i} + (1 - y_{i}) \\log (1 - p_{i})) \\\\ &=-\\frac{1}{N}\\sum_{i=1}^{N} \\log \\acute{p}_{i}  \\end{align}$\n","<br><br>\n","<font color=\"black\">$y_{i} $: <font color=\"silver\">正例かどうか表すラベル<br>\n","<font color=\"black\">$p_{i}$ : <font color=\"silver\">正例である予測確率<br>\n","<font color=\"black\">$\\acute{p}_{i}$ : <font color=\"silver\">真の値を予測している確率</font></font></font><br><br>\n",">  - multiclass logloss\n",">    - <font color=\"black\">$\\textrm{multiclass logloss} = -\\displaystyle\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{m=1}^{M}y_{i,m}\\log p_{i,m}$<br>\n","<font color=\"black\">$M$ : <font color=\"silver\">クラス数<br>\n","<font color=\"black\">$ y_{i,m} $: <font color=\"silver\">レコード$i$がクラス$m$に属するか表すラベル<br>\n","<font color=\"black\">$p_{i,m} $: <font color=\"silver\">レコードiがクラスmに属する予測確率 <br><br></font><br>\n","e.g.真の確率分布$p$, 推定した確率分布$q$ : (1, 0, 0), (0.8, 0.1, 0.1)<br><br>\n","$\\scriptsize{\\begin{align}\n","H(p, q)& = -(1*\\log 0.8 + 0*\\log 0.1 + 0*\\log 0.1) \\\\\n","& = -(\\log 0.8) \\\\\n","& = -(-0.09) \\\\\n","& = 0.09\n","\\end{align}}$<br><br>\n","e.g.真の確率分布$p$, 推定した確率分布$q$ : (1, 0, 0), (0.3, 0.4, 0.3)<br><br>\n","$\\scriptsize{\\begin{align}\n","H(p, q)& = -(1*\\log 0.3 + 0*\\log 0.4 + 0*\\log 0.3) \\\\\n","& = -(\\log 0.3) \\\\\n","& = -(-0.52) \\\\\n","& = 0.52\n","\\end{align}}$<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://cor.tokyo/terminology/74/)</font></font><br>\n","<img src=\"https://greencolor.net/wp-content/uploads/2022/01/%E3%80%90%E8%A9%B3%E7%B4%B0%E3%82%A4%E3%83%A1%E3%83%BC%E3%82%B8%E3%80%91%E4%BA%A4%E5%B7%AE%E3%82%A8%E3%83%B3%E3%83%88%E3%83%AD%E3%83%94%E3%83%BC%E8%AA%A4%E5%B7%AE_-.png\" width=\"480\">"],"metadata":{"id":"zAxR7IT-vCg9"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Loss│オッズ比\n",">- <font color=\"silver\">確率, $\\quad 0 < p(x) < 1$</font><br><br>\n","発生確率$p$, 発生しない確率$1-p$<br><br>\n","- <font color=\"silver\">オッズ比, $\\quad 0 < {\\rm Odds} < ∞$</font><br><br>\n","$\\cfrac{p}{1-p}=\\exp({w^{\\mathrm{T}}x)}$<br><br>\n","- <font color=\"silver\">ロジット関数, $\\quad 0 <-∞ < {\\rm logit}  < ∞$</font><br><br>\n","$ f(p) = \\log \\cfrac{p}{1-p}$<br><br>\n","- <font color=\"silver\">ロジスティック関数, $\\quad 0 < {\\rm logistic}  < 1$</font><br><br>\n","$g(x) = \\cfrac{1}{1+e^{-x}}$<br><br>\n","> - <font color=\"Blue\">description</font><br>\n",">  - オッズ比は、それぞれの係数が1増加したとき、予測確率にどの程度影響があるかを示す。<br>\n",">  - オッズ比が大きい又は小さいほど、事象の間に関連があることを意味する。オッズ比が1.0だとすると、事象間の関係が独立していて互いに影響していないことを意味する。<br>\n",">  - 回帰係数にexpをとると、オッズ比に一致し、また、対数オッズ比は、回帰係数と一致する。<br><br>\n","> - <font color=\"Blue\">オッズ比の導出</font><br><br>\n",">  - $P(y=1|\\boldsymbol{x})=\\hat{y}$<br>\n","$P(y=0|\\boldsymbol{x})=1-\\hat{y}$<br><br>\n","$o=\\cfrac{P(y=1|\\boldsymbol{x})}{P(y=0|\\boldsymbol{x})}$<br><br>\n","$\\begin{align}\n","o\n","&=\\frac{\\hat{y}}{1-\\hat{y}}\\\\\n","&=\\frac{\\sigma(\\boldsymbol{w}^{\\prime\\mathrm{T}}\\boldsymbol{x}^{\\prime})}{1-\\sigma(\\boldsymbol{w}^{\\prime\\mathrm{T}}\\boldsymbol{x}^{\\prime})}\\\\\n","&=\\frac{(1+\\exp{(-\\boldsymbol{w}^{\\prime\\mathrm{T}}\\boldsymbol{x}^{\\prime}}))^{-1}}\n","{1-(1+\\exp{(-\\boldsymbol{w}^{\\prime\\mathrm{T}}\\boldsymbol{x}^{\\prime}}))^{-1}}\\\\\n","&=\\frac{1}\n","{(1+\\exp{(-\\boldsymbol{w}^{\\prime\\mathrm{T}}\\boldsymbol{x}^{\\prime})})-1}\\\\\n","&=\\frac{1}\n","{\\exp{(-\\boldsymbol{w}^{\\prime\\mathrm{T}}\\boldsymbol{x}^{\\prime})}}\\\\\n","&=\\exp{(\\boldsymbol{w}^{\\prime\\mathrm{T}}\\boldsymbol{x}^{\\prime})}\n","\\end{align}$<br><br>\n","> - <font color=\"Blue\">ロジット関数の導出</font><br><br>\n",">  - $\\displaystyle{\\rm logit} (p(\\hat{y}=1|\\pmb{x})) =  \\log \\frac{p(y=1|\\pmb{x})}{1-p(y=1|\\pmb{x})}$\n","<br><br>\n","$\\displaystyle{\\rm logit} (\\pmb{x}) =  \\log \\frac{p}{1-p} $\n","<br><br>\n","$\\displaystyle \\log \\frac{p}{1-p} = \\pmb{x}^\\top \\pmb{w} $"],"metadata":{"id":"RF1BrjPkck5V"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Loss│線形回帰</font>\n","> - 推論<br><br>\n","$\\hat{y} = \\pmb{x}^\\top \\pmb{w}$<br><br>\n","> - 損失<br><br>\n","$\\displaystyle L = \\frac{1}{2}\\sum_{n=1}^N (y^{(n)}-t^{(n)})^2+λ \\sum_{i=1}^Kw_i^2$<br><br>\n","> - 勾配<br><br>\n","$\\displaystyle \\nabla {\\mathcal{L}}_{\\mathcal{D}}(\\pmb{w})=\\frac{\\partial L}{\\partial w}=\\pmb{x}_n^{\\top}(y - t)+2λw$<br><br>"],"metadata":{"id":"RukQDezCcaze"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Loss│ロジスティック回帰</font>\n","> - 推論<br>\n"," - 二値分類<br><br>\n","$\\begin{eqnarray}\n","\\hat{y} =\n","  \\begin{cases}\n","    1 & (~p \\ge 0.5~ ) \\\\\n","    0 & (~p \\lt 0.5~ )\n","  \\end{cases}\n","\\end{eqnarray}$\n","<br><br>\n","$p = P(\\hat{y} = 1 | \\pmb{x}) =\\sigma(\\pmb{x}^\\top \\pmb{w}) = \\cfrac{1}{1 + \\exp\\left(-\\pmb{x}^\\top \\pmb{w}\\right)}$<br><br>\n"," - 多値分類<br><br>\n","$\\hat{y} = \\mathop{\\rm argmax}\\limits_{y \\in \\mathcal{Y}} \\pmb{w}_y^\\top\\pmb{x}$\n","<br><br>\n","$p = P(\\hat{y} =  \\mathcal{C}_j|\\pmb{x}) =\\sigma(\\pmb{w}^\\top \\pmb{x})=\\cfrac{\\exp (\\pmb{w}_k^\\top\\pmb{x})}{\\sum_{k=1}^{K} \\exp (\\pmb{w}_k^\\top\\pmb{x})}$\n","> - 損失<br><br>\n","$\\displaystyle L = - \\sum_n (t_n \\log y_n + (1 - t_n) \\log (1 - y_n)) $<br><br>\n","> - 勾配<br><br>\n",">$\\displaystyle \\nabla {\\mathcal{L}}_{\\mathcal{D}}(\\pmb{w})=\\frac{\\partial L}{\\partial w}=\\displaystyle  \\sum_{n=1}^{N}\\pmb{x}_n^\\top(y_n - t_n)$<br>"],"metadata":{"id":"Z267auTNcgdb"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Loss│appendix</font>\n","> ※ 最尤法のアプローチでは、ニューラルネットワークは P(y=1 | x) を予測する必要があり、 ネットワークの出力値が確率値として有効であるためには、その値の範囲は [0, 1] を満たす必要がある。<br><br>\n","> ※ この制約を満たすために、線形ユニットを用いてその出力に閾値を設ける方法を考えると、 勾配降下法による学習が効率的に行えないため、代わりに、シグモイドを用いることが一般的である。<br><br>\n","> ※ シグモイドユニットは2つの要素で構成されている。まず、前段の層の出力 h, 前段の層からの接続である重み行列 w と バイアスベクトル b から線形層 z を計算する。次に、シグモイド活性化関数を利用して、z を確率値へと変換する。<br><br>\n","> ※ ユニットを用いる際には、最尤推定と組み合わせる。最尤法で使用されるコスト関数(クロスエントロピー関数)の中の対数が、シグモイドの指数を打ち消すためである。この効果により、シグモイドの飽和による勾配消失をさけることが出来る。"],"metadata":{"id":"kGt_9mYGco_O"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Batch Norm  [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!5554&cid=b0f01606242a6ed3&CT=1669754435225&OR=ItemsView)</font>"],"metadata":{"id":"mxQ2p8SkYzjS"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Batch Norm│Batch Norm</font>\n","> - <font color=\"silver\">Description</font><br>\n","   - Batch Norm\n","   - <font color=\"Blue\">$\\tiny{\\rm Link}$ […</font>](https://data-analytics.fun/2021/09/11/understanding-batch-normalization/)  […</font>](https://cvml-expertguide.net/terms/dl/layers/batch-normalization-layer/)</font>\n","     - Description\n","       - $\\gamma$と$\\beta$は、標準化された$x$の分布を最適な分布に変換する係数で、学習の過程で最適化されるパラメータ。$\\gamma$は標準偏差に対応するパラメータで、$\\beta$は平均に対応するパラメータ<br>\n","       - 1つのミニバッチ内で計算される平均$\\mu$と分散$\\sigma^2$とは値が異なる。<br>\n","       - 学習時に、予測で使用するための移動平均した平均値と分散値を保存する。<br><br>\n","       - $\\pmb{h} = f(BN_{\\gamma,\\beta}(\\pmb{W}^{\\top}\\pmb{x}))$<br><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F72390941-4b93-a1d2-3d20-3c880577bf5a.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=62ab9fbab135bd64236cbd583fabebeb\" width=\"240\"> \n","<img src=\"https://camo.qiitausercontent.com/087293548f89da40a84f1ee27e958521f94c7d6a/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f66666262653265342d393165632d366630662d343336322d6434633866346362303937362e706e67\" width=\"280\"><br><br>\n","     -Batch Normの<font color=\"Blue\">課題</font><br>\n","       - バッチサイズが小さいと、データ統計分布が小さすぎてうまく正規化できず、バッチ正規化の効果が発揮できない。\n","       - GPUメモリの上限との兼ね合いから小さなバッチサイズで学習するしかないような「大規模なモデル」の場合は学習しづらい。<br>\n","       - バッチサイズが大きいほど、正則化の効果が減る(汎化性能向上に寄与しなくなる)。<br>\n","       - 再帰型のモデルの場合、バッチの各要素ごとにSequenceが異なるため、単純にバッチ平均が取れない問題<br>\n","       - 動画入力ネットワーク向けに使用すると、隣接フレーム同士でデータの相関（もとい類似性）が高いことから、平均・分散の推定が安定せず、学習がなかなか進行しない。<br><br>\n","     -学習時と予測時<br>\n","       - 学習時<br><br>\n","$\\displaystyle \\mu = \\frac{1}{n}\\sum_{i=1}^{n}x_i\\qquad\\displaystyle \\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\mu)^2\\qquad\\displaystyle \\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2+\\epsilon}}\\qquad\\displaystyle y_i = \\gamma \\hat{x}_i + \\beta$<br><br>\n","<font color=\"black\">mu = np.mean(x, axis=0) <font color=\"silver\"> # 入力xをNの方向に平均 (D, ) <font color=\"blue\">axis=0</font><br></font>\n","mu = np.broadcast_to(mu, (N, D)) <font color=\"silver\"> # Nの方向にブロードキャスト (N, D)<br></font>\n","x_mu = x - mu <font color=\"silver\"> # 入力xから平均値を引く (N, D)<br></font>\n","var = np.mean(x_mu ** 2, axis=0) <font color=\"silver\"> # 入力xの分散, 分散は(x - mu) ** 2を平均したもの, (D, )  <font color=\"blue\">axis=0</font> <br></font>\n","std = np.sqrt(var + epsilon) <font color=\"silver\"> # 入力xの標準偏差, 標準偏差は分散の平方根, (D, ) <br></font>\n","std_inv = 1 / std <font color=\"silver\"> # 標準偏差の逆数 (D, ) <br></font>\n","std_inv = np.broadcast_to(std_inv, (N, D)) <font color=\"silver\"> # Nの方向にブロードキャスト (N, D)  <br></font>\n","x_std = x_mu * std_inv <font color=\"silver\"> # 標準化 (N, D)<br></font>\n","out = gamma * x_std + beta <font color=\"silver\"> # gammaでスケール、betaでシフト (N, D)<br><br></font>\n","       - 予測時<br>\n","バッチごとの平均の平均を母集団の平均とし、バッチごとの分散の平均を母集団の分散としている。実際の計算は移動平均を計算する<br><br>\n","$\\displaystyle\\mathbb{E}[x]=\\mathbb{E}_\\mathcal{B}[\\mu_\\mathcal{B}]$<br>\n","$\\displaystyle\\text{Var}[x]=\\frac{m}{m-1}\\mathbb{E}_\\mathcal{B}[\\sigma^2_\\mathcal{B}]$<br><br>\n","$\\mathbb{E}[x] \\leftarrow \\eta \\times \\mathbb{E}[x] + (1-\\eta)\\times \\mu_{\\mathcal{B}}$<br>\n","$\\text{Var}[x] \\leftarrow \\eta \\times \\text{Var}[x] + (1-\\eta)\\times \\sigma^2_{\\mathcal{B}}$<br><br>\n","$\\begin{align} y&=\\gamma\\cdot \\widehat{x}+\\beta\\\\ &=\\gamma\\frac{x-\\mathbb{E}[x]}{\\sqrt{\\text{Var}[x]+\\epsilon}}+\\beta\\\\ &=\\frac{\\gamma}{\\sqrt{\\text{Var}[x]+\\epsilon}}\\cdot x+\\left(\\beta – \\frac{\\gamma \\mathbb{E}[x]}{\\sqrt{\\text{Var}[x]+\\epsilon}} \\right) \\end{align}$ <br><br>\n","moving_mean = rho * moving_mean + (1-rho) * mu<br>\n","moving_var = rho * moving_var + (1-rho) * var<br>\n","x_mu = x - moving_mean <font color=\"silver\"> # (N, D)<br></font>\n","x_std = x_mu / np.sqrt(moving_var + epsilon)  # (N, D)<br></font>\n","out = gamma * x_std + beta <font color=\"silver\"> # gammaでスケール、betaでシフト (N, D)<br><br></font>\n","     -計算グラフ<br>\n","<img src=\"https://kratzert.github.io/images/bn_backpass/BNcircuit.png\" width=\"640\"><br>\n",">  - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://konchangakita.hatenablog.com/entry/2021/01/12/210000)<br></font>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/k/konchangakita/20210111/20210111154320.png\" width=\"640\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://rikei-logistics.com/batch-normalization1)<br></font>\n","<img src=\"https://rikei-logistics.com/wp-content/uploads/2021/11/1-3-5.png\" height=\"160\">\n","<img src=\"https://rikei-logistics.com/wp-content/uploads/2021/11/1-5-5.png\" height=\"160\"><br>\n","<img src=\"https://rikei-logistics.com/wp-content/uploads/2021/11/1-6-5.png\" width=\"320\"><br>"],"metadata":{"id":"8m7_O0PhY1Kg"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Batch Norm│Layer Norm</font>\n","> - Description<br>\n"," - <font color=\"Blue\">$\\tiny{\\rm Link}$ […</font>](https://data-analytics.fun/2020/07/16/understanding-layer-normalization/) </font>\n",">  - 系列モデル向け<br>\n",">  - 訓練サンプルごとに独立に正規化パラメータが計算されるため、オンライン学習に適する。\n",">  - 各時間ステップで正規化パラメータが計算されるため、可変⻑入力データに対してロバスト\n",">  - 訓練サンプルごとに独立に正規化パラメータが計算されるため、訓練時とテスト時の計算方法が同一である。\n",">  - 平均、標準偏差を同じ層内のすべてのユニットを対象に計算するため、ミニバッチ内のデータごとに、正規化に用いる平均と標準偏差が異なる<br><br>\n","> - Algorithm<br>\n",">  - $\\displaystyle \\bar{x}^{(l)}_n=\\frac{1}{K}\\sum^K_{k=1}{x}^{(l, k)}_n$\n","<br><br>\n",">  - $\\displaystyle s^{(l)}_n=\\sqrt{\\frac{1}{K}\\sum^K_{k=1}\\left({x}^{(l, k)}_n-\\bar{x}^{(l, k)}_n\\right)^2}$<br>"],"metadata":{"id":"_eb7I1fsY4sB"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Batch Norm│Instance Norm</font>\n","> - Description<br>\n"," -  [<font color=\"Blue\">$\\tiny{\\rm Link}$…</font>](https://cvml-expertguide.net/terms/dl/layers/batch-normalization-layer/#32)</font><br>\n",">  - 画像生成、RNN、ニューラルスタイルトランスファーむけ<br>\n",">   - Instance Normの正規化</font><br><br>\n",">    -  $\\displaystyle y_{tijk}= \\frac{x_{tijk}-\\mu_i}{\\sqrt{\\sigma^2_i+\\epsilon}}$\n","<br><br>\n",">    -  $\\displaystyle\\mu_{i}= \\frac{1}{HWT}\\sum^{T}_{t=1} \\sum^{W}_{l=1} \\sum^{H}_{m=1} x_{tilm}$\n","<br><br>\n",">    -  $\\displaystyle\\sigma^2_{i}= \\frac{1}{HWT}\\sum^{T}_{t=1} \\sum^{W}_{l=1} \\sum^{H}_{m=1} \\left( x_{tilm} – \\mu_i\\right)^2$<br>"],"metadata":{"id":"aXo8iTLyY-5f"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Batch Norm│Group Norm</font>\n","> - Description</font><br>\n"," - [<font color=\"Blue\">$\\tiny{\\rm Link}$…</font>](https://cvml-expertguide.net/terms/dl/layers/batch-normalization-layer/#33_CNN)</font><br>\n",">  - 物体検出、セグメンテーション、小バッチ、多タスクの大規模CNN向け改善\n","<br>\n"],"metadata":{"id":"S5TtKaiPZC4w"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Batch Norm│appendix, 適用範囲の比較</font>\n","\n","><img src=\"https://files.ai-pool.com/a/6465ee9f74a38d57b91f0016797e351e.png\" width=\"480\">\n","\n"],"metadata":{"id":"f9OhWiQyZHXS"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Batch Norm│appendix, Algorithmの比較</font>\n","> - Batch Norm と Layer Norm<br>\n",">  - Batch Normの正規化</font><br><br>\n",">    -  $\\displaystyle \\mu = \\frac{1}{n}\\sum_{i=1}^{n}x_i\\qquad\\displaystyle \\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\mu)^2\\qquad\\displaystyle \\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2+\\epsilon}}\\qquad\\displaystyle y_i = \\gamma \\hat{x}_i + \\beta$<br><br>\n",">  - Layer Normの正規化</font><br><br>\n",">    -  $\\displaystyle \\bar{x}^{(l)}_n=\\frac{1}{K}\\sum^K_{k=1}{x}^{(l, k)}_n\\qquad\\displaystyle s^{(l)}_n=\\sqrt{\\frac{1}{K}\\sum^K_{k=1}\\left({x}^{(l, k)}_n-\\bar{x}^{(l, k)}_n\\right)^2}$<br><br>\n","> - Batch Norm と Instance Norm<br>\n",">  - Batch Normの正規化</font><br><br>\n",">    -  $\\displaystyle y_{tijk}= \\frac{x_{tijk}-\\mu_{ti}}{\\sqrt{\\sigma^2_{ti}+\\epsilon}}$\n","<br><br>\n",">    -  $\\displaystyle\\mu_{ti}= \\frac{1}{HW}\\sum^{W}_{l=1} \\sum^{H}_{m=1} x_{tilm}$\n","<br><br>\n",">    -  $\\displaystyle\\sigma^2_{ti}= \\frac{1}{HW}\\sum^{W}_{l=1} \\sum^{H}_{m=1} \\left( x_{tilm} – \\mu_{it}\\right)^2$<br>\n","<br>\n",">   - Instance Normの正規化</font><br><br>\n",">    -  $\\displaystyle y_{tijk}= \\frac{x_{tijk}-\\mu_i}{\\sqrt{\\sigma^2_i+\\epsilon}}$\n","<br><br>\n",">    -  $\\displaystyle\\mu_{i}= \\frac{1}{HWT}\\sum^{T}_{t=1} \\sum^{W}_{l=1} \\sum^{H}_{m=1} x_{tilm}$\n","<br><br>\n",">    -  $\\displaystyle\\sigma^2_{i}= \\frac{1}{HWT}\\sum^{T}_{t=1} \\sum^{W}_{l=1} \\sum^{H}_{m=1} \\left( x_{tilm} – \\mu_i\\right)^2$<br><br>\n","\n"],"metadata":{"id":"73wetRt-ZK12"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Batch Norm│appendix, 実装の比較</font>\n","> - Batch Normの実装</font><br>\n",">  - def Batch_normalization(x, gamma, beta, epsilon=1e-8):<br>\n","$\\quad$N, D = x.shape<br>\n","$\\quad$mu = np.mean(x, axis=0) <br>\n","$\\quad$x_mu = x - mu<br>\n","$\\quad$var = np.mean(x_mu2, axis=0)<br>\n","$\\quad$std = np.sqrt(var + epsilon)<br>\n","$\\quad$x_std = x_mu * std_inv<br>\n","$\\quad$out = gamma * x_std + beta<br>\n","$\\quad$return out<br>\n","> - Layer Normの実装</font><br>\n",">  - def layer_normalization(x, gamma, beta, epsilon=1e-8):<br>\n","$\\quad$N, D = x.shape<br>\n","$\\quad$mu = np.mean(x, axis=1)<br>\n","$\\quad$x_mu = x - mu<br>\n","$\\quad$var = np.mean(x_mu  2, axis=1)<br>\n","$\\quad$std = np.sqrt(var + epsilon)<br>\n","$\\quad$x_std = x_mu / std<br>\n","$\\quad$out = gamma * x_std + beta<br>\n","$\\quad$return out<br>"],"metadata":{"id":"xH7DrqfJZO_i"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Batch Norm│appendix, Algorithm</font>\n","> - Algorithm<br>\n"," - <br><img src=\"https://data-analytics.fun/wp-content/uploads/2021/09/image-14.png\" width=\"480\"><br><br>\n","<img src=\"https://data-analytics.fun/wp-content/uploads/2021/09/image-15.png\" width=\"480\"><br>"],"metadata":{"id":"8kP5HHnqZVYr"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Attention [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4638&cid=b0f01606242a6ed3&CT=1674074334754&OR=ItemsView)\n"],"metadata":{"id":"DB3UVuKxZsS6"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Attention│Soft Attention\n","> - Hard Attention\n",">  - Hard Attentionは重みが0か1になり、特定のkeyを「選択」する動作になる。微分が難しいので、強化学習的な探索が必要になる。自己注意機構の問題点は計算量が大きいことである。特に画像のように注意対象が多くなればなるほど計算量が大きくなる。例えば全ての画素を対称に注意を計算した場合、計算量は画素数の二乗かかることになる。そのため、注意対象を重要な要素だけに絞ることで計算量を減らすことが望ましい。\n","> - Soft Attention\n",">  - 全ての候補に0より大きい注意を与えるのをSoft Attentionと呼び、一部の要素だけ0より大きい注意を与えるのをHard Attentionと呼ぶ。計算量を減らせるのはHard Attentionだが、この場合、注意をあてなかった要素は試されず、勾配が発生しないという問題がある。そのため、強化学習と同様に新しい要素を試してみなければならないという利用と探索のジレンマが発生する。この問題に対し、ベクトルのノルムが大きいものだけを残すことでHard Attentionを達成する手法7）は単純でありながら、高精度を達成でき、自己注意機構を効率的に実現することができ有望である。\n"],"metadata":{"id":"kfXDl-biZtTx"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Attention│Score Function\n","> - Description<br>\n"," - ドット積．学習不可なので関数の外に線形層を設ける<br><br><font color=\"Black\">\n",">$Ω(h^{(s)}_i, {h}^{(t)}_j)=h^{(s)}_i･{h}^{(t)}_j$<br><br>\n"," - バイリニアモデル．学習可能<br><br>\n","$Ω(h^{(s)}_i, {h}^{(t)}_j)=h^{(s)}_i・W{h}^{(t)}_j$\n","<br><br>\n"," - ベクトル結合．学習可能<br><br>\n","$Ω(h^{(s)}_i, {h}^{(t)}_j)=v \\tanh (W[ h^{(s)}_i; {h}^{(t)}_j])=v \\tanh (W_1 h^{(s)}_i + W_2 {h}^{(t)}_j)$<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)</font><br>\n","<img src=\"https://lena-voita.github.io/resources/lectures/seq2seq/attention/score_functions-min.png\" width=\"640\"><br><font color=\"Black\">\n","\n",">|Name|Alignment score function|\n","| :--- | :--- | \n","|Content-base Attention|$\\text{score}(\\boldsymbol{s}_t, \\boldsymbol{h}_i) = \\text{cosine}[\\boldsymbol{s}_t, \\boldsymbol{h}_i]$|\n","|Additive Attention|$\\text{score}(\\boldsymbol{s}_t, \\boldsymbol{h}_i) = \\mathbf{v}_a^\\top \\tanh(\\mathbf{W}_a[\\boldsymbol{s}_t; \\boldsymbol{h}_i])$|\n","|Location-Base Attention|$\\alpha_{t,i} = \\text{softmax}(\\mathbf{W}_a \\boldsymbol{s}_t)$|\n","|General Attention|$\\text{score}(\\boldsymbol{s}_t, \\boldsymbol{h}_i) = \\boldsymbol{s}_t^\\top\\mathbf{W}_a\\boldsymbol{h}_i$|\n","|Dot-Product Attention|$\\text{score}(\\boldsymbol{s}_t, \\boldsymbol{h}_i) = \\boldsymbol{s}_t^\\top\\boldsymbol{h}_i$|\n","|Scaled Dot-Product Attention|$\\text{score}(\\boldsymbol{s}_t, \\boldsymbol{h}_i) = \\frac{\\boldsymbol{s}_t^\\top\\boldsymbol{h}_i}{\\sqrt{n}}$|\n","\n"],"metadata":{"id":"LUpwWDuKZw9s"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Attention│Attention</font>\n","> - Description<br>\n","<font color=\"Blue\">$\\tiny{\\rm Link}$ […</font>](https://cvml-expertguide.net/terms/dl/seq2seq-translation/seq2seq-with-attention/)  […</font>](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86_Note.md#ID_10-6)</font><br>\n",">  - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)</font><br>\n","<img src=\"https://lena-voita.github.io/resources/lectures/seq2seq/attention/general_scheme-min.png\" width=\"720\"><br><br>\n",">  - $\\boldsymbol{Q}$：デコーダー, 検索文, 計算するターゲット<br>\n","$\\boldsymbol{K}$：エンコーダー, 辞書のインデックス, 類似度の計算に使う単語ベクトルの集まり<br>\n","$\\boldsymbol{V}$：エンコーダー, 辞書の本文, 重み付け和計算に使うベクトルの集まり<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://nlpillustration.tech/?p=2171)</font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F68cb794b-92b2-9c4a-55e6-b4696545efd7.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=8042404aad0e06e993a9568404500a92\" width=\"480\"><br><br>\n"," - Output<br>\n","   - 全結合して確率を計算<br>\n","$p(y_j \\mid y_{<j},x)={\\rm softmax}(W_s, \\tilde{h}^{(t)}_j)\\qquad$<br><br>\n","$\\tilde{h}^{(t)}_j=\\tanh (W_c[ c_j; {h}^{(t)}_j])$<br><br>\n"," - Attention Output<br>\n","   - アダマール積を足し合わせる<br><br>\n","$c_j = \\displaystyle\\sum_i^I \\alpha_{ij} h^{(s)}_i$<br><br>\n"," - Attention Weight<br>\n","   - ソフトマックス<br><br>\n","$\\alpha_{ij} = \\cfrac{\\exp(e_{ij})}{\\sum_{i=1}^{I} \\exp(e_{ij})}$<br><br>\n"," - Attention Score<br>\n","   - リピートで次元を合わせてアダマール積を行いaxis=2で足し合わせる<br><br>\n","$e_{ij}=Ω(h^{(s)}_i, {h}^{(t)}_j)$<br><br>\n"," - Attention Input<br>\n","   - エンコーダの全てのタイプステップとデコーダの出力をインプットする<br><br>\n","$hs$（N, T, H）, $h$（N, H）<br><br>\n"," - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/ac86f1992b7beefa1f0c)</font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F1a019fc5-7cc8-5c24-f896-78c5ac20b478.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=7999c583133fcf245a0449025b3cbaa2\" width=\"720\"><br><br>\n","hr = h.reshape(N, 1, H).repeat(T, axis=1)<br>\n","t = hs × hr<br>\n","s = np.sum(t, axis=2)<br>\n","a = softmax.forward(s)<br>\n","ar = a.reshape(N, T, 1).repeat(H, axis=2)<br>\n","t = hs * ar<br>\n","c = np.sum(t, axis=1)<br>"],"metadata":{"id":"c8SOZmnWZ2a7"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Attention│Bahdanau et al., 2015.\n","><font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)</font><br>\n","<img src=\"https://lena-voita.github.io/resources/lectures/seq2seq/attention/bahdanau_model-min.png\" width=\"640\"><br>\n","><font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://lilianweng.github.io/posts/2018-06-24-attention/)</font><br>\n","<img src=\"https://lilianweng.github.io/posts/2018-06-24-attention/encoder-decoder-attention.png\" width=\"640\">"],"metadata":{"id":"7a16yCHhZ6C1"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Attention│Luong, et al., 2015.\n","><font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)</font><br>\n","<img src=\"https://lena-voita.github.io/resources/lectures/seq2seq/attention/luong_model-min.png\" width=\"640\"><br>\n","><font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://lilianweng.github.io/posts/2018-06-24-attention/)</font><br>\n","<img src=\"https://lilianweng.github.io/posts/2018-06-24-attention/luong2015-fig2-3.png\" width=\"640\">"],"metadata":{"id":"Ew6SBVjpZ9k8"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Attention│SENet  [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4873&cid=b0f01606242a6ed3&CT=1666903640848&OR=ItemsView)</font>\n","\n","> ※ 認識に有益なチャネルに着目した学習する<br>\n","> ※ 認識に有益なチャネルに対して重み付けするSE blockをベースラインのネットワークに導入することで認識精度の高精度化ができる手法<br>\n","> ※ 少ないパラメータで設計されているため，非常に効率的であり，わずかなパラメータ数の上昇で認識精度の向上が可能<br>\n","> ※ 汎用性が高く，従来のあらゆるモデルに導入することができる<br><br>\n","> ❶ 特徴マップにGAPを施し，各特徴マップの空間方向に対する平均値を算出<br>\n","> ❷ 2層の全結合層，ReLU，Sigmoid関数を介して特徴マップの各チャネルに対する重みを獲得<br>\n","> ❸ 各チャネルの重みを対応するチャネルに重み付けする<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://colab.research.google.com/github/machine-perception-robotics-group/MPRGDeepLearningLectureNotebook/blob/master/11_cnn_pytorch/04_senet.ipynb)</font></font><br>\n","<img src=\"https://drive.google.com/uc?export=view&id=1PXZ86xjVO1YWaBdRHhCCgoY1eWQP9pR6\" width=\"640\"><br>\n","<br>\n","> ❶ GAPでH×W×Cの特徴マップを潰しC次元のベクトルに変換<br>\n","> ❷ チャンネル毎に重みを掛ける(重みは学習させる)<br>\n","> ❸ 重みを掛けた後の値をシグモイド関数に通して(0, 1)に変換<br>\n","> ❹ それを元の特徴マップに掛けて出力する<br>\n","<br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F99911%2F3cf614de-8e6a-65f6-4d23-b44289e71d0e.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=cd9f5f8d9378eeeddc431403fb3fadb5\" width=\"640\"><br>\n","<br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F99911%2F98b4c20e-80ec-dead-3274-d5031bca20b3.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=96565c2794550fc28b3e868e9f599097\" width=\"320\">\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F99911%2Fecfd46ea-de9e-efc3-dd50-d1841669a022.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=fe863511e949bb861463c23f51bfbd20\" width=\"320\">"],"metadata":{"id":"TYcRRn9RaBMA"}},{"cell_type":"markdown","source":["# <font color=\"silver\">最適化"],"metadata":{"id":"YUwhRn4qgdMB"}},{"cell_type":"markdown","source":["# <font color=\"silver\">最適化│活性化関数</font>\n","> - Maxout<br>\n"," - [<font color=\"Blue\">$\\tiny{\\rm Link}$…</font>](https://qiita.com/Nezura/items/f52fdc483e5e7eceb6b9#maxout)</font></font><br>\n"," - 通常のaffine変換をn個用意し、そのうち最大の値を採用<br><br>\n","$\\begin{align}\n","u_{ik}^{(l)} &= \\sum_{j=1}^m(w_{ijk}^{(l)}z_{j}^{(l-1)}) + b_{ik}^{(l)}\\\\\n","z_i^{(l)} &= max(u_{ik}^{(l)})\n","\\end{align}$<br><br>\n","> -<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://blog.negativemind.com/2019/09/07/deep-convolutional-gan/)</font></font><br>\n","<img src=\"https://miro.medium.com/max/1200/1*ZafDv3VUm60Eh10OeJu1vw.png\" width=\"480\"><br><br>\n","> -<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://iq.opengenus.org/linear-activation-function/)</font></font><br>\n","<img src=\"https://iq.opengenus.org/content/images/2021/11/Comparion-of-Activation-Functions--1-.png\" width=\"800\"><br>"],"metadata":{"id":"fiybDMsHa1la"}},{"cell_type":"markdown","source":["# <font color=\"silver\">最適化│Dropout [<font color=\"silver\">…](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!5889&cid=b0f01606242a6ed3&CT=1671337337741&OR=ItemsView)</font></font>\n","> - Description<br>\n"," - [<font color=\"Blue\">$\\tiny{\\rm Link}$…</font>](https://data-analytics.fun/2021/11/13/understanding-dropout/)</font></font><br>\n",">  - CNNの入力である画像は隣接画素に相関があるので、ランダムにドロップアウトしたとしてもその周りのピクセルで補間できてしまうため、正則化の効果が限定的となる。そのため、ドロップアウトは全結合層に入れる。<br>\n",">  - 予測時はドロップアウトしない確率$p-1$をかける。すべてのノードを採用すると、学習時に比べ結合後の値が大きくなるので、ドロップアウトしない確率$p-1$を掛けることにより、値を調整している。</font><br>\n",">  - 逆伝播時は、順伝播計算時に使用したノードだけ、誤差を伝播させる<br><br>\n","torch.nn.Dropout(p=0.5, inplace=False)<br><br>\n","class Dropout:<br>\n","$\\quad$def __ init __(self, dropout_ratio=0.5):<br>\n","$\\quad$$\\quad$self.dropout_ratio = dropout_ratio<br>\n","$\\quad$$\\quad$self.mask = None<br>\n","$\\quad$def forward(self, x, train_flg=True):<br>\n","$\\quad$$\\quad$if train_flg: <font color=\"silver\"> # 0〜1の乱数で x.shape の行列を生成</font><br>\n","$\\quad$$\\quad$$\\quad$self.mask = np.random.rand(*x.shape) > self.dropout_ratio <font color=\"blue\">$~\\leftarrow\\ast$</font><br>\n","$\\quad$$\\quad$$\\quad$return x * self.mask<br>\n","$\\quad$$\\quad$else:<br>\n","$\\quad$$\\quad$$\\quad$return x * (1.0 - self.dropout_ratio) <font color=\"blue\">$~\\leftarrow\\ast$</font><br>\n","$\\quad$ def backward(self, dout):<br>\n","$\\quad$$\\quad$return dout * self.mask <font color=\"blue\">$~\\leftarrow\\ast$</font><br>"],"metadata":{"id":"jRSzNiWcaGiX"}},{"cell_type":"markdown","source":["# <font color=\"silver\">最適化│重みの初期値</font> \n","[…](https://kiyosucyberclub.web.fc2.com/ZeroDLPage04-03.html) […](https://cvml-expertguide.net/terms/dl/optimization/weight-initialization/) […](https://pystyle.info/pytorch-parameters-initialization/)<br>\n","０や同じ値を初期値にすると、全て同じ値で推移してしまうため、ガウス分布を利用して、全くランダムな値ではなく「同じような値」だが微妙に違うという値を重みの初期値とする。<br><br>\n","torch.nn.init.normal_(tensor, mean=0.0, std=1.0)<br>\n","torch.nn.init.xavier_normal_(tensor, gain=1.0)<br>\n","torch.nn.init.kaiming_normal_(tensor, a=0, mode=\"fan_in\", nonlinearity=\"leaky_relu\") <br><br>\n","Xavierの初期値</font><br>\n","sigmoid関数やtanh関数のように点対称で中央付近で線形関数としてみなせる活性化関数に向いている。<br><br>\n","$\\sqrt {\\dfrac {1}{n}},\\quad\\displaystyle \\sqrt \\frac{2}{n1+n2}$を標準偏差とするガウス分布<br><br>\n","n1：<font color=\"silver\">前の層のノード数,</font> n2：<font color=\"silver\">後ろの層のノード数</font><br><br>\n","Heの初期値</font><br>\n","Xavierより広がりを持った初期値<br><br>\n","<font color=\"Black\">$\\sqrt {\\dfrac {2}{n}},\\quad\\displaystyle \\sqrt \\frac{2}{n1}$を標準偏差とするガウス分布<br><br>\n","n1：<font color=\"silver\">前の層のノード数</font><br><br>\n","def init_weights(m: nn.Module):<br>\n","$\\qquad$for name, param in m.named_parameters():<br>\n","$\\qquad$$\\qquad$if 'weight' in name:<br>\n","$\\qquad$$\\qquad$$\\qquad$nn.init.normal_(param.data, mean=0, std=0.01)<br>\n","$\\qquad$$\\qquad$else:<br>\n","$\\qquad$$\\qquad$$\\qquad$nn.init.constant_(param.data, 0)<br>\n","model.apply(init_weights)<br>\n","\n"],"metadata":{"id":"UX9mctkDa9O8"}},{"cell_type":"markdown","source":["# <font color=\"silver\">最適化│WeightDecay</font>\n","> - <font color=\"silver\">Description</font><br>\n"," - 重みの値が大きくなることによって過学習が起きることが多いため、重みの値が大きくなることにペナルティを与え、その重みの値が大きくならないように制限する。\n"," - <font color=\"blue\">パラメータが原点から離れていくことに対してペナルティを課す</font>。\n"," - 正則化係数の値は層によって異なっていてもよい。\n"," - 正則化係数を大きく設定しすぎると、過剰適合ではなく過少適合の傾向が表れる \n"," -  バイアスにはペナルティを課さない。\n","   -  バイアスは重みとは異なり、大きくなることが過剰適合に繋がることは少ない。\n","   -  バイアスに対する正則化は、モデル の表現力を過剰に低下させるおそれがある 。\n","   -  バイアスはときに大きな値をとる必要があるため、バイアスにはペナルティを課さない。<br><br>\n"," - 損失関数<br><br>\n","$\\displaystyle L = L' + \\frac{1}{2} \\lambda \\sum_{l=1}^{layers} \\sum_{i, ~~j} (w_{ij}^{l})^2$<br>  \n","$w_{i,j}^l$ :<font color=\"silver\"> $l$層目の重み行列${\\bf W}$の$i, j $成分<br></font>\n","$L$ :<font color=\"silver\"> 正則化項を加えた後の損失<br></font>\n","$L'$ : <font color=\"silver\">正則化項を加える前の損失 <br></font>\n","$layers$ :<font color=\"silver\"> 層番号<br></font>\n","$\\lambda$ :<font color=\"silver\"> 係数<br></font><br>\n","def loss(self, x, t):<br>\n","$\\qquad$y = self.predict(x)<br>\n","$\\qquad$lmd = self.weight_decay_lambda<br>\n","$\\qquad$weight_decay = 0<br>\n","$\\qquad$for idx in range(1, self.hidden_layer_num + 2):<br>\n","$\\qquad$$\\qquad$W = self.params['W' + str(idx)]<br>\n","$\\qquad$$\\qquad$weight_decay += 0.5 * lmd * np.sum(W**2) <font color=\"silver\"> # 全ての行列Wについて積算していく</font><br>\n","$\\qquad$return self.lastLayer.forward(y, t) + weight_decay <br><br>\n"," - 勾配の補正<br><br>\n","実装する際は、上記の更新を勾配の補正によって考慮するのが簡単でよい<br><br>\n","$\\displaystyle \\frac{\\partial L}{\\partial {\\bf W}_t} = \\frac{\\partial L'}{\\partial {\\bf W}_t} + \\lambda {\\bf W}_{t}$<br>  \n","$\\bf W$ :  <font color=\"silver\">重み行列  </font><br>\n","$L'$ :  <font color=\"silver\">正則化項を加える前の損失   </font><br>\n","$\\lambda$ :  <font color=\"silver\">係数</font><br><br>\n","def gradient(self, x, t):<br>\n","$\\qquad$self.loss(x, t)<br>\n","$\\qquad$dout = self.lastLayer.backward(dout=1) <br>\n","$\\qquad$layers = list(self.layers.values())<br>\n","$\\qquad$layers.reverse()<br>\n","$\\qquad$for layer in layers:<br>\n","$\\qquad$$\\qquad$dout = layer.backward(dout)<br>\n","$\\qquad$lmd = self.weight_decay_lambda<br>\n","$\\qquad$grads = {}<br>\n","$\\qquad$for idx in range(1, self.hidden_layer_num+2):<br>\n","$\\qquad$$\\qquad$grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + lmd * self.layers['Affine' + str(idx)].W<br>\n","$\\qquad$$\\qquad$grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db<br>\n","$\\qquad$return grads<br><br>\n"," - パラメータ更新<br>\n","$\\displaystyle {\\bf W}_{t+1} = {\\bf W}_{t} - \\eta (\\frac{\\partial L'}{\\partial {\\bf W}_t} + \\lambda {\\bf W}_{t})$   <br>  \n","$\\bf W$ :<font color=\"silver\">  重み行列  </font><br>\n","$L'$ : <font color=\"silver\"> 正則化項を加える前の損失    </font><br>\n","$\\eta$ : <font color=\"silver\"> 学習率</font><br>\n","$\\lambda$ : <font color=\"silver\"> 係数</font><br><br>\n","- <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/Nezura/items/f7416ad17d79ccd1eab6)</font></font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F212365%2F1ec60965-676e-8fa7-7138-cfed89f3cf8d.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=5822e0c167d3bb202f01c76f970256c0\" width=\"480\"><br>\n","\n","\n","\n","\n"],"metadata":{"id":"wbzK6uR7at_R"}},{"cell_type":"markdown","source":["# <font color=\"silver\">最適化│重み共有</font>\n","> ※ 位置によって別々のパラメータを用いるのではなく、１つのパラメータを共有する<br>\n","> ※ パラメータ数が少なくて済むが、これは一方で、 <font color=\"blue\">パラメータの自由度が制限</font>されているという見方もでき、 正則化の一種であると解釈 できる。"],"metadata":{"id":"5nhR8sUQbFRE"}},{"cell_type":"markdown","source":["# <font color=\"silver\">最適化│勾配クリッピング</font>\n","> - <font color=\"silver\">Description</font><br>\n","torch.nn.utils.clip_grad_norm_(parameters, max_norm, norm_type=2.0, error_if_nonfinite=False)<br><br>\n"," - 勾配のL2ノルムがしきい値を超えた場合に、$g$ に$\\cfrac{v}{\\|g\\|}$を掛けて勾配を修正する。<br>\n","<br>\n","$if \\parallel{g}\\parallel > v:$<br><br>\n","$\\displaystyle g← \\frac{v}{\\|g\\|}g$<br><br>\n","$g$ ：<font color=\"silver\">すべてのパラメータに対する勾配をひとひとつにまとめたもの。</font><br><br>\n","$\\rm -threshold < \\parallel gradient \\parallel < threshold$<br><br>\n","$\\displaystyle\\|g\\|= \\sqrt{ \\Bigl(\\frac{\\partial L}{\\partial \\mathbf{W}_1}\\Bigr)^2  + \\Bigl(\\frac{\\partial L}{\\partial \\mathbf{W}_2}\\Bigr)^2}$<br><br>\n"," - def clip_grads(grads, max_norm):<br>\n","$\\quad$total_norm = 0<br>\n","$\\quad$for key, grad in grads.items():<br>\n","$\\quad$$\\quad$total_norm += np.sum(grad ** 2, axis=None)<br>\n","$\\quad$total_norm = np.sqrt(total_norm)<br>\n","$\\quad$rate = max_norm / (total_norm + 1e-6)<br>\n","$\\quad$if rate < 1:<br>\n","$\\quad$$\\quad$for key, grad in grads.items():<br>\n","$\\quad$$\\quad$$\\quad$grads[key] *= rate<br>\n","def gradient_clipping(grad, threshold):<br>\n","$\\quad$norm = np.linalg.norm(grad)<br>\n","$\\quad$rate = threshold / norm<br>\n","$\\quad$if rate < 1:<br>\n","$\\quad$$\\quad$return grad * rate<br>\n","$\\quad$return grad<br>\n","<font color=\"silver\">returnを指定しない関数のため値を返さず、引数に渡したgradsの値を直接更新する</font><br>\n"],"metadata":{"id":"cQwfNiSqbOIS"}},{"cell_type":"markdown","source":["# <font color=\"silver\">最適化│ラベル平滑化</font>\n","<font color=\"silver\">ラベル平滑化, Label Smoothing</font><br>\n","> - <font color=\"silver\">Description</font><br>\n"," - ソフトマックス関数の正則化に用いられる手法。<br>\n"," - 正解ラベルとして１が、不正解ラベルとして０が設定されている教師データについて、 正解ラベルを1 – εに、不正解ラベルをε/(k − 1)として設定する。<br>\n"," - （例）5クラス分類問題の場合の例<br>\n","   - 元のラベル: 0,0,1,0,0<br>\n","   - ラベル平滑化適応時のラベル: $\\epsilon$/4, $\\epsilon$/4,1 − $\\epsilon$, $\\epsilon$/4, $\\epsilon$/4<br><br>\n","$\\begin{align}\n","t_k^{LS} = (1 – \\epsilon) t_k + \\epsilon u(k) \n","\\end{align}$\n","<br><br>\n","$\\begin{align}\n","L \n"," &= – \\sum_{i=1}^N \\sum_{k=1}^K t_k^{LS} \\log p_{\\theta}(\\pmb{y}^{(i)} | \\pmb{x}^{(i)}) \\\\\n","&= – \\sum_{i=1}^N \\sum_{k=1}^K [ (1 – \\epsilon) t_k + \\epsilon u(k)] \\log p_{\\theta}(\\pmb{y}^i |\\pmb{x}^i) \n","\\end{align}$"],"metadata":{"id":"-TAvaBabbX1e"}},{"cell_type":"markdown","source":["# <font color=\"silver\">最適化│早期終了</font>\n","<font color=\"silver\">早期終了, Early Stopping</font><br>\n","> - <font color=\"silver\">Description</font><br>\n"," - 検証誤差が最小になる時点（訓練誤差を超える時点ではない！）に終了<br>\n"," - 打ち切るタイミング: テストデータに対する誤差関数が上昇傾向に転じたとき。<br>\n"," - 早期終了は、パラメータにある種の制約を課していると考えられるため、正則化の一種であるといえる。<br>\n"," - 2次の誤差関数を持つ単純な線形モデルを単純な勾配降下法で最適化すると き、早期終了は正則化と同等な効果を示すことがあります。 パラメータの 初期値を原点付近にとって訓練を始めた場合、訓練終了時のパラメータは、 早期終了を適応しない場合に比べて、 原点に近い値になります。 正則化を 適用した場合も、適用しなかった場合に比べて、訓練終了時のパラメータが 原点に近い値になります。\n"," <br><br>\n","<img src=\"https://www.azusuki.com/free/wp-content/uploads/2020/06/image-20.png\" width=\"240\"><br><br>\n","> - 計算手順<br>\n"," - 学習の各ステップにおいて、テスト用データを用いて検証誤差(test_loss)を算出する。\n"," - 検証誤差が前ステップの検証誤差よりも大きければ、1カウントする。\n"," - このカウントが指定回数以上になれば、学習を終了させる。\n"," - ここでの指定回数はハイパーパラメータ。"],"metadata":{"id":"Aj99o9gwbjlU"}},{"cell_type":"markdown","source":["# <font color=\"silver\">最適化│スパース表現</font>\n","\n","> ※ 高次元の行列・ベクトルの成分の多くを0にすること<br>\n","> ※ 重みや入力などを適度にスパースにすることにより学習が効率的に進む<br>\n","> ※ 重みをスパースにする→L1正則化<br>\n","> ※ 表現(層間で受け渡されるベクトル)をスパースにする→ReLUなど<br>\n","> ※ 表現に対してノルムペナルティを課すこともある<br>"],"metadata":{"id":"IG2-bIm7byzp"}},{"cell_type":"markdown","source":["# <font color=\"silver\">最適化│アンサンブル学習</font>\n","<font color=\"silver\">バギング・ブースティング・スタッキング</font>"],"metadata":{"id":"SysDD1s0brMN"}},{"cell_type":"markdown","source":["# <font color=\"silver\">勾配法 [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!5215&cid=b0f01606242a6ed3&CT=1671609229685&OR=ItemsView)"],"metadata":{"id":"fl0eeNaoXKTz"}},{"cell_type":"markdown","source":["# <font color=\"silver\">勾配法│勾配消失勾配爆発</font>\n","> - <font color=\"Blue\">勾配消失問題</font><br>\n",">  - SigmoidとTanh<br><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F4d903d18-2e93-ede0-a5d6-f7ef870b9ea6.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=70a59a7304d5c88549f58fce365b56d7\" width=\"240\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2Fc20c428f-c14b-7a14-6c57-bde6b696e3ed.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=57dfc2a09e75bbc7a8a3b58f2bc4833c\" width=\"240\"><br>"],"metadata":{"id":"V7dQxsEIs8kO"}},{"cell_type":"markdown","source":["# <font color=\"silver\">勾配法│ミニバッチ学習</font>\n","> <font color=\"Blue\">バッチサイズ</font><br>\n","バッチサイズが大きいほど、トレーニングにかかる時間は短くなる一方、精度は低くなります。バッチサイズが小さくなるにつれて、テストエラー率が指数関数的に減少する。<br>\n","> <font color=\"Blue\">学習率</font><br>\n",">学習率とは一回の学習で重みパラメータをどれくらい変化させるか、という指標。大きいと一気にパラメータが更新され、逆に小さいとチビチビと進んでいくイメージ。学習率が大きいと損失関数が最小となる谷を一気に通り過ぎるリスクがあり、小さいと局所解にトラップされて学習が進まなくなる可能性がある。<br>\n","<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://shephexd.github.io/deep%20learning/2019/01/22/Deep_learning(7)-Optimization_algorithms.html)</font></font><br>\n","<img src=\"https://shephexd.github.io/assets/images/articles/DeepLearning/batch_vs_mini_batch_gradient.png\" width=\"640\"><br><br>\n","<img src=\"https://shephexd.github.io/assets/images/articles/DeepLearning/sgd_vs_mini_batch.png\" width=\"640\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/35f6f0d26f9e58f01e4e)</font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F1dfa016c-1a7a-7678-74b6-c4591833e2cc.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=ec4b913ade5298101377eb77fb94acfb\" width=\"640\"><br><br>"],"metadata":{"id":"hHGVXb_vNf_p"}},{"cell_type":"markdown","source":["# <font color=\"silver\">勾配法│Define-by-Run</font>\n","\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://eetimes.itmedia.co.jp/ee/articles/1805/09/news030_4.html)</font></font><br>\n","<img src=\"https://image.itmedia.co.jp/ee/articles/1805/09/mat20180507nvidia_image16.jpg\" width=\"640\">\n","<br><br>\n","<img src=\"https://image.itmedia.co.jp/ee/articles/1805/09/l_mat20180507nvidia_image14.jpg\" width=\"640\">\n","<br><br>\n","<img src=\"https://image.itmedia.co.jp/ee/articles/1805/09/l_mat20180507nvidia_image15.jpg\" width=\"640\">"],"metadata":{"id":"r2NEtwkoRbj0"}},{"cell_type":"markdown","source":["# <font color=\"silver\">勾配法│SGD</font>\n","> $\\displaystyle {\\boldsymbol \\theta}_{t+1} = {\\boldsymbol \\theta}_t - \\eta \\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t}$<br><br>\n","> while True:<br>\n","> $\\quad$dx = compute_gradient(x)<br>\n","> $\\quad$x -= learning_rate * dx<br>"],"metadata":{"id":"JbNSKV0espHl"}},{"cell_type":"markdown","source":["# <font color=\"silver\">勾配法│Momentum</font>\n","> ※ 真っ直ぐに進んでいるときは移動量が大きく、曲がりながら進んでいるときは緩やかに曲がるという物理法則に準じる動きをする。<br>\n","> ※ モメンタムは、最適化に向けた移動に「慣性」をつける。同じ方向に移動していく場合はどんどん加速し、逆方向に進もうとすると「慣性」により減速する。v には、前回の移動の方向と移動量が記録されている。<br>\n","<br>\n","$\\displaystyle {\\boldsymbol v}_{t+1} = \\alpha {\\boldsymbol v}_t - \\eta \\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t}$\n","<br>\n","$\\displaystyle {\\boldsymbol \\theta}_{t+1} = {\\boldsymbol \\theta}_t + {\\boldsymbol v}_{t+1}$<br>\n","<br>\n","$\\alpha$ :  <font color=\"silver\">モーメンタム係数(0以上1未満)</font><br>\n","<br>\n","vx = 0<br>\n","rho = 0.9<br>\n","while True:<br>\n","$\\quad$dx = compute_gradient(x)<br>\n","$\\quad$vx = rho * vx - learning_rate * dx<br>\n","$\\quad$x += vx<br>\n","<br>\n","torch.optim.SGD(<br>\n","$\\quad$params, <br>\n","$\\quad$lr=<required parameter>, <br>\n","$\\quad$momentum=0, <br>\n","$\\quad$dampening=0, <br>\n","$\\quad$weight_decay=0, <br>\n","$\\quad$nesterov=False, <br>\n","$\\quad$*, <br>\n","$\\quad$maximize=False, <br>\n","$\\quad$foreach=None, <br>\n","$\\quad$differentiable=False<br>\n","$\\quad$)<br>"],"metadata":{"id":"2FuJU5PAsvbd"}},{"cell_type":"markdown","source":["# <font color=\"silver\">勾配法│AdaGrad</font>\n","> ※ パラメータ毎に学習率を適応させるadaptiveな方法。<br>\n","> ※ 学習が進むにつれ、見かけの学習率が減衰していく。<br>\n","> ※ 学習率εが勾配の2乗と関係があり、最初に勾配が大きいときは学習率も大きくなり、次第に勾配が小さくになるにつれて学習率も小さくなる。<br>\n","> ※ 利点として手動で学習率の調整を行わなくても、学習率が自動調整できることが挙げられる。勾配の緩やかな斜面に対して、最適値に近づける。<br>\n","> ※ 欠点として過去の勾配が訓練の間増加し続け、学習率が低下してくる。\n","<br><br>\n","$\\displaystyle {\\boldsymbol  h}_{t+1} = {\\boldsymbol  h}_{t} + \\frac{\\partial L}{\\partial {\\boldsymbol  \\theta}_t} \\odot \\frac{\\partial L}{\\partial {\\boldsymbol  \\theta}_t}$<br>\n","$\\displaystyle {\\boldsymbol \\theta}_{t+1} = {\\boldsymbol \\theta}_t - \\eta \\frac{1}{ \\epsilon + \\sqrt{{\\boldsymbol  h}_{t+1}}} \\odot \\frac{\\partial L}{\\partial {\\boldsymbol  \\theta}_t}$<br>\n","<br>\n","delta = 1e-7<br>\n","grad_squared = 0<br>\n","while True:<br>\n","$\\quad$dx = compute_gradient(x)<br>\n","$\\quad$grad_squared += dx * dx<br>\n","$\\quad$x -= learning_rate * dx / (np.sqrt(grad_squared) + delta)<br>\n","<br>\n","torch.optim.Adagrad(<br>\n","$\\quad$params, <br>\n","$\\quad$lr=0.01, <br>\n","$\\quad$lr_decay=0, <br>\n","$\\quad$weight_decay=0, <br>\n","$\\quad$initial_accumulator_value=0, <br>\n","$\\quad$eps=1e-10, <br>\n","$\\quad$foreach=None, <br>\n","$\\quad$*, <br>\n","$\\quad$maximize=False<br>\n","$\\quad$)<br>"],"metadata":{"id":"ZdqqsrBps12o"}},{"cell_type":"markdown","source":["# <font color=\"silver\">勾配法│RMSprop</font>\n","> ※ 勾配の2乗の移動平均を用いて、見かけの学習率を変化させていく。<br>\n","> ※ ここでの移動平均とは、指数平滑化移動平均のこと。減衰率$\\rho$の割合で足していくことによって、過去の情報が指数関数的に薄れていく<br>\n","> ※ 勾配の小さな領域にとどまると、${\\boldsymbol h}_{t+1}$が小さくなっていき、見かけの学習率が大きくなっていく。これにより、プラトーを抜け出しやすくなる。<br>\n","> ※ α が過去の勾配による影響を減衰させるパラメータである。最大の変更点は、α によって過去の勾配の影響を抑えると共に、htを優先して反映させるという効果を狙っている<br>\n","<br>\n","$\\displaystyle {\\boldsymbol h}_{t+1} = \\rho{\\boldsymbol h}_{t}+ (1-\\rho)\\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t} \\odot \\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t}$<br>\n","$\\displaystyle {\\boldsymbol \\theta}_{t+1} =  {\\boldsymbol \\theta}_t - \\eta \\frac{1}{\\sqrt{{\\boldsymbol h}_{t+1}+\\epsilon}} \\odot \\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t}$<br>\n","<br>\n","$\\rho$ : <font color=\"silver\">減衰率. 0.9など<br></font>\n","<br>\n","delta = 1e-7<br>\n","grad_squared = 0<br>\n","decay_rate = 0.9<br>\n","while True:<br>\n","$\\quad$dx = compute_gradient(x)<br>\n","$\\quad$grad_squared = decay_rate * grad_squared + (1-decay_rate) * dx * dx<br>\n","$\\quad$x -= learning_rate * dx / (np.sqrt(grad_squared) + delta)<br>\n","<br>\n","torch.optim.RMSprop(<br>\n","$\\quad$params, <br>\n","$\\quad$lr=0.01, <br>\n","$\\quad$alpha=0.99, <br>\n","$\\quad$eps=1e-08, <br>\n","$\\quad$weight_decay=0, <br>\n","$\\quad$momentum=0, <br>\n","$\\quad$centered=False, <br>\n","$\\quad$foreach=None, <br>\n","$\\quad$maximize=False, <br>\n","$\\quad$differentiable=False<br>\n","$\\quad$)<br>"],"metadata":{"id":"hHuBIxZgs8EM"}},{"cell_type":"markdown","source":["# <font color=\"silver\">勾配法│Adam</font>\n","\n","> ※ バイアス修正式の目的は、「パラメータの更新量を大きくすることではなく、0で初期化されている$\\,m\\,$と$\\,v\\,$を大きく評価すること」。バイアス修正式がないと、変動がとても大きくなり不安定になる<br>\n","> ※ $\\,m\\,$のバイアス修正は、学習初期のパラメータ更新量を大きくすることにつながる。<br>\n","> ※ $\\,v\\,$のバイアス修正は、学習初期において見かけの学習率が過剰に大きくなることを防ぐ。<br>\n","> ※ 勾配の2乗の移動平均を用いて、見かけの学習率を変化させていく。<br>\n","> ※ 移動平均をとると、過去の情報が少しずつ薄れていき、新しい情報が反映されていく。<br>\n","> ※ バイアス補正は、計算初期の頃のモーメントを補正することが目的<br>\n","> ※ バイアス補正は、計算が進行する(self.iterが大きくなる)と、分母は1に近づく<br><br>\n","$\\displaystyle {\\boldsymbol m}_{t+1} = \\rho_1 {\\boldsymbol m}_{t} + (1 - \\rho_1)\\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t}$\n","<br>\n","$\\displaystyle {\\boldsymbol v}_{t+1} = \\rho_2 {\\boldsymbol v}_{t} + (1 - \\rho_2)\\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t} \\odot \\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t}$\n","<br>\n","$\\displaystyle {\\hat{\\boldsymbol m}}_{t+1} = \\frac{{\\boldsymbol m}_{t+1}}{1-\\rho_1^t}$<br>\n","$\\displaystyle {\\hat{\\boldsymbol v}}_{t+1} = \\frac{{\\boldsymbol v}_{t+1}}{1-\\rho_2^t}$<br>\n","$\\displaystyle {\\boldsymbol \\theta}_{t+1} = {\\boldsymbol \\theta}_t - \\eta \\frac{1}{\\sqrt{\\hat{\\boldsymbol v}_{t+1}}+\\epsilon} \\odot {\\hat{\\boldsymbol m}}_{t+1}$<br>\n","<br>\n","$\\rho_1$:  <font color=\"silver\">減衰率. 0.9が推奨されている</font><br>\n","$\\rho_2$: <font color=\"silver\">減衰率. 0.99が推奨されている</font><br>\n","<br>\n","first_moment = 0<br>\n","second_moment = 0<br>\n","beta1, beta2 = 0.9, 0.999<br>\n","for i in range(num_iterations):<br>\n","$\\quad$dx = compute_gradient(x)<br>\n","$\\quad$first_moment = beta1 *  first_moment + (1 - beta1)  * dx<br>\n","$\\quad$second_moment = beta2 *  second_moment + (1 - beta2)  *  dx * dx<br>\n","$\\quad$first_unbias = first_moment / (1 - beta1 * * i)<br>\n","$\\quad$second_unbias = second_moment / (1 - beta2 * * i)<br>\n","$\\quad$x -= learning_rate * first_unbias / (np.sqrt(second_unbias) + delta)<br><br>\n","> ※ $\\,m\\,$のバイアス修正は、学習初期のパラメータ更新量を大きくすることにつながる。<br>\n","> ※ $\\,v\\,$のバイアス修正は、学習初期において見かけの学習率が過剰に大きくなることを防ぐ。<br><br>\n","<font color=\"black\">torch.optim.Adam(<br>\n","$\\quad$params, <br>\n","$\\quad$lr=0.001, <br>\n","$\\quad$betas=(0.9, 0.999), <br>\n","$\\quad$eps=1e-08, <br>\n","$\\quad$weight_decay=0, <br>\n","$\\quad$amsgrad=False, <br>\n","$\\quad$*, <br>\n","$\\quad$foreach=None, <br>\n","$\\quad$maximize=False, <br>\n","$\\quad$capturable=False, <br>\n","$\\quad$differentiable=False, <br>\n","$\\quad$fused=False<br>\n","$\\quad$)<br>"],"metadata":{"id":"CbUE3gy3tBo5"}},{"cell_type":"markdown","source":["# <font color=\"silver\">誤差逆伝播 [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!7068&cid=b0f01606242a6ed3&CT=1674995676050&OR=ItemsView) [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!5949&cid=b0f01606242a6ed3&CT=1671581237070&OR=ItemsView) </font>"],"metadata":{"id":"9fxWgMBFje2S"}},{"cell_type":"markdown","source":["# <font color=\"silver\">誤差逆伝播│数値微分と自動微分</font>\n","> - <font color=\"silver\">Description</font><br>\n"," - 数値微分<br>\n","   - 中心差分近似、極限を解くことは困難なた数値微分で近似する。<br>\n","   - 数値微分は、近似解のため、誤差が発生する<br><br>\n","$f'(x) =\\displaystyle \\lim_{h \\to 0} \\frac{f(x+h) – f(x)}{h}$<br><br>\n","$f'(x) = \\displaystyle \\frac{f(x+h) – f(x-h)}{2h}$<br><br>\n"," - 自動微分<br>\n","   - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://yusuke-ujitoko.hatenablog.com/entry/2016/12/31/230118)</font><br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/y/yusuke_ujitoko/20161231/20161231234352.png\" height=\"240\">\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/y/yusuke_ujitoko/20161231/20161231234829.png\" height=\"240\">"],"metadata":{"id":"gheBpR9hkRfI"}},{"cell_type":"markdown","source":["# <font color=\"silver\">誤差逆伝播│連鎖率</font>\n","<font color=\"Blue\">$\\tiny{\\rm Link}$ [<font color=\"Blue\">…</font>](https://qiita.com/kwi0303/items/e43efa6657ff4b6b59ad)</font><br>\n","- $\\overset{x}{\\longrightarrow}\n","{f}\n","\\overset{y=f(x)}{\\longrightarrow}\n","{g}\n","\\overset{z=g(y)}{\\longrightarrow}$<br><br>\n","$\\overset{\\frac{\\partial L}{\\partial x}= \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial x}= \\frac{\\partial L}{\\partial z} \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x}}{\\Longleftarrow}\n","{f}\n","\\overset{\\frac{\\partial L}{\\partial y}= \\frac{\\partial L}{\\partial z} \\frac{\\partial z}{\\partial y}}{\\Longleftarrow}\n","{g}\n","\\overset{\\frac{\\partial L}{\\partial z}}{\\Longleftarrow}$<br><br>\n","- $\\overset{x}{\\longrightarrow}\n","{f}\n","\\overset{y=f(x)}{\\longrightarrow}\n","{g}\n","\\overset{z=g(y)}{\\longrightarrow}$<br><br>\n","$\\overset{\\frac{\\partial L}{\\partial x}= \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial x}= \\frac{\\partial L}{\\partial z}  g'(y)  f'(x)}{\\Longleftarrow}\n","{f}\n","\\overset{\\frac{\\partial L}{\\partial y}= \\frac{\\partial L}{\\partial z} g'(y)}{\\Longleftarrow}\n","{g}\n","\\overset{\\frac{\\partial L}{\\partial z}}{\\Longleftarrow}$<br><br>\n","- <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://becominghuman.ai/back-propagation-in-convolutional-neural-networks-intuition-and-code-714ef1c38199)</font></font><br>\n","<img src=\"https://miro.medium.com/max/1100/1*q1M7LGiDTirwU-4LcFq7_Q.webp\" width=\"640\"><br>\n","- <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/kwi0303/items/e43efa6657ff4b6b59ad)</font></font><br><br><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F1529393%2F07672325-40a6-0ff0-15c3-f318f00e9146.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=a75f6896f1c198bc50ecc59815d4b631\" width=\"320\"><br><br>\n","- 微分\n","    - $y = f(x) \\qquad \\cfrac{dy}{dx} = f'(x)$\n","- 合成関数\n","    - $z=t^2\\quad t=x+y$<br><br>\n","    - $\\cfrac{dz}{dx} = \\cfrac{dz}{dt}\\cfrac{dt}{dx}$<br><br>\n","- 連鎖率\n","    - $\\cfrac{df(g(x))}{dx}=\\cfrac{df}{dx} = \\cfrac{df}{dg}\\cdot\\cfrac{dg}{dx}$<br><br>\n","   - $\\cfrac{dL}{dx} = \\cfrac{d}{dx}f(g(h(u(v(x))))) = \\cfrac{df}{dg}\\cdot\\cfrac{dg}{dh}\\cdot\\cfrac{dh}{du}\\cdot\\cfrac{du}{dv}\\cdot\\cfrac{dv}{dx}$<br><br>\n","- 内積\n","   - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/kwi0303/items/e43efa6657ff4b6b59ad)</font></font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F1529393%2Fa4c386f1-fc66-e8ce-a33b-727592866643.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=1d67cb8ccb51b68dd3a50ef6067e4dce\" width=\"320\"><br><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F1529393%2F07672325-40a6-0ff0-15c3-f318f00e9146.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=a75f6896f1c198bc50ecc59815d4b631\" width=\"320\"><br><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F1529393%2F99159d50-d463-18a0-e052-7c1cd17f0b5f.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=2f04e1b163bca1e1eefce2bf80bc57b3\" width=\"320\"><br><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F1529393%2F67c46004-6db9-5dc0-6ca5-d67ffba67fa8.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=b4fc6e6ca6b968f2b5102dc3dd22d067\" width=\"320\">\n"],"metadata":{"id":"Q4uuhDBqkZcd"}},{"cell_type":"markdown","source":["# <font color=\"silver\">誤差逆伝播│基本の計算グラフ</font>\n","> - <font color=\"silver\">Description</font><br>\n"," - <font color=\"Blue\">$\\tiny{\\rm Link}$ [<font color=\"Blue\">…</font>](https://qiita.com/t-tkd3a/items/031c0a4dbf25fd2866a3)</font><br>\n"," - レイヤの種類に関わらず、一つ前の偏微分に偏微分をかけることで計算<br>\n","><font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://becominghuman.ai/back-propagation-in-convolutional-neural-networks-intuition-and-code-714ef1c38199)</font></font><br>\n","<img src=\"https://miro.medium.com/max/1100/1*q1M7LGiDTirwU-4LcFq7_Q.webp\" width=\"640\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://prog-you.com/relu-sigmoid/)</font></font><br>\n","<img src=\"https://prog-you.com/syohyou/wp-content/uploads/2022/09/42ef5db2ee4380d19915e41ae51b1b6e-768x487.jpg\" width=\"240\"><img src=\"https://prog-you.com/syohyou/wp-content/uploads/2022/09/a514080e39d9c94928b87dd2c28d9035-768x430.jpg\" width=\"240\"><img src=\"https://prog-you.com/syohyou/wp-content/uploads/2022/09/f9888232fd876865b3dd2b81831f642c-768x482.jpg\" width=\"240\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://prog-you.com/relu-sigmoid/) </font></font><br>\n","<img src=\"https://prog-you.com/syohyou/wp-content/uploads/2022/09/01293bf68bea6c67f93e67a5db1415ac-1024x447.jpg\" width=\"240\">\n","<img src=\"https://prog-you.com/syohyou/wp-content/uploads/2022/09/6731d1b8e624de5e6251848fbb421cd4-1024x422.jpg\" width=\"240\">\n","<img src=\"https://prog-you.com/syohyou/wp-content/uploads/2022/09/30b98940bec2394dcea80266a7f47f61-1024x360.jpg\" width=\"240\"><br>\n","> - <font color=\"silver\">Description</font><br>\n"," - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/t-tkd3a/items/031c0a4dbf25fd2866a3) </font></font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F169589%2F2c45d364-2def-9fad-553c-c25ee1d05e18.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=734c25e7e092e0749db7fd86ca2b56e6\" width=\"240\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F169589%2F93b6f59f-a278-f820-85df-f13a2874192c.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=ef9565419bb7e4f6503041e7d4af7849\" width=\"240\"><img src=\"https://camo.qiitausercontent.com/da64457dcc8cd66bc959e02fed5e42961fd84da1/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3136393538392f39383336373436332d626631382d623838392d633463652d6433373339303935386630632e706e67\" width=\"240\"><br>"],"metadata":{"id":"VnMbao0o7sxh"}},{"cell_type":"markdown","source":["# <font color=\"silver\">誤差逆伝播│ReLU型</font><br>\n","> - <font color=\"silver\">Description</font><br>\n",">  - ReLU型<br>\n",">     - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://www.researchgate.net/figure/ReLU-activation-function-vs-LeakyReLU-activation-function_fig2_358306930)</font></font><br><img src=\"https://www.researchgate.net/publication/358306930/figure/fig2/AS:1119417702318091@1643901386378/ReLU-activation-function-vs-LeakyReLU-activation-function.png\" width=\"480\"><br><br>\n",">     - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://coding-yoon.tistory.com/132)</font></font><br><img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fc5OSH2%2FbtqVFCjbt1m%2Fy64HkW61TdJAZ1xBU3jHz1%2Fimg.png\" width=\"640\"><br><br>\n",">     - Description<br>\n","         - 通常の Relu では、x≦0 の領域で、勾配が 0 になるために、学習を進めることが出来なくなってしまうのに対し、Leaky Relu では、x≦0 の領域でも、勾配が 0 とはならないので、学習を進めることができる<br>\n","         - Leaky-ReLUは、GANやVAEなどの生成分野に使われる。<br><br>\n",">     - ReLU<br><br>\n","$ y = \\left\\{\n","  \\begin{array}{cc}\n","    x & (x \\gt 0) \\\\\n","    0 & (x \\le 0)\n","  \\end{array}\n","\\right. \\qquad \\cfrac{\\partial y}{\\partial x} = \\left\\{\n","  \\begin{array}{cc}\n","    1 & (x \\gt 0) \\\\\n","    0 & (x \\le 0)\n","  \\end{array}\n","\\right.$<br><br>\n","または、$ y = \\max(0, x)$<br><br>\n",">     - Leaky-ReLU<br><br>\n","$ y = \\left\\{\n","  \\begin{array}{cc}\n","    x & (x \\gt 0) \\\\\n","    \\alpha x & (x \\le 0)\n","  \\end{array}\n","\\right. \\qquad \\cfrac{\\partial y}{\\partial x} = \\left\\{\n","  \\begin{array}{cc}\n","    1 & (x \\gt 0) \\\\\n","    \\alpha & (x \\le 0)\n","  \\end{array}\n","\\right.$<br><br>\n","または、$ y = \\max(0.1x, x)$<br><br>\n",">     - Numpy実装<br><br>\n","x = np.array([-2, -1, 0, 1, 2, 3])<br>\n","mask = (x <= 0) <font color=\"silver\"> # array([ True,  True,  True, False, False, False])</font><br>\n","x[mask] = 0 <font color=\"silver\"> # array([0, 0, 0, 1, 2, 3])</font><br>\n","<font color=\"red\">dout = np.array([1, 1, 1, 1, 1, 1])<br>\n","dout[mask] = 0</font><font color=\"silver\"> # array([0, 0, 0, 1, 1, 1])</font><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/kwi0303/items/e43efa6657ff4b6b59ad)</font></font><br><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F1529393%2F8a8ced0f-43a9-52fc-14a2-8bbbadd14e1e.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=ec2f2dd521b8f79de86641c5710dd823\" width=\"640\">"],"metadata":{"id":"X0UzBjRoA9Gp"}},{"cell_type":"markdown","source":["# <font color=\"silver\">誤差逆伝播│Sigmoid型</font>\n"," >  - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://coding-yoon.tistory.com/132)</font></font><br><img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbL2BKo%2FbtqVIatb61f%2FRUwb8Jt9NOqC5pQz9KJeC1%2Fimg.png\" width=\"640\"><br>\n",">  - Sigmoid関数<br><br>\n","$ y = \\cfrac{1}{1 + e^{-x}}$<br><br>\n","$ \\cfrac{\\partial y}{\\partial x} = y(1-y)$<br><br>\n","$  \\cfrac{\\partial L}{\\partial x} = \\cfrac{\\partial L}{\\partial y}\\cfrac{\\partial y}{\\partial x} = \\cfrac{\\partial L}{\\partial y}y(1-y)$<br><br>\n","out = 1 / (1 + np.exp(-x))<br>\n","<font color=\"red\">dx = dout * (1.0 - out) * out</font><br><br>\n","$\\quad\\overset{x}{\\longrightarrow}\\quad\n","{\\rm sigmoid}\\quad\n","\\overset{y}{\\longrightarrow}$<br><br>\n","$\\overset{\\frac{\\partial L}{\\partial y}y(1-y)}{\\Longleftarrow}\n","\\quad{\\rm sigmoid}\\quad\n","\\overset{\\frac{\\partial L}{\\partial y}}{\\Longleftarrow}$<br><br>\n",">  - Tanh関数<br><br>\n","$ y = \\cfrac{\\sinh x}{\\cosh x} = \\cfrac {e^x - e^{-x}} {e^x + e^{-x}}\n","  = \\cfrac{e^{2x} - 1} {e^{2x} + 1}$<br><br>\n","$ \\cfrac{\\partial y}{\\partial x} = 1 - y^2$<br><br>\n","$  \\cfrac{\\partial L}{\\partial x} = \\cfrac{\\partial L}{\\partial y}\\cfrac{\\partial y}{\\partial x} = \\cfrac{\\partial L}{\\partial y}1 - y^2$<br><br>\n","$\\quad\\overset{x}{\\longrightarrow}\\quad\n","{\\rm tanh}\\quad\n","\\overset{y}{\\longrightarrow}$<br><br>\n","$\\overset{\\frac{\\partial L}{\\partial y}(1-y^2)}{\\Longleftarrow}\n","\\quad{\\rm tanh}\\quad\n","\\overset{\\frac{\\partial L}{\\partial y}}{\\Longleftarrow}$<br><br>\n","out = np.tanh(x)<br>\n","<font color=\"red\">dx = dout * (1.0 - out ** 2)</font><br><br>\n",">  - 計算グラフ<br><br>\n","$\\scriptsize/：y = \\cfrac{1}{x}\\qquad\\cfrac{∂y}{∂x}=-y^2$<br><br>\n","$\\scriptsize{\\rm exp}：y=e^x\\qquad\\cfrac{∂y}{∂x}=e^x$<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/kwi0303/items/e43efa6657ff4b6b59ad)</font></font><br>\n","<img src=\"https://camo.qiitausercontent.com/909ba915324bf98753519587673d4dc449a80ea6/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313532393339332f31636261353432302d653034392d313033312d356532632d3961646134633133613238642e706e67\" width=\"640\"><br><br>\n","<img src=\"https://camo.qiitausercontent.com/e84afbcbafaa3874dad51f3f8c900a96b4d29b91/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313532393339332f35336633326137362d343939652d653465312d353338632d6466363337636266376664312e706e67\" width=\"240\"><br><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F1529393%2F247dd585-4bde-64d5-a96c-e0dc167b0e3b.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=f459ca0ced93e3ba18a7e26014eb8ac4\" width=\"240\"><br><br>\n","><font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://prog-you.com/relu-sigmoid/)</font></font><br>\n","<img src=\"https://prog-you.com/syohyou/wp-content/uploads/2022/09/b30f13e277ff8d357f5c46e146b8b601-1024x223.jpg\" width=\"480\"><br>\n","<img src=\"https://prog-you.com/syohyou/wp-content/uploads/2022/09/93b531d91cdf580bc45c581082a02035-1024x232.jpg\" width=\"480\"><br>\n","$\\scriptsize/：y = \\cfrac{1}{x}\\qquad\\cfrac{∂y}{∂x}=-y^2$<br><br>\n","$\\scriptsize{\\rm exp}：y=e^x\\qquad\\cfrac{∂y}{∂x}=e^x$<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/kuroitu/items/221e8c477ffdd0774b6b)</font></font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F640911%2F1c90cbfd-8ec1-2932-50cb-b2daa8bce545.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=bc263381e749805a600483c358e17029\" width=\"480\"><br>"],"metadata":{"id":"z-8_KwAxztDs"}},{"cell_type":"markdown","source":["# <font color=\"silver\">誤差逆伝播│Softmax</font>\n","> - <font color=\"silver\">Description</font><br>\n"," - 入力の各要素すべてに同じスカラーを加えてもソフトマックスの出力は変わらない、すなわち、ソフトマックス関数は入力の差のみに依存する。よって、入力$x_i$と$x_j$に対するソフトマックス関数の値の比は、入力の差$x_i - x_j$のみに依存する。<br><br>\n"," - この性質を用いると、オーバーフロー対策として、入力から入力の最大値 を減じることで、ソフトマックス関数が数値的に安定する。この 場合、入力から入力の最大値を減じた値は常に0以下を取り、指数関数を 適用したあとの最大値はe=1となるため、オーバーフローが起きなくなる。<br><br>\n"," - $ y_{n,k} = \\cfrac{\\exp(a_{n,k})}{\\sum_{k=1}^{K} \\exp(a_{n,k})} \\qquad \\cfrac{dy_i}{d x_j}=\\left\\{\\begin{array}{ll} y_i(1-y_j)&i=j\\\\ -y_iy_j&i\\neq j \\end{array}\\right.$<br><br>\n","$\\delta_{ij}=\\begin{cases} \n","  \\ 1 　　(i=k)\\\\ \n","  \\ 0　　 (i\\neq k) \n","\\end{cases} \\qquad \\cfrac{\\partial y_i}{\\partial x_j}=y_i(\\delta_{ij}-y_j)$<br><br>\n",">  - x = np.exp(x) / np.sum(np.exp(x))<br>\n","<font color=\"red\">dx = out * dout</font><br>\n","<font color=\"red\">sumdx = np.sum(dx, axis=1, keepdims=True)</font><br>\n","<font color=\"red\">dx -= out * sumdx</font><br><br><font>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/kwi0303/items/e43efa6657ff4b6b59ad)</font></font><br>\n","<img src=\"https://camo.qiitausercontent.com/4ae72835cc359f923fdd538bbf60f25fc7652728/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313532393339332f30386633313031612d313837302d393262362d393764322d3265303361343865366665322e706e67\" width=\"640\"><br><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F1529393%2F3b0ade38-d3a5-83da-9733-c1eb0fec5ddb.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=2b73d489c67e6e752dd4e4e40f1a0347\" width=\"800\"><br><br>"],"metadata":{"id":"LUp3MiVOz2Y_"}},{"cell_type":"markdown","source":["# <font color=\"silver\">誤差逆伝播│SoftmaxWithLoss</font>\n","> - <font color=\"silver\">Description</font><br>\n"," - $ L = - \\cfrac{1}{N}\\displaystyle\\sum_{n=1}^{N}\\sum_{k=1}^{K} t_{nk} \\log y_{nk}$<br><br>\n","$\\scriptsize\\begin{bmatrix}\n","1 \\\\\n","0 \\\\\n","\\vdots \\\\\n","0\n","\\end{bmatrix}^T\n","\\times\n","\\ \\begin{bmatrix}\n","0.6 \\\\\n","0.01 \\\\\n","\\vdots \\\\\n","0.4\n","\\end{bmatrix} = -1 \\times \\log 0.6 = 0.51$<br><br><br><br>\n"," - $\\overset{x}{\\longrightarrow}\n","{\\rm sigmoid}\n","\\overset{t,y}{\\longrightarrow}\n","{\\rm Binary Cross Entropy}\n","\\overset{\\mathtt{L}}{\\longrightarrow}$<br><br>\n","$\\overset{\\frac{\\partial L}{\\partial x}= \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial x} = y-t}{\\Longleftarrow}\n","{\\rm sigmoid}\n","\\overset{\\frac{\\partial L}{\\partial y} = -\\frac{t}{y} + \\frac{1 - t}{1 - y}}{\\Longleftarrow}\n","{\\rm Binary Cross Entropy}\n","\\overset{\\mathtt{1}}{\\Longleftarrow}$<br><br>\n","$\\scriptsize\\displaystyle\\frac{\\partial L}{\\partial x}= \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial x} =  -\\frac{t}{y} + \\frac{1 - t}{1 - y}･y(1-y)= y-t$<br><br><br><br>\n"," - $\\overset{\\mathtt{a}}{\\longrightarrow}\n","{\\rm softmax}\n","\\overset{t,y}{\\longrightarrow}\n","{\\rm Cross Entropy}\n","\\overset{\\mathtt{L}}{\\longrightarrow}$<br><br>\n","$\\overset{\\frac{\\partial L}{\\partial x}= \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial x} = y-t}{\\Longleftarrow}\n","{\\rm softmax}\n","\\overset{\\frac{\\partial L}{\\partial y} = -\\frac{t}{y}}{\\Longleftarrow}\n","{\\rm Cross Entropy}\n","\\overset{\\mathtt{1}}{\\Longleftarrow}$<br><br>\n","$\\scriptsize\\displaystyle\\frac{\\partial L}{\\partial x}= \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial x} =  -\\frac{t}{y}･\\frac{\\partial y}{\\partial x}= y-t$<br><br><br><br>\n"," - $\\overset{x}{\\longrightarrow}\n","{\\rm Liner}\n","\\overset{t,y}{\\longrightarrow}\n","{\\rm Mean Squared Error}\n","\\overset{\\mathtt{L}}{\\longrightarrow}$<br><br>\n","$\\overset{\\frac{\\partial L}{\\partial x}= \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial x} = y-t}{\\Longleftarrow}\n","{\\rm Liner}\n","\\overset{\\frac{\\partial L}{\\partial y} = y-t}{\\Longleftarrow}\n","{\\rm Mean Squared Error}\n","\\overset{\\mathtt{1}}{\\Longleftarrow}$<br><br>\n","$\\scriptsize\\displaystyle\\frac{\\partial L}{\\partial x}= \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial x} = y-t･1= y-t$<br><br><br><br>\n","><font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://shephexd.github.io/deep%20learning/2019/01/16/Deep_learning%284%29-Back_propagation.html)</font></font><br>\n","<img src=\"https://shephexd.github.io/assets/images/articles/DeepLearning/softmax_with_loss.png\" width=\"320\"><br><br>\n","><font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/kwi0303/items/e43efa6657ff4b6b59ad)</font></font><br>\n","<img src=\"https://camo.qiitausercontent.com/2c0dc659bcfb529a0cedc4eae99b720a17cd2ef8/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313532393339332f36306138366533322d316534362d336537622d356265662d3764323339643364656361352e706e67\" width=\"800\"><br><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F1529393%2F3b0ade38-d3a5-83da-9733-c1eb0fec5ddb.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=2b73d489c67e6e752dd4e4e40f1a0347\" width=\"800\"><br><br>"],"metadata":{"id":"qXPFFW8Sz5hg"}},{"cell_type":"markdown","source":["# <font color=\"silver\">誤差逆伝播│Batch Norm</font>\n","> - <font color=\"silver\">Description</font><br>\n",">   - Batch Normのグラフ</font><br>\n","><img src=\"https://kratzert.github.io/images/bn_backpass/BNcircuit.png\" width=\"640\"><br>\n",">  - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://konchangakita.hatenablog.com/entry/2021/01/12/210000)<br></font>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/k/konchangakita/20210111/20210111154320.png\" width=\"640\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://rikei-logistics.com/batch-normalization1)<br></font>\n","<img src=\"https://rikei-logistics.com/wp-content/uploads/2021/11/1-3-5.png\" height=\"160\">\n","<img src=\"https://rikei-logistics.com/wp-content/uploads/2021/11/1-5-5.png\" height=\"160\"><br>\n","<img src=\"https://rikei-logistics.com/wp-content/uploads/2021/11/1-6-5.png\" width=\"320\"><br><br>\n",">$\\displaystyle \\widehat{x}_i=\\frac{x_i-\\mu_{\\mathcal{B}}}{\\sqrt{\\sigma^2_{\\mathcal{B}}+\\epsilon}}  \\quad \\displaystyle y_i=\\gamma\\widehat{x}_i+\\beta $ <br><br>\n","<font color=\"black\">mu = np.mean(x, axis=0) <font color=\"silver\"> # 入力xをNの方向に平均 (D, )<br></font>\n","mu = np.broadcast_to(mu, (N, D)) <font color=\"silver\"> # Nの方向にブロードキャスト (N, D)<br></font>\n","x_mu = x - mu <font color=\"silver\"> # 入力xから平均値を引く (N, D)<br></font>\n","var = np.mean(x_mu ** 2, axis=0) <font color=\"silver\"> # 入力xの分散 (D, )  <br></font>\n","std = np.sqrt(var + epsilon) <font color=\"silver\"> # 入力xの標準偏差 (D, ) <br></font>\n","std_inv = 1 / std <font color=\"silver\"> # 標準偏差の逆数 (D, ) <br></font>\n","std_inv = np.broadcast_to(std_inv, (N, D)) <font color=\"silver\"> # Nの方向にブロードキャスト (N, D)  <br></font>\n","x_std = x_mu * std_inv <font color=\"silver\"> # 標準化 (N, D)<br></font>\n","out = self.gamma * x_std + self.beta <font color=\"silver\"> # gammaでスケール、betaでシフト (N, D)<br></font><br>\n","out = self.gamma * x_std + self.beta <font color=\"silver\"> # gammaでスケール、betaでシフト (N, D)<br></font>\n","<font color=\"red\">dbeta = np.sum(dout, axis=0) </font><font color=\"silver\"> # (D, )<br></font></font>\n","<font color=\"red\">dgamma = np.sum(self.x_std * dout, axis=0)<font color=\"silver\"># (D, )<br></font></font>\n","<font color=\"red\">dx_std = self.gamma * dout  </font><font color=\"silver\">  # (N, D)<br></font></font>\n","x_std = x_mu * std_inv <font color=\"silver\"> # 標準化 (N, D)<br></font>\n","<font color=\"red\">dx_mu_1 = dx_std / std  </font><font color=\"silver\"> # a2, Xmuの勾配(1つ目)<br></font></font>\n","<font color=\"red\">dstd_inv = dx_std * x_mu  </font><font color=\"silver\">  # a3, 標準偏差の逆数の勾配<br></font></font>\n","std_inv = np.broadcast_to(std_inv, (N, D)) <font color=\"silver\"> # Nの方向にブロードキャスト (N, D)  <br></font>\n","<font color=\"red\">dstd_inv = np.sum(dstd_inv, axis=0)  </font><font color=\"silver\">  #  a3, Nの方向に合計<br></font></font>\n","std_inv = 1 / std <font color=\"silver\"> # 標準偏差の逆数 (D, ) <br></font>\n","<font color=\"red\">dstd = -(dstd_inv) / (std * std)  </font><font color=\"silver\">  #   a4, 標準偏差の勾配<br></font></font>\n","std = np.sqrt(var + epsilon) <font color=\"silver\"> # 入力xの標準偏差 (D, ) <br></font>\n","<font color=\"red\">dvar = 0.5 * dstd / std  </font><font color=\"silver\">  # a5, 分散の勾配<br></font></font>\n","var = np.mean(x_mu**2, axis=0) <font color=\"silver\"> # 入力xの分散 (D, )  <br></font>\n","<font color=\"red\">a6 = dvar / batch_size  </font><font color=\"silver\">  # Xmuの2乗の勾配<br></font></font>\n","<font color=\"red\">a6 = np.broadcast_to(a6, (N, D))  </font> </font><font color=\"silver\"> # Nの方向にブロードキャスト\" <br></font></font>\n","<font color=\"red\">dx_mu_2 = 2.0  * x_mu * a6  </font><font color=\"silver\">  # a7, Xmuの勾配(2つ目)<br></font></font>\n","x_mu = x - mu <font color=\"silver\"> # 入力xから平均値を引く (N, D)<br></font>\n","<font color=\"red\">dmu = -(dx_mu_1+dx_mu_2)  </font><font color=\"silver\"> # a8, a2+a7, muの勾配<br></font></font>\n","mu = np.broadcast_to(mu, (N, D)) <font color=\"silver\"> # Nの方向にブロードキャスト (N, D)<br></font>\n","<font color=\"red\">dmu = np.sum(dmu, axis=0)  </font><font color=\"silver\">   a8, # Nの方向に合計<br></font></font>\n","<font color=\"black\">mu = np.mean(x, axis=0) <font color=\"silver\"> # 入力xをNの方向に平均 (D, )<br></font>\n","<font color=\"red\">a9 = dmu / batch_size  </font><font color=\"silver\">  # Xの勾配<br></font></font>\n","<font color=\"red\">a9 = np.broadcast_to(a9, (N, D))  </font><font color=\"silver\">  # Nの方向にブロードキャスト<br></font></font>\n","<font color=\"red\">dx = a2 + a7 + a9<br></font>"],"metadata":{"id":"zd25R5p90jCO"}},{"cell_type":"markdown","source":["# <font color=\"silver\">誤差逆伝播│Attention</font>\n","> - <font color=\"silver\">Description</font><br>\n"," - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/ac86f1992b7beefa1f0c)</font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F1a019fc5-7cc8-5c24-f896-78c5ac20b478.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=7999c583133fcf245a0449025b3cbaa2\" width=\"640\"><br><br>\n","hr = h.reshape(N, 1, H).repeat(T, axis=1)<br>\n","t = hs × hr<br>\n","s = np.sum(t, axis=2)<br>\n","a = softmax.forward(s)<br>\n","ar = a.reshape(N, T, 1).repeat(H, axis=2)<br>\n","t = hs * ar<br>\n","c = np.sum(t, axis=1)<br>\n","<br>\n","c = np.sum(t, axis=1)<br>\n","<font color=\"red\">dt = dc.reshape(N, 1, H).repeat(T, axis=1)</font><br>\n","t = hs * ar<br>\n","<font color=\"red\">dar = dt * hs</font><br>\n","<font color=\"red\">dhs = dt * ar</font><br>\n","ar = a.reshape(N, T, 1).repeat(H, axis=2)<br>\n","<font color=\"red\">da = np.sum(dar, axis=2)</font><br>\n","a = softmax.forward(s)<br>\n","<font color=\"red\">ds = softmax.backward(da)</font><br>\n","s = np.sum(t, axis=2)<br>\n","<font color=\"red\">dt = ds.reshape(N, T, 1).repeat(H, axis=2)</font><br>\n","t = hs × hr<br>\n","<font color=\"red\">dhs = dt * hr</font><br>\n","<font color=\"red\">dhr = dt * hs</font><br>\n","hr = h.reshape(N, 1, H).repeat(T, axis=1)<br>\n","<font color=\"red\">dh = np.sum(dhr, axis=1)<br></font>\n","\n","\n"],"metadata":{"id":"Las9UfpXDXem"}},{"cell_type":"markdown","source":["# <font color=\"silver\">誤差逆伝播│Embedding</font>\n","> - <font color=\"silver\">Description</font><br>\n"," - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/04a9adc2857f2a403cab)</font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F76cfed43-9b2b-bc02-b059-72b21c385747.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=9a1881b63d6af1673c509a4298e7a444\" width=\"240\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://take-tech-engineer.com/zero-deep-learning/)</font><br>\n","<img src=\"https://take-tech-engineer.com/wp-content/uploads/2022/04/3-768x644.png\" width=\"480\"><br>\n","<img src=\"https://take-tech-engineer.com/wp-content/uploads/2022/04/4-768x345.png\" width=\"480\"><br><br>\n","<font color=\"black\">out = W [idx]<br>\n","<font color=\"red\">dW.fill(0)</font><font color=\"silver\"># 配列の全ての要素に0を代入する</font><br>\n","<font color=\"red\">np.add.at(dW, idx, dout)</font><br>"],"metadata":{"id":"wkShCBeBf78c"}},{"cell_type":"markdown","source":["# <font color=\"silver\">誤差逆伝播│RNN</font>\n",">  - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/35f6f0d26f9e58f01e4e)</font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F19a3800d-3b64-913e-9ae4-71afde545a8e.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=7297e80d594fe5f8a5038f1251e11b9d\" width=\"640\"><br><br>\n","$h_{(t)} = \\tanh(W_{x}^T \\cdot x_{(t)} + W_{h}^T \\cdot h_{(t-1)} + b)$<br><br>\n","t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b<br>\n","h_next = np.tanh(t)<br><br>\n","h_next = np.tanh(t)<br>\n","<font color=\"red\">dt = dh_next * (1 - h_next ** 2)</font><br>\n","t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b<br>\n","<font color=\"red\">db = np.sum(dt, axis=0)<br>\n","dWh = np.dot(h_prev.T, dt)<br>\n","dh_prev = np.dot(dt, Wh.T)<br>\n","dWx = np.dot(x.T, dt)<br>\n","dx = np.dot(dt, Wx.T)</font><br>"],"metadata":{"id":"8tISKSTPmL14"}},{"cell_type":"markdown","source":["# <font color=\"silver\">誤差逆伝播│LSTM</font>\n","> - <font color=\"silver\">Description</font><br>\n",">  - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/35f6f0d26f9e58f01e4e)</font><br>\n","<img src=\"https://camo.qiitausercontent.com/bd2fad9749a2dd322a1dd9f8dcfbcf3ee209107a/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f62366631386438372d613436662d643762302d303435382d3733656163373434663465662e706e67\" width=\"640\"><br><br>\n","$\\mathbf{F}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xf} + \\mathbf{H}_{t-1} \\mathbf{W}_{hf} + \\mathbf{b}_f)$<br>\n","$\\mathbf{I}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xi} + \\mathbf{H}_{t-1} \\mathbf{W}_{hi} + \\mathbf{b}_i)$<br>\n","$\\mathbf{O}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xo} + \\mathbf{H}_{t-1} \\mathbf{W}_{ho} + \\mathbf{b}_o)$<br>\n","$\\tilde{\\mathbf{C}}_t = \\text{tanh}(\\mathbf{X}_t \\mathbf{W}_{xc} + \\mathbf{H}_{t-1} \\mathbf{W}_{hc} + \\mathbf{b}_c)$<br>\n","$\\mathbf{C}_t = \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t$<br>\n","$\\mathbf{H}_t = \\mathbf{O}_t \\odot \\tanh(\\mathbf{C}_t)$<br><br>\n","N, H = h_prev.shape<br>\n","A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b<br>\n","f = A[:, :H]<br>\n","g = A[:, H:2H]<br>\n","i = A[:, 2H:3H]<br>\n","o = A[:, 3H:]<br>\n","f = sigmoid(f)<br>\n","g = np.tanh(g)<br>\n","i = sigmoid(i)<br>\n","o = sigmoid(o)<br>\n","c_next = f * c_prev + g * i<br>\n","tanh_c_next = np.tanh(c_next)<br>\n","h_next = o * tanh_c_next<br><br>\n","<img src=\"https://camo.qiitausercontent.com/bd2fad9749a2dd322a1dd9f8dcfbcf3ee209107a/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f62366631386438372d613436662d643762302d303435382d3733656163373434663465662e706e67\" width=\"640\"><br><br>\n","><font color=\"black\">tanh_c_next = np.tanh(c_next)<br></font>\n","<font color=\"black\">h_next = o * tanh_c_next<br></font>\n","<font color=\"red\">A2 = (dh_next * o) * (1 - tanh_c_next ** 2)<br></font>\n","<font color=\"red\">ds = dc_next + A2<br></font>\n","<font color=\"red\">do = dh_next * tanh_c_next<br></font>\n","<font color=\"black\">c_next = f * c_prev + g * i<br></font>\n","<font color=\"red\">dc_prev = ds * f<br></font>\n","<font color=\"red\">di = ds * g<br></font>\n","<font color=\"red\">df = ds * c_prev<br></font>\n","<font color=\"red\">dg = ds * i<br></font>\n","<font color=\"black\">i = sigmoid(i)<br></font>\n","<font color=\"black\">f = sigmoid(f)<br></font>\n","<font color=\"black\">o = sigmoid(o)<br></font>\n","<font color=\"black\">g = np.tanh(g)<br></font>\n","<font color=\"red\">di *= i * (1 - i)<br></font>\n","<font color=\"red\">df *= f * (1 - f)<br></font>\n","<font color=\"red\">do *= o * (1 - o)<br></font>\n","<font color=\"red\">dg *= (1 - g * 2)<br></font>\n","<font color=\"black\">A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b<br></font>\n","<font color=\"black\">f = A[:, :H]<br></font>\n","<font color=\"black\">g = A[:, H:2H]<br></font>\n","<font color=\"black\">i = A[:, 2H:3H]<br></font>\n","<font color=\"black\">o = A[:, 3H:]<br></font>\n","<font color=\"red\">dA = np.hstack((df, dg, di, do))<br></font>\n","<font color=\"red\">dWh = np.dot(h_prev.T, dA)<br></font>\n","<font color=\"red\">dWx = np.dot(x.T, dA)<br></font>\n","<font color=\"red\">db = dA.sum(axis=0)<br></font>\n","<font color=\"red\">dx = np.dot(dA, Wx.T)<br></font>\n","<font color=\"red\">dh_prev = np.dot(dA, Wh.T)</font><br><br>\n",">  - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/35f6f0d26f9e58f01e4e)</font><br>\n","<img src=\"https://camo.qiitausercontent.com/bd2fad9749a2dd322a1dd9f8dcfbcf3ee209107a/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f62366631386438372d613436662d643762302d303435382d3733656163373434663465662e706e67\" width=\"640\"><br><br>\n","h_next = o * tanh_c_next<br>\n","<font color=\"red\">dtanh_c_next = dh_next * o</font><br>\n","<font color=\"red\">do = dh_next * tanh_c_next</font><br>\n","tanh_c_next = np.tanh(c_next)<br>\n","<font color=\"red\">dc_next = dtanh_c_next * (1.0 - tanh_c_next ** 2)</font><br>\n","<font color=\"red\">dc_next = (dh_next * o) * (1.0 - tanh_c_next ** 2)</font><br>\n","c_nextはc_nextとh_nextに分岐<br>\n","<font color=\"red\">ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)<br></font>"],"metadata":{"id":"XJn_RegypZbb"}},{"cell_type":"markdown","source":["# <font color=\"silver\">誤差逆伝播│Affine</font>\n","> - <font color=\"silver\">Description</font><br>\n",">   - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/poisuke/items/33d2a454a6ebd1908aee)</font></font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F188565%2Ffe94ac75-4de5-43c7-a90a-314a7c15389b.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=eae50b12629e93ad3786d5ff3185c261\" width=\"480\"> <br><br>\n",">   - $\\scriptsize\\pmb{Y} = \\pmb{X} \\pmb{W}+ \\pmb{B} \\qquad \\cfrac{\\partial L}{\\partial \\pmb{X}}= \\cfrac{\\partial L}{\\partial \\pmb{Y}}\\pmb{W}^{\\top} \\qquad \\cfrac{\\partial L}{\\partial \\pmb{W}}= \\pmb{X}^{\\top} \\cfrac{\\partial L}{\\partial \\pmb{Y}}  \\qquad \\cfrac{\\partial L}{\\partial \\pmb{B}} = \\cfrac{\\partial L}{\\partial \\pmb{Y}}$<br><br>\n",">   - out = np.dot(x, W) + b<br>\n","<font color=\"red\">dx = np.dot(dout, W.T)<br>\n","dW = np.dot(x.T, dout)<br>\n","db = np.sum(dout, axis=0)</font><br><br>\n",">   - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/kwi0303/items/e43efa6657ff4b6b59ad)</font></font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F1529393%2F67c46004-6db9-5dc0-6ca5-d67ffba67fa8.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=b4fc6e6ca6b968f2b5102dc3dd22d067\" width=\"320\">\n"],"metadata":{"id":"lWZntlGC3S_a"}},{"cell_type":"markdown","source":["# <font color=\"silver\">誤差逆伝播│appendix, 逆伝播実装①</font>\n","> out = 1 / (1 + np.exp(-x))<br>\n","<font color=\"red\">dx = dout * (1.0 - out) * out</font><br>\n","x = np.exp(x) / np.sum(np.exp(x))<br>\n","<font color=\"red\">dx = out * dout</font><br>\n","<font color=\"red\">sumdx = np.sum(dx, axis=1, keepdims=True)</font><br>\n","<font color=\"red\">dx -= out * sumdx</font><br>\n","y = softmax(x)<br>\n","loss = cross_entropy_error(y, t)<br>\n","<font color=\"red\">dx = (y - t) / batch_size<br></font>\n","h_next = np.tanh(t)<br>\n","<font color=\"red\">dt = dh_next * (1 - h_next ** 2)</font><br>"],"metadata":{"id":"zDrcYpv3mh7h"}},{"cell_type":"markdown","source":["# <font color=\"silver\">誤差逆伝播│appendix, 逆伝播実装➁</font>\n","> x = np.array([-2, -1, 0, 1, 2, 3])<br>\n","mask = (x <= 0) <font color=\"silver\"> # array([ True,  True,  True, False, False, False])</font><br>\n","x[mask] = 0 <font color=\"silver\"> # array([0, 0, 0, 1, 2, 3])</font><br>\n","<font color=\"red\">dout = np.array([1, 1, 1, 1, 1, 1])<br>\n","dout[mask] = 0</font><font color=\"silver\"> # array([0, 0, 0, 1, 1, 1])</font><br>\n","out = np.max(col, axis=1) <font color=\"silver\"> # (N×OH×OW×C, )<br></font>\n","<font color=\"red\">dcol = np.zeros((dout.size, pool_size))<br></font>\n","<font color=\"red\">dcol[np.arange(dcol.shape[0]), self.arg_max] = dout.flatten()</font><font color=\"silver\"> # 最大値となった場所にdoutを配置する<br></font>\n","<font color=\"black\">out = W [idx]<br>\n","<font color=\"red\">dW.fill(0)</font><font color=\"silver\"># 配列の全ての要素に0を代入する</font><br>\n","<font color=\"red\">np.add.at(dW, idx, dout)</font><br>"],"metadata":{"id":"u-ZIJJXVKjn3"}},{"cell_type":"markdown","source":["# <font color=\"silver\">誤差逆伝播│appendix, 逆伝播実装③</font>\n",">out = np.dot(x, W) + b<br>\n","<font color=\"red\">dx = np.dot(dout, W.T)<br>\n","dW = np.dot(x.T, dout)<br>\n","db = np.sum(dout, axis=0)</font><br>\n","t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b<br>\n","<font color=\"red\">db = np.sum(dt, axis=0)<br>\n","dWh = np.dot(h_prev.T, dt)<br>\n","dh_prev = np.dot(dt, Wh.T)<br>\n","dWx = np.dot(x.T, dt)<br>\n","dx = np.dot(dt, Wx.T)</font><br>"],"metadata":{"id":"sYlZY5UHe_2C"}},{"cell_type":"markdown","source":["# <font color=\"silver\">評価 [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!5772&cid=b0f01606242a6ed3&CT=1670918310043&OR=ItemsView)"],"metadata":{"id":"IoX4rvKnfSxe"}},{"cell_type":"markdown","source":["# <font color=\"silver\">評価│情報量基準</font>\n","> - <font color=\"silver\">Description</font><br>\n",">  - AIC\n",">    - AICは、新たに得られるデータの予測の良さをもとに、モデルの良さを決める指標<br>\n",">    - モデルのパラメータ数でペナルティをつけてモデルを選ぶことで過剰適合しないモデルを選択する<br><br>\n","$AIC = -2 \\log L + 2k$\n","<br><br>\n","<font color=\"black\">$L$：<font color=\"silver\">最大尤度<br>\n","<font color=\"black\">$k$：<font color=\"silver\">パラメータ数\n","<br><br></font></font></font></font>\n",">  - BIC\n",">    - BICは、候補として用意した統計モデルに対し、真のモデルである確率が高いモデルが良いモデル\n","<br><br>\n","<font color=\"black\">$BIC = -2 \\log L + k \\log(n)$\n","<br><br>\n","<font color=\"black\">$L$：<font color=\"silver\">最大尤度<br>\n","<font color=\"black\">$k$：<font color=\"silver\">パラメータ数<br>\n","<font color=\"black\">$n$：<font color=\"silver\">サンプルサイズ\n","<br><br>\n"],"metadata":{"id":"Mk_gHhyr7zzo"}},{"cell_type":"markdown","source":["# <font color=\"silver\">評価│混合行列</font>\n","\n",">|    |  実際にはポジティブ（P）  |  実際にはネガティブ（N）  |\n","| :----: | :----: | :----: |\n","|  予測がポジティブ（P） |TP|FP|\n","|  予測がネガティブ（N） |FN|TN|\n","\n","\n",">|    |  迷惑なメールだった（P）  |  普通のメールだった（N）  |\n","| :----: | :----: | :----: |\n","|  迷惑メールに分類（P） |迷惑メールと的中（TP）|迷惑メールと誤認（FP）|\n","|  普通メールに分類（N） |普通メールと誤認（FN）|普通メールと的中（TN）|\n","\n","\n",">正解率, $\\scriptsize\\rm{Accuracy} = \\cfrac{TP + TN}{TP + TN + FP + FN}= \\cfrac{迷惑メールと的中したもの}{すべてのメール}$\n","<br><br>\n","適合率, 誤検知に強い, $\\scriptsize\\rm{Precision} = \\cfrac{TP}{TP + FP}  = \\cfrac{迷惑メールと的中したもの}{迷惑メールに分類したもの}$\n","<br><br>\n","再現率, 見逃しに強い, $\\scriptsize\\rm{Recall} = \\cfrac{TP}{TP + FN}= \\cfrac{迷惑メールと的中したもの}{迷惑メールだったもの}$\n","<br><br>\n","$\\scriptsize\\rm{F1₋score} = \\cfrac{2 \\times \\textrm{recall} \\times \\textrm{precision}}{\\textrm{recall} + \\textrm{precision}} = \\cfrac{2 TP}{2 TP+FP+FN} $\n","<br><br>\n","Precision（正解と判断したものが本当に正解か）は正直どうでもいい。精密検査をすればいいだけ。それよりもRecallが1より小さい方がまずい。ガンじゃないと判断して、ガンだったら訴訟モノなので、是が非でもRecallを1に持っていきたい。"],"metadata":{"id":"zyGHL-ZM74S4"}},{"cell_type":"markdown","source":["# <font color=\"silver\">評価│集合の類似度</font>\n","> - <font color=\"silver\">Description</font><br>\n","><font color=\"Blue\">$\\Tiny\\text{link}$ [<font color=\"SteelBlue\">…</font>](https://mieruca-ai.com/ai/jaccard_dice_simpson/)<br></font>\n"," - NMS, Non-Maximum Suppression, 非極大値抑制 <br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://cvml-expertguide.net/terms/dl/object-detection/non-maximum-suppression/)</font></font><br>\n","<img src=\"https://i0.wp.com/cvml-expertguide.net/wp-content/uploads/2022/08/07e652a1b89611f1c0aa98bc50c80ac6.png?resize=768%2C317&ssl=1\" width=\"640\"><br> \n"," - GT boundign boxとPredicted bounding box<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://techplay.jp/event/846663)</font></font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F191313%2Ff651a053-dd86-b14e-1214-4491387c99e6.jpeg?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=280c5057c285c6b8013c836650884993\" width=\"240\"><br><br>\n","\n","|    |  実際にはポジティブ（P）  |  実際にはネガティブ（N）  |\n","| :----: | :----: | :----: |\n","|  予測がポジティブ（P） |TP|FP|\n","|  予測がネガティブ（N） |FN|TN|\n","\n","<br>\n","\n","|    |  迷惑なメールだった（P）  |  普通のメールだった（N）  |\n","| :----: | :----: | :----: |\n","|  迷惑メールに分類（P） |迷惑メールと的中（TP）|迷惑メールと誤認（FP）|\n","|  普通メールに分類（N） |普通メールと誤認（FN）|普通メールと的中（TN）|\n","\n","<br>\n","\n","|    |  IoU ≥ 0.5（P）  |  IoU < 0.5（N）  |\n","| :----: | :----: | :----: |\n","|  Predicted BBoxに対してGround-truth BBoxが（P） |十分に重なっている（TP）|重ならなかったPrediction Bboxの数（FP）|\n","|  Ground-truth BBoxに対してPredicted BBoxが（N） |重ならないことは予測していないので無し|重ならなかったGT Bboxの数（TN）|\n","\n","<br>\n","\n","<font color=\"Blue\">Jaccard係数</font><br><br>\n","Jaccard係数, $\\rm{IoU} =  \\cfrac{TP}{TP + FP + FN} $<br><br>\n","${\\rm IoU(S_{true}, S_{pred})\n","=\\cfrac{|S_{true}\\cap S_{pred}|}{|S_{true} \\cup S_{pred}|}\n","=\\cfrac{|S_{true}\\cap S_{pred}|}{|S_{true}|+|S_{pred}|- |S_{true}\\cap S_{pred}|}}$<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://marshmallow444.github.io/tech_blog/2021/12/21/deep-learning-4.html)</font></font><br>\n","<img src=\"https://ml.1book.info/cv/example_of_confusion_matrix_2_4.jpg\" width=\"640\"><br><br>\n","<font color=\"Blue\">Dice係数</font><br>\n","2つの面積の平均に対する重なり部分の面積の割合<br><br>\n","Dice係数, $\\rm{F1₋score} = \\cfrac{2 \\times \\textrm{recall} \\times \\textrm{precision}}{\\textrm{recall} + \\textrm{precision}} = \\cfrac{2 TP}{2 TP+FP+FN} = \\cfrac{TP}{TP + \\frac{1}{2}(FP + FN)}$\n","<br><br>\n","${\\rm Dice(S_{true}, S_{pred})=\\cfrac{2|S_{true}\\cap S_{pred}|}{|S_{true}|+|S_{pred}|}\n","}$<br><br>\n","<font color=\"Blue\">Simpson係数</font><br>\n","${\\rm Simpson(S_{true}, S_{pred})=\\cfrac{|S_{true}\\cap S_{pred}|}{\\min(|S_{true}|,|S_{pred|})}\n","}$<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://mieruca-ai.com/ai/jaccard_dice_simpson/)</font></font><br>\n","<img src=\"https://mieruca-ai.com/ai/wp-content/uploads/2018/05/fig01.png\" Height=\"160\"><img src=\"https://mieruca-ai.com/ai/wp-content/uploads/2018/05/fig03.png\" Height=\"160\"><img src=\"https://mieruca-ai.com/ai/wp-content/uploads/2018/05/fig05.png\" Height=\"160\"><br><br>"],"metadata":{"id":"h6h1F9fheYzq"}},{"cell_type":"markdown","source":["# <font color=\"silver\">評価│ROC曲線とPR曲線</font>\n","\n","|    |  実際にはポジティブ（P）  |  実際にはネガティブ（N）  |\n","| :----: | :----: | :----: |\n","|  予測がポジティブ（P） |TP|FP|\n","|  予測がネガティブ（N） |FN|TN|\n","\n","<br>\n","\n","|    |  迷惑なメールだった（P）  |  普通のメールだった（N）  |\n","| :----: | :----: | :----: |\n","|  迷惑メールに分類（P） |迷惑メールと的中（TP）|迷惑メールと誤認（FP）|\n","|  普通メールに分類（N） |普通メールと誤認（FN）|普通メールと的中（TN）|\n","\n","<br>\n","$~~~$真陽性率, $\\scriptsize\\rm{TPR}= \\cfrac{TP}{TP + FN}= \\cfrac{迷惑メールと的中したもの}{迷惑メールだったもの}$\n","<br><br>\n","$~~~$偽陽性率, $\\scriptsize\\rm{FPR} = \\cfrac{FP}{FP + TN}= \\cfrac{迷惑メールと誤認したもの}{普通メールだったもの}$\n","<br><br>\n","\n","| ランキング | 真実 | TPR=Recall | FPR | Precision|\n","| :------: | :------: | :------: | :------: | :------: |\n","|1|1|1÷3=0.333|0.000|1.000|\n","|2|0|1÷3=0.333|0.250|0.500|\n","|3|1|2÷3=0.666|0.250|0.666|\n","|4|1|3÷3=1.000|0.250|0.750|\n","|5|0|3÷3=1.000|0.250|0.600|\n","|6|0|3÷3=1.000|0.250|0.500|\n","|7|0|3÷3=1.000|0.250|0.286|\n","\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/skyshk/items/016cd1820650ea78d101)</font></font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F51256%2F1c77dedc-bce7-46eb-05ca-5e745c890522.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=6e3792eeee199decaf4b8cb997396145\" width=\"320\">\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F51256%2Fdfd20912-ea95-9a8e-b1b9-4b2b1a6be97a.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=dd9c26a928a4ffd02ca83cf3326a121d\" width=\"320\">\n","\n"],"metadata":{"id":"j1cSYTia78qa"}},{"cell_type":"markdown","source":["# <font color=\"silver\">評価│mAP</font>\n","<font color=\"Blue\">$\\Tiny\\text{link}$ [<font color=\"SteelBlue\">…</font>](https://qiita.com/cv_carnavi/items/08e11426e2fac8433fed) [<font color=\"SteelBlue\">…</font>](https://ml.1book.info/cv/example_of_confusion_matrix_2.html)</font><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/DeepTama/items/aab46729d2aa51a8954d)</font></font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F1412761%2Fca067391-d8b3-5809-31d9-ff8ce089fa57.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=062de0e66da7edc41929bb6e78b39ce7\" width=\"800\">\n","<br><br>\n","\n","|    |  実際にはポジティブ（P）  |  実際にはネガティブ（N）  |\n","| :----: | :----: | :----: |\n","|  予測がポジティブ（P） |TP|FP|\n","|  予測がネガティブ（N） |FN|TN|\n","\n","<br>\n","\n","|    |  迷惑なメールだった（P）  |  普通のメールだった（N）  |\n","| :----: | :----: | :----: |\n","|  迷惑メールに分類（P） |迷惑メールと的中（TP）|迷惑メールと誤認（FP）|\n","|  普通メールに分類（N） |普通メールと誤認（FN）|普通メールと的中（TN）|\n","\n","<br>\n","\n","|    |  IoU ≥ 0.5（P）  |  IoU < 0.5（N）  |\n","| :----: | :----: | :----: |\n","|  Predicted BBoxに対してGround-truth BBoxが（P） |十分に重なっている（TP）|重ならなかったPrediction Bboxの数（FP）|\n","|  Ground-truth BBoxに対してPredicted BBoxが（N） |重ならないことは予測していないので無し|重ならなかったGT Bboxの数（TN）|\n","\n","<br>\n","\n","$~~~$$\\scriptsize{\\rm Precision} = \\cfrac{\\rm TP}{\\rm TP + FP}= \\cfrac{それまで見てきた十分に重なっている{\\rm True}の数}{それまで見てきた{\\rm Predicted BBoxの総数}}$\n","<br><br>\n","$~~~$$\\scriptsize{\\rm Recall} = \\cfrac{\\rm TP}{\\rm TP + FN}= \\cfrac{それまで見てきた十分に重なっている{\\rm True}の数}{全ての{\\rm GT BBox}の数}$\n","<br><br>\n","\n","| Sorted Number | Confidence Score(%) | Correct? | Precision | Recall |\n","|------------------:|:-----------------------:|:------------:|:-------------:|:----------:|\n","|                 #1|           96            |     True     |    1/1 = 1    | 1/5 = 0.2|  \n","|                 #2|           92            |     True     |    2/2 = 1    | 2/5 = 0.4 |\n","|                 #3|           89            |     False    |    2/3 = 0.667| 2/5 = 0.4 |\n","|                 #4|           88            |     False    |    2/4 = 0.5  | 2/5 = 0.4 |\n","|                 #5|           84            |     False    |    2/5 = 0.4  | 2/5 = 0.4 |\n","|                 #6|           83            |     True     |    3/6 = 0.5  | 3/5 = 0.6 |\n","|                 #7|           80            |     True     |    4/7 = 0.571| 4/5 = 0.8 |\n","|                 #8|           78            |     False    |    4/8 = 0.5  | 4/5 = 0.8 |\n","|                 #9|           74            |     False    |    4/9 = 0.444| 4/5 = 0.8 |\n","|                #10|           72            |     True     |    5/10 = 0.5| 5/5 = 1.0 |\n","\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/skyshk/items/016cd1820650ea78d101)</font></font><br>\n","<img src=\"https://camo.qiitausercontent.com/258ee1e3dc7cdd87faa0f8f011fb0a4b0cad2956/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3139313331332f31656539653364392d353762332d656464642d313461642d3532633432333361366231342e6a706567\" width=\"640\">\n","<br><br>\n","$\\begin{align}\n","\\rm{AP}\n","&= \\displaystyle\\int_0^1 p(r) dr\\\\\n","&= \\cfrac{1}{11}*(p(0) + p(0.1) + ・・・ + p(1.0))\\\\\n","&= \\frac{1}{11}*(1.0 * 5 + 0.571 * 4 + 0.5 * 2) = \\frac{1}{11}*8.284 = 0.753\n","\\end{align}$\n","<br><br>\n","$\\rm AP_{StopSign} = 0.753,\\;AP_{TrafficLight} = 0.990,\\;AP_{Car} = 0.683\\;$のとき\n","<br><br>\n","$\\rm mAP = \\frac{1}{3} * (0.753 + 0.990 + 0.683) = 0.8086$\n","\n"],"metadata":{"id":"tCuh92x88A3k"}},{"cell_type":"markdown","source":["# <font color=\"silver\">評価│コサイン類似度</font>\n","※ コサイン類似度とは、ベクトルどうしの類似度を測る指標。</font><br>\n","※ 埋め込みベクトルどうしの類似度を測る際にも使用される。</font><br><br>\n","$d(\\pmb{x}, \\pmb{y}) = \\cfrac{x\\,y}{||x||\\,||y||} = \\cfrac{x_1y_1 + x_2y_2 + \\cdots + x_ny_n}{\\sqrt{x_1^2\\cdots+x_n^2}\\sqrt{y_1^2\\cdots+y_n^2}}$<br>\n","<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://yolo-kiyoshi.com/2020/08/26/post-2234/)</font></font><br>\n","<img src=\"https://yolo-kiyoshi.com/wp/wp-content/uploads/2020/08/20200825_cosine-1.jpeg\" width=\"320\">\n"],"metadata":{"id":"K9TIrUgS8FKx"}},{"cell_type":"markdown","source":["# <font color=\"silver\">評価│パープレキシティ</font>\n","<font color=\"silver\">perplexity, パープレキシティ</font><br><br>\n","※ 言語モデルを評価するための指標。<br>\n","※ 言語モデルは次の単語を明確に判別できる方が精度が高いとみなせる</font><br>\n","※ perplextityは理論的には「次の単語として絞り込めた候補単語数」と言える</font><br>\n","→ perplexityが低いほど、単語を絞り込めているのでいいモデル</font><br>\n","<br>\n","※ 確率の逆数という意味を持っており、正解となる単語を選ぶ難しさ、と解釈できる。<br>\n","※ $y$ = 0.1のとき、その確率の逆数は10。これは、 10個の単語候補の中から正解\n","となる単語を見つけるくらい難しい、と解釈できる。<br>\n","※ 正解の単語に対応する出力値(確率)を$y$とすると、$e^{-\\log y}=(e^{\\log y})^{-1}=\\cfrac{1}{y}$であるため、パープレキシティは、確率の逆数という意味になる。<br><br>\n","$\\mathrm{perplexity} = \\exp(-\\log y_k) = y_k^{-1} = \\cfrac{1}{y_k}$<br><br>\n","\n","※ Hはある文の単語あたりのエントロピーである。</font><br>\n","※ cross entropyに対して最適化することとperplexityに対して最適化することは同じ。</font><br>\n","※ 1以上の値をとり、値が小さいほどモデルの性能が良い。</font><br><br>\n","$\\displaystyle \\text{ppl}=\\exp\\left(-\\frac{1}{N}\\sum_{n}\\sum_k t_{n,k}\\log p_\\text{model}(y_{n, k})\\right)$ \n","<br>\n","<font color=\"silver\">$\\text{ppl}=\\exp\\left(-\\frac{1}{N}\\sum_{n} \\log (\\text{正解となる単語の予測確率})\\right)$ </font>\n","<br><br>\n","$\\displaystyle \\begin{align} \\text{ppl}&=\\exp (L)\\\\ L&=-\\frac{1}{N}\\sum_{n}\\sum_k t_{n,k}\\log p_\\text{model}(y_{n, k}) \\end{align}$<br>\n","<br>\n","y = np.arange(0.1, 1.01, 0.01)<br>\n","L = - np.log(y)<br>\n","perplexity = np.exp(L)<br>"],"metadata":{"id":"F7pBei-98Jii"}},{"cell_type":"markdown","source":["# <font color=\"silver\">評価│BLEU</font>\n","<font color=\"silver\">BLEU(bilingual evaluation understudy)</font><br>\n","> - <font color=\"silver\">Description</font><br>\n",">  - BLEU\n",">    - 翻訳タスクの指標、機械翻訳文が人手で作成された正解翻訳文にどれだけ近いかを測る<br>\n",">    - BLEUスコアが0であるとき、機械翻訳文に人手翻訳文と一致する部分がない(品質が低い)ことを意味する。<br>\n",">    - BLEUスコアが1のとき、機械翻訳文が人手翻訳文と完全に一致している(品質が高い)ことを意味する。<br><br>\n","$\\displaystyle{\\rm BLEU}={\\rm BP}\\exp\\left(\\sum_{n=1}^N w_n\\log p_n\\right)$\n","<br><br>\n","$\\displaystyle{\\rm BLEU}=\\min\\Big(1, \\exp\\big(1-\\frac{\\text{r}}{\\text{c}}\\big)\\Big)\\exp\\left(\\frac{1}{N}\\sum_{n=1}^N \\log p_n\\right)$<br><br>\n","$\\rm BP$：<font color=\"silver\">翻訳文が短い文のときに$P_n$が高くなってしまうことに対するペナルティ</font><br>\n","$w_n$：<font color=\"silver\">重み, 1/4とすることが多い</font><br>\n","$N$：<font color=\"silver\">N-gramまで考慮する, 4とすることが多い</font><br>\n","$p_n$：<font color=\"silver\">修正されたn-gramの精度, 適合率</font><br>\n","$c$：<font color=\"silver\">candidate, 翻訳タスクなどによって生成された文の長さ</font><br>\n","$r$：<font color=\"silver\">reference, 正解の文の長さ</font><br><br>\n","    - Brevity Penalty</font><br>\n","      - 機械翻訳文が短かいほど、BLEUが良くなる傾向があるため、短いことに対してペナルティを課す。\n","      - $P_n$は適合率であるが、このペナルティによって、再現率を考慮していると解釈することもできる。\n","      - 最も近い参照訳の長さと比較して、生成された翻訳が短すぎる場合に指数関数的減衰を使用してペナルティを課す。<br><br>\n","$\\text{BP} \n","\\begin{cases} \n","  1 & c > r \\\\ \n","  e^{1-r/c} & c \\leqq  r \n","\\end{cases}$<br><br>\n","    - nグラム適合率, Modified Precision</font><br>\n","      - 機械翻訳文中の単語が人手翻訳文中に入っていると、適合率が高くなる。<br>\n","      - 1 グラム、2 グラム、3 グラム、4 グラム（i=1,...,4）が参照訳内の対応する n グラムと一致する数をカウントする。\n","      - 1 グラムは適切さを評価し、それよりも長い n グラムは翻訳の流暢さを評価する。\n","      - オーバーカウントを避けるために、n グラムのカウントは、参照訳に出現する n グラムの最大カウント（$m_{ref}^n$）に切り詰める。<br><br>\n","$\\displaystyle p_n=\\frac{\\sum_{C\\in\\{\\rm Candidates\\}}\\sum_{\\text{n-gram}\\in C}{\\rm Count}_{\\rm clip}(\\text{n-gram})}{\\sum_{C'\\in \\{\\rm Candidates\\}}\\sum_{\\text{n-gram}'\\in C'}{\\rm Count}(\\text{n-gram}')}$<br><br>\n","<img src=\"https://machinelearninginterview.com/wp-content/uploads/2021/11/image-3.png\" width=\"320\"><br><br>\n","<img src=\"https://i0.wp.com/kikaben.com/wp-content/uploads/2022/04/image-372.png?w=800&ssl=1\" width=\"480\"><br><br>\n","<img src=\"https://i0.wp.com/kikaben.com/wp-content/uploads/2022/04/image-374.png?w=800&ssl=1\" width=\"480\"><br><br>\n","<img src=\"https://i0.wp.com/kikaben.com/wp-content/uploads/2022/04/image-371.png?w=800&ssl=1\" width=\"480\"><br><br>\n","<img src=\"https://i0.wp.com/kikaben.com/wp-content/uploads/2022/04/image-367.png?w=800&ssl=1\" width=\"480\"><br>\n","<img src=\"https://i0.wp.com/kikaben.com/wp-content/uploads/2022/04/image-366.png?w=800&ssl=1\" width=\"480\"><br><br>\n","    - BLEUの<font color=\"Blue\">課題</font><br>\n","      - <font color=\"Blue\">コーパスベースの指標</font><br>\n","        - BLEUスコアは、個々の文の評価に使用してもうまく機能しないことがわかって\n","おり、通常はコーパス全体で評価する。<br>\n","        - 異なるコーパス間や言語間でBLEUスコアを比較することは推奨されていない。<br>\n","        - たとえば、例の 2 つの文はほとんどの意味をとらえていますが、どちらの BLEU スコアも低くなっています。個々の文の n グラム統計はあまり意味がないので、BLEU はコーパスベースの指標として設計されています。つまり、スコアを計算する際には、コーパス全体で統計が収集されます。上記で定義した BLEU 指標は個々の文に対して因数分解できません。<br>\n","      - <font color=\"Blue\">内容語と機能語が区別されない</font><br>\n","        - 「a」や「your」のような機能語が抜けることは、文意を理解する上でそれほど重\n","要でないが、「Tokyo」と「Kyoko」を間違うことと、等価に扱われてしまう。<br>\n","        - 「a」のような機能語が抜けている語句の場合、「NASA」という名前が誤って「ESA」に置換された場合と同じペナルティが課せられます。<br>\n","      - <font color=\"Blue\">文の意味や文法の正しさまでは十分に評価できない</font><br>\n","        -  「not」のような単語が 1 つ抜けると、文の意味が正反対になるが、BLEUスコア\n","ではそれを考慮できない。<br>\n","        -  また、n が 4 以下の n グラムのみを考慮すると、長い範囲の依存性が無視されるため、BLEU は、文法に合わない文に小さいペナルティしか課さないことがよくあります。<br>\n","      - <font color=\"Blue\">正規化とトークン化<br></font>\n","        -  BLEU スコアを計算する前に、参照訳と候補訳の両方が正規化され、トークン化されます。正規化とトークン化の手順の選択は、最終的な BLEU スコアに大きな影響を与えます。\n"],"metadata":{"id":"eWK2LpPN8PVS"}},{"cell_type":"markdown","source":["# <font color=\"silver\">評価│appendix, BLEUの数式比較</font>\n","$\\displaystyle\\text{BLEU} = \\underbrace{\\vphantom{\\prod_i^4}\\min\\Big(1, \\exp\\big(1-\\frac{\\text{reference-length}}{\\text{output-length}}\\big)\\Big)}_{\\text{brevity penalty}}\\underbrace{\\Big(\\prod_{i=1}^{4} precision_i\\Big)^{1/4}}_{\\text{n-gram overlap}}$\n","<br><br>\n","$\\displaystyle precision_i = \\dfrac{\\sum_{\\text{snt}\\in\\text{Cand-Corpus}}\\sum_{i\\in\\text{snt}}\\min(m^i_{cand}, m^i_{ref})}\n"," {w_t^i = \\sum_{\\text{snt'}\\in\\text{Cand-Corpus}}\\sum_{i'\\in\\text{snt'}} m^{i'}_{cand}}$<br><br>\n","$m_{cand}^i\\hphantom{xi}$：<font color=\"silver\">参照訳と一致する候補訳内の$i$グラムのカウント</font><br>\n","$m_{ref}^i\\hphantom{xxx}$：<font color=\"silver\">参照訳内の$i$グラムのカウント</font><br>\n","$w_t^i\\hphantom{m_{max}}$：<font color=\"silver\">候補訳内の$i$グラムの総数</font><br><br>\n","$P_n = \\cfrac{\\sum(正解の文での{\\rm n-gram}の共有数の最大値)}{生成された文中の{\\rm n-gram}数}$</font><br><br>"],"metadata":{"id":"76s1zlKywrUg"}},{"cell_type":"markdown","source":["# <font color=\"silver\">評価│appendix, 例題</font>\n","<font color=\"Blue\">$\\Tiny\\text{link}$ [<font color=\"SteelBlue\">…</font>](https://cloud.google.com/translate/automl/docs/evaluate?hl=ja#examples)<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://ameblo.jp/javaee7/entry-12551006649.html)</font></font><br>\n","<img src=\"https://stat.ameba.jp/user_images/20191201/06/javaee7/65/34/p/o0537069414655434716.png?caw=800\" width=\"480\">"],"metadata":{"id":"9Q44P__za-fP"}},{"cell_type":"markdown","source":["# <font color=\"silver\">評価│appendix, 一覧</font>\n","|名称|性能の良い方向|メリット|デメリット|\n","|---|---|---|---|\n","|**Accuracy**|大きいほどGood|適合率と再現率のバランスが見られる|不均衡データに弱い|\n","|**Precision**|大きいほどGood|偽陽性（見すぎ）を評価できる|Recallとトレードオフ|\n","|**Recall**|大きいほどGood|見逃しを評価できる|Precisionとトレードオフ|\n","|**F1-score**|大きいほどGood|不均衡データに強い|PR-AUCより汎化性に劣る|\n","|**LogLoss**|小さいほどGood|正誤の度合いを連続的に評価できる|他データとの性能比較できず＆確率算出が必要|\n","|**AUC**|大きいほどGood|他データとの性能比較ができる|計算量多い＆確率算出が必要|\n","|**PR-AUC**|大きいほどGood|不均衡データに強い|計算量多い＆確率算出が必要|"],"metadata":{"id":"HKUVUPv89qPA"}},{"cell_type":"markdown","source":["# <font color=\"silver\">データ拡張 [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!6354&cid=b0f01606242a6ed3&CT=1671850360626&OR=ItemsView)"],"metadata":{"id":"5vsTggm1uw8f"}},{"cell_type":"markdown","source":["# <font color=\"silver\">データ拡張│標準化・正規化</font>\n","> - <font color=\"silver\">Description</font><br>\n",">  - <font color=\"Blue\">標準化</font><br>\n",">     - 標準化は、データの平均と標準偏差を用いた前処理のこと<br>\n",">     - 各変数ごとに平均と標準偏差を求め、以下の式によって標準化する<br>\n",">     - テスト用データの標準化に用いる平均・標準偏差は、学習用データで求めた値を用いる<br>\n",">     - 外れ値が存在する場合<br>\n",">     - 回帰モデルを構築する際は一般的に用いられる<br><br>\n","$\\cfrac{X-μ}{σ}$\n","<br><br>\n","    def fit_transform(self, X):<br>\n","$\\quad$        self.mean_ = X.mean(axis=0)<br>\n","$\\quad$        self.std_ = X.std(axis=0)<br>\n","$\\quad$        Xsd = (X - self.mean_) / self.std_<br>\n","$\\quad$        return Xsd<br>\n","    def transform(self, X):<br>\n","$\\quad$        Xsd = (X - self.mean_) / self.std_<br>\n","$\\quad$        return Xsd<br>\n","    def inverse_transform(self, Xsd):<br>\n","$\\quad$        X = (Xsd * self.std_) + self.mean_<br>\n","$\\quad$        return X<br><br>\n",">  - <font color=\"Blue\">正規化</font><br>\n",">     - 正規化は、データのスケールを0から1の間に収めるための前処理<br>\n",">     - 各変数ごとに最大値と最小値を求めて正規化する<br>\n",">     - テスト用データの正規化に用いる最小値・最大値は学習用データで求めた値を用いる<br>\n",">     - 外れ値に影響されやすいので注意<br>\n",">     - 画像処理におけるRGBの強さを扱う場合<br>\n","<br>\n","$\\cfrac{X-X_{min}}{X_{max}-X_{min}}$<br><br>\n",">  - <font color=\"Blue\">無相関化</font><br>\n",">  - <font color=\"Blue\">白色化</font><br>"],"metadata":{"id":"Q3O5cL2gwBrc"}},{"cell_type":"markdown","source":["# <font color=\"silver\">データ拡張│エンコーディング</font>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://di-acc2.com/programming/python/3737/)</font></font><br>\n","<img src=\"https://di-acc2.com/wp-content/uploads/2021/04/fca32cbcf58c27a46897c6fc29450d26.jpg\" width=\"480\">"],"metadata":{"id":"k5Qw4DTTxuNk"}},{"cell_type":"markdown","source":["# <font color=\"silver\">データ拡張│サンプリング</font>"],"metadata":{"id":"uzStnD1Ixz-r"}},{"cell_type":"markdown","source":["# <font color=\"silver\">データ拡張│時系列データ</font>"],"metadata":{"id":"MbgF-6vZx3DL"}},{"cell_type":"markdown","source":["# <font color=\"silver\">データ拡張│Crop</font>\n","><font color=\"black\">torchvision.transforms.RandomCrop(  <font color=\"silver\">ランダムに領域を切り取る<br>\n","$\\quad$<font color=\"black\">size, <font color=\"silver\">パディング幅<br>\n","$\\quad$<font color=\"black\">padding=None, <font color=\"silver\">切り抜く大きさ<br>\n","$\\quad$<font color=\"black\">pad_if_needed=False, <font color=\"silver\">size に指定した大きさが画像より大きい場合にパディングするかどうか<br>\n","$\\quad$<font color=\"black\">fill=0, <font color=\"silver\"> padding_mode=\"constant\" を指定した場合に使用する色<br>\n","$\\quad$<font color=\"black\">padding_mode='constant'<font color=\"silver\"> パディング方法<br>\n","$\\quad$<font color=\"black\">)<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/kurilab/items/b69e1be8d0224ae139ad)</font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F215987%2F308412ad-9d56-2490-d0b9-ba8dc51838f2.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=eac413806d725b76f4970d1db616acca\" width=\"320\"><br>\n"],"metadata":{"id":"lqNlC6JOux6n"}},{"cell_type":"markdown","source":["# <font color=\"silver\">データ拡張│Flip・Rotate</font>\n","><font color=\"black\">torchvision.transforms.RandomHorizontalFlip(p=0.5)<br>\n","torchvision.transforms.RandomVerticalFlip(p=0.5)<br>\n","torchvision.transforms.RandomRotation( <br>\n","$\\quad$<font color=\"black\">degrees, <font color=\"silver\">角度<br>\n","$\\quad$<font color=\"black\">interpolation=InterpolationMode.NEAREST, <font color=\"silver\">補間モード<br>\n","$\\quad$<font color=\"black\">expand=False, <font color=\"silver\">回転した際にすべての画素が収まるように出力画像の大きさを調整するかどうか<br>\n","$\\quad$<font color=\"black\">center=None, <font color=\"silver\">回転する中心<br>\n","$\\quad$<font color=\"black\">fill=0, <font color=\"silver\">外挿する際に使用する色<br>\n","$\\quad$<font color=\"black\">)<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/kurilab/items/b69e1be8d0224ae139ad)</font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F215987%2Fa2c32338-c9ec-3770-dd7e-d1c06f50e876.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=3cd6d873d57651eefd1ccf31a594ac06\" Height=\"80\">\n","<img src=\"https://camo.qiitausercontent.com/5b4b23aba0cf90726d54c263ea1ebec4f46198a6/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3231353938372f32343436383833322d393634332d316537372d306332322d3562316161376439626638372e706e67\" Height=\"80\">\n","<img src=\"https://camo.qiitausercontent.com/35d69be82cb25fe7e263077008cdfb3ce247abf5/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3231353938372f39383064393531312d313266372d366630332d326166632d3536353438383333316565612e706e67\" Height=\"80\"><br>"],"metadata":{"id":"7gXnWXssu0wl"}},{"cell_type":"markdown","source":["#<font color=\"silver\">データ拡張│Color Space</font><br>\n",">torchvision.transforms.ToTensor() <font color=\"silver\"> 画像のグレースケール化（RGBの0～255を0~1の範囲に正規化）</font><br>\n","torchvision.transforms.Normalize(mean, std, inplace=False) <font color=\"silver\"> Z値化（RGBの平均meanと標準偏差stdをタプルで設定して正規化）</font><br>\n","<font color=\"black\">torchvision.transforms.ColorJitter(<br>\n","$\\quad$<font color=\"black\">brightness=0, <font color=\"silver\">明るさの変動幅<br>\n","$\\quad$<font color=\"black\">contrast=0, <font color=\"silver\">コントラストの変動幅<br>\n","$\\quad$<font color=\"black\">saturation=0, <font color=\"silver\">彩度の変動幅<br>\n","$\\quad$<font color=\"black\">hue=0 <font color=\"silver\">色相の変動幅<br>\n","$\\quad$<font color=\"black\">)<br>\n","<font color=\"black\">torchvision.transforms.RandomAutocontrast(p=0.5)<br><br>\n","<font color=\"black\">ToTensor\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/kurilab/items/b69e1be8d0224ae139ad)</font><br>\n","<img src=\"https://camo.qiitausercontent.com/e3fee3eb52371dedbe01de46ac4833490031f5b8/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3231353938372f35303665393235392d346332632d653234352d666563362d6366303866366538383466382e706e67\" width=\"280\"><br>\n","<font color=\"black\">Normalize\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/kurilab/items/b69e1be8d0224ae139ad)</font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F215987%2Fb7d1ce48-ba54-4796-0774-f2b3568c3133.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=f1dbac050ca751d8e117643e18653929\" width=\"640\"><br>\n","<font color=\"black\">Brightness\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/kurilab/items/b69e1be8d0224ae139ad)</font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F215987%2F4a2edcc9-581f-87d9-5882-25b3bc2c786f.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=42e185ca0b545c9247dade62ae8e1fbc\" width=\"640\"><br>\n","<font color=\"black\">Contrast\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/kurilab/items/b69e1be8d0224ae139ad)</font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F215987%2Ff1a3b737-3b12-6158-20a9-079f4f812de6.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=8b4746e7ed851f42fb9f3aab45d7faf8\" width=\"640\"><br>\n"],"metadata":{"id":"GRkFZ6NWu8Mc"}},{"cell_type":"markdown","source":["# <font color=\"silver\">データ拡張│Erase</font><br>\n","><font color=\"black\">torchvision.transforms.RandomErasing(<br>\n","$\\quad$<font color=\"black\">p=0.5, <font color=\"silver\">Erasingされる確率<br>\n","$\\quad$<font color=\"black\">scale=(0.02, 0.33), <font color=\"silver\">Erasingされる面積を全体の面積で割った数<br>\n","$\\quad$<font color=\"black\">ratio=(0.3, 3.3), <font color=\"silver\">長方形の縦横のアスペクト比<br>\n","$\\quad$<font color=\"black\">value=0, <font color=\"silver\">Erasingした範囲に適応する値<br>\n","$\\quad$<font color=\"black\">inplace=False<br>\n","$\\quad$<font color=\"black\">)<br>\n","\n",">|    |  Cutout   |  Random Erasing|\n","| ---- | ---- | ---- |\n","|  マスクの場所  |  ランダム  | ランダム   |\n","|  マスクの大きさ  |  ランダム  | ランダム   |\n","|  マスクを行うか  | 全てにマスクをかける  |  確率pでランダムに決定  |\n","|  マスクのアスペクト比  |  正方形   | ハイパラの範囲内でランダム  |\n","|  マスクの範囲  |  ハイパラ（縦横の1/3~1/2を推奨）  | ハイパラの範囲内でランダム   |\n","|  マスクの画素  |   画素値の平均 | 0～255でランダム   |\n","\n","><img src=\"https://camo.qiitausercontent.com/08290f49c4157099dd5e13d33a99869cb8ad8236/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3133393830392f38623063663732662d303639382d666638392d656631312d6538396331363931376431342e706e67\" width=\"640\">\n","\n"],"metadata":{"id":"b8wOetYCvA3m"}},{"cell_type":"markdown","source":["# <font color=\"silver\">データ拡張│MixUp</font>\n","> ※ 2つの画像を合成して新しいサンプルを作成する手法<br>\n","> ※ モデルの学習時にはラベル側も混ぜ合わせる<br>\n","> ※ 画像の合成により、正則化の効果が生まれ、かつ、画像の中間も識別できるようになる。<br>\n","> ※ 同じラベル間でmixupしても性能向上に寄与しない (異なるラベル間でmixupする必要がある)<br>\n","> ※ 実装ではランダムサンプリングされたミニバッチ内でmixupするとラク<br>\n","<br>\n","$\\begin{align}\n","x_{mixup} &= \\lambda x_i + (1 - \\lambda ) x_j, \\\\\n","y_{mixup} &= \\lambda y_i + (1 - \\lambda) y_j, \\\\\n","\\end{align}$<br>\n","<br>\n","$\\lambda$：<font color=\"silver\">$\\lambda \\sim \\beta (\\alpha, \\alpha) \\  for \\  \\alpha \\in (0, \\infty)$, Beta分布に従う。</font><br>\n","$\\,x_{mixup}\\,$：<font color=\"silver\">入力$\\,x_i\\,$と$\\,x_j\\,$を重み$\\,λ\\,$で足し合わせて、新たな入力とする</font><br>\n","$\\,y_{mixup}\\,$：<font color=\"silver\">ラベルも同様に重み$\\,λ\\,$で足し合わせて、新たな入力する</font><br>\n"],"metadata":{"id":"OCw3GQp1vFfN"}},{"cell_type":"markdown","source":["# <font color=\"silver\">データ拡張│CutMix</font>\n"," <font color=\"silver\">これまでの課題<br>\n","> ※ Eracingは、マスク部分を塗りつぶすため、ロス計算やバッチ正則化の観点で学習の効率が低下。 <br>\n","> ※ mixupは、異なる画像をピクセル単位で足し合わせてしまうため、局所的には不自然で曖昧な入力となり、物体検出では精度改善につながらない。<br>\n"," <font color=\"silver\">CutMix<br>\n","> ※ 画像の一部を別の画像に貼り付ける<br>\n","> ※ 2枚の画像同士の面積比に従ったソフトラベル<br>\n","> ※ 学習データ同士をラベルとともに繋ぎ合わせることで、学習効率の低下を防ぎつつ入力領域のドロップアウトと同等の効果を得る一方、局所的な入力信号を維持することで物体検出にも使えるオーグメンテーションにする、というのがCutMixのモチベーション。実際、画像分類・物体検出のいずれでもmixupやCutoutよりも高い精度をマーク。<br>\n","<br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/o/ohke/20200711/20200711145211.png\" width=\"240\">\n","<br>\n","$M \\in {0, 1}^{W \\times H}$：<font color=\"silver\">マスク</font><br>\n","$\\lambda$：<font color=\"silver\">面積比で、(0, 1)の一様分布でサンプリング</font>\n","<br><br>\n","<img src=\"https://camo.qiitausercontent.com/bfc22e451af5cf9fa96aa574166d2d9e887e6734/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f34303331302f35653836326462342d666464382d316664342d333661372d6539386536373633393664352e706e67\" width=\"480\">\n","<br><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F40310%2Feec2f727-f0bd-e994-42ba-c35d127eaff6.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=1ddb19afba900eac04d917ec123063f9\" width=\"640\">"],"metadata":{"id":"mF30WxRwvJt9"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MFCC [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4736&cid=b0f01606242a6ed3&CT=1668059654634&OR=ItemsView)</font><br>\n"],"metadata":{"id":"BLMVHVLO-6r3"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MFCC│<font color=\"silver\">Description</font><br></font>\n","> - <font color=\"silver\">Description</font>\n"," - MFCC<br>\n","   - 短時間フーリエ変換, STFT, Short term Fourier transform<br>\n","     - 音声波形に短時間フーリエ変換, STFT を適用する流れは、\n","       - 時間信号波形をある窓長(e.g. 1024 サンプル)で切り出す\n","       - 切り出されたサンプルに窓関数(e.g. ハニング窓)をかける\n","       - 窓かけされた信号にフーリエ変換を施す\n","       - 時間信号波形の切り出す場所をシフトして 1. に戻る\n","     - となる。 以上の処理により、下図のように時間信号波形から周波数スペクトログラムが得られ、これが機械学習技術を適用する対象とされる。\n","     - 現実世界で得られた音声データは計算機で扱うために離散信号に変換されるため、フーリエ変換後に得られる周波数スペクトルの周波数分解能は窓長に依存する。窓長が大きくなるほど、周波数分解能は高くなる。また、サンプルから観測可能な元の連続信号の最大周波数はサンプリング周波数に等しい。この事実をナイキストの定理と呼ぶ。\n","     - サンプリング周波数を fs [Hz] 、窓長を N とすると、サンプリング周期は 1/fs [s] であり。 窓の時間長は N/fs [s] である。スペクトルの周波数間隔は窓の時間長の逆数 fs/N であるため。窓の時間長を大きくするとスペクトルの周波数間隔は小さくなり。周波数分解能は高くなります。 しかしながら、窓の時間長を大きくすると、時間分解能が低くなります。つまり、短時間フーリエ変換においては周波数分解能と時間分解能はトレードオフの関係にある。\n","     - ナイキストの定理(サンプリング定理)は信号の最大周波数が f の信号に対して、2f よりも高い周波数でサンプリングを行うと、元の信号を再現できるという定理です。例えば、40 kHz でサンプリングを行った場合、元の信号に含まれる 20 kHz までの成分は再現できますが、30kHz の信号は、30 − 20 = 10 kHz の周波数の元の信号には存在しない雑音として表れます。この現象をエイリアシングという。<br><br>\n","     - 窓長は 2 の冪乗に設定すると、高速フーリエ変換(Fast Fourier transform;FFT)により速い計算が可能となるため、一般的に窓長は 1024 や 2048 サンプルに設定される(サンプリング周波数に依存する)。 \n","     - サンプリング周波数 16 kHz の信号に 1024 サンプルの窓をかけてフーリエ変換した場合を考える。 このときに得られる周波数スペクトルは、0 Hz から 8 kHz までの、513 個の等分点における周波数情報が得られる。\n","   - メル尺度<br>\n","     - 人間の聴覚特性は、周波数に対してほぼ対数的であり、具体的にはメル尺度と呼ばれる周波数感度となっている。機械学習を用いる際にも、この知見に基づいて周波数スペクトルをメル尺度に変換したものを利用することが多い。具体的には、下図のように、線形周波数(横軸)をメル尺度(縦軸)に変換する。\n","     - 線形尺度と比べた時のメル尺度の性質は、低周波数域では分解能が高く、高周波数域では分解能が低くなる。"],"metadata":{"id":"3ZcHf078xpeO"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MFCC│標本化定理<br>\n","- <font color=\"silver\">Description</font>\n"," - サンプリング周波数\n","   - 標本化 (サンプリング) を行う際の測定周期の逆数。 1 秒間に何回波形のサンプルを取るかを表す。（例)16000 Hz など。<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://edn.itmedia.co.jp/edn/articles/1308/19/news012.html)</font><br>\n","<img src=\"https://image.itmedia.co.jp/edn/articles/1308/19/l_mm130819ti_analogvocab_fig1.jpg\" width=\"480\">\n"," - 標本化定理\n","   - 記録したい信号の 2 倍の $f_s$でサンプリングを行えば、測定したデジタル信号から元の波形を復元できる。一方で、$f_s$/2 より高い周波数の信号は記録・復元できないという定理。<br>\n","   - 2倍以下のサンプリング周波数$f_s$では元の信号を正確に再現できない（エイリアシング）。\n","   - サンプリング周波数$f_s$の半分の周波数を、元の信号が再現可能かどうかの境界としてナイキスト周波数という。<br><br>\n","   - サンプルから観測可能な元の連続信号の最大周波数は、サンプリング周波数$f_s$の 1/2 である<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://engineer-climb.com/adc-basic/) [<font color=\"silver\">…</font>](https://kudoshun.com/2021/03/26/high-resolution/)</font><br>\n","<img src=\"https://kudoshun.com/wp-content/uploads/2021/03/2021032664744.jpg\" width=\"320\"> <img src=\"https://i0.wp.com/engineer-climb.com/wp-content/uploads/2022/07/%E6%A8%99%E6%9C%AC%E5%8C%96%E5%AE%9A%E7%90%86-500x488.jpg?resize=500%2C488&ssl=1\" width=\"240\"><br>\n"],"metadata":{"id":"z-esOCgsZmJc"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MFCC│離散フーリエ変換</font><br>\n","> - <font color=\"silver\">Description</font><br>\n","  - 離散フーリエ変換\n","    - 音声信号に対し、離散フーリエ変換を行うと、周波数スペクトルが得られる<br>\n","    - フーリエ変換の結果は実部（コサイン波）と虚部（サイン波）の複素数となり、その絶対値をとったものがスペクトル。 スペクトルを時間方向に並べたものを、スペクトログラムという。<br>\n","    - 時間に対する振幅で表現されている信号を、波がどの周波数の基本波がどれくらいの割合で構成されているかという視点で表現<br>\n","    - フレーム⻑を 2 のべき乗にすると、処理を高速化できる<br>\n","<br>\n","\n",">|  時間領域  |  周波数領域  | 変換手法  |\n","| ---- | ---- | ---- |\n","|  連続  |  離散  |  フーリエ級数展開  |\n","|  連続  |  連続  |  フーリエ変換  |\n","|  離散  |  連続  |  離散時間フーリエ変換 |\n","|  離散  |  離散  |  離散フーリエ変換 (DFT: Discrete Fourier Transform)  |\n","\n"," >   - 「この音は、周波数が4Hzと10Hzと…の波が合わさった音なんだな」というような解釈ができる<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://rike-kotton.com/g%E6%A4%9C%E5%AE%9A%E3%81%AE%E5%8B%89%E5%BC%B7%EF%BD%9E%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E6%89%8B%E6%B3%95%E3%80%80%E9%9F%B3%E5%A3%B0%E5%87%A6/)</font><br>\n","<img src=\"https://rike-kotton.com/wp-content/uploads/2022/10/790f4ed593f98d398d1c73e34937bae8.png\" width=\"640\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://www.techeyesonline.com/article/tech-column/detail/Reference-FFTAnalyzer-01/)</font><br>\n","<img src=\"https://www.techeyesonline.com/img/article/23_02-2019-09-002-Y_F01.png\" width=\"480\"><br>\n"],"metadata":{"id":"2QbyhHU4AXJo"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MFCC│窓関数</font><br>\n","- 窓関数の種類<br>\n"," - 窓関数の条件\n","     - (1) 主成分（メインローブ；main-lobe）の幅が小さいこと<br>\n","       - この幅が狭いほど、主成分の周波数分解能が高い。\n","     - (2) サイドローブ（side-lobe）の振幅が小さいこと<br>\n","       - この値が小さいほど、小電力のスペクトルを検出する能力が高く、ダイナミックレンジが大きい\n","   - <font color=\"silver\">$\\tiny\\text{画像引用元}$ [<font color=\"silver\">…</font>](https://www.yamaha-ss.co.jp/published-articles/journals-09.html)</font>  [<font color=\"silver\">…</font>](https://edn.itmedia.co.jp/edn/articles/2007/14/news004_2.html)</font><br>\n","<img src=\"https://www.yamaha-ss.co.jp/published-articles/images/fir3_fig04.jpg\" width=\"320\"> <img src=\"https://image.itmedia.co.jp/edn/articles/2007/14/jn200706yokokawa04.png\" width=\"320\"><br><br>\n","     - 窓関数とは特定の有限区間の外でゼロを取るような関数の総称\n","     - 窓関数は、窓区間の信号が周期的であるという有限区間フーリエ変換の仮定に基づき、窓の両端の値を滑らかに接続するために用いる。\n","     - 切り出されたサンプルの両端の値が異なっていると、フーリエ変換は上手くいかない。そのため多くの窓関数は、元の信号との積をとった際に窓区間の両端の値が滑らかになるように設計されている。<br>\n","     - 窓関数の性質を測る指標として窓関数のパワースペクトルに現れる、メインローブの幅が挙げられる。メインローブの幅は小さいほど、周波数分解能が高い。方形窓（矩形窓）はメインローブの幅が窓関数の中で最も小さい事で知られている。しかし、同時にサイドローブ(メインローブ以外の山)の幅および振幅スペクトラムの値も非常に大きくなるため、ダイナミックレンジが非常に小さくなるという問題を抱えている。これらの問題を解決した窓関数として、Hanning 窓、Hamming 窓、Blackman 窓などがある。<br><br>"],"metadata":{"id":"lLpmAOU2Tnpw"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MFCC│窓関数処理</font><br>\n","> - <font color=\"silver\">Description</font><br>\n","  - 離散フーリエ変換と窓関数\n","    - 長い信号のスペクトル解析では、信号の一部を切り出してフーリエ変換を行う。フーリエ変換は、離散値のデータを用いざるを得ず、離散フーリエ変換となる。\n","      - フーリエ変換の第1の課題は切り出すデータの数をどのように決めるか<br>\n","        - データ数が少ないと、周波数分解能＝スペクトルの精度が低下する。一方、データ数が多いと、計算量はデータ数の2乗で増え、処理時間が急激に増える。\n","        - 窓の時間長を大きくするとスペクトルの周波数間隔は小さくなり.周波数分解能は高くなる。一方で、窓の時間長を大きくすると、時間分解能が低くなる。\n","        - 短時間フーリエ変換においては周波数分解能と時間分解能はトレードオフの関係にある。\n","      - フーリエ変換の第2の課題は、切り出したデータの両端の影響をどのように押さえるか<br>\n","        - 離散フーリエ変換では、暗黙のうちにデータの周期性が仮定されているため、右端と左端のデータ値が大きく異なると、その部分で急峻に変化しているような影響が現れ、結果として高い周波数成分（高調波成分）の歪が発生する。<br>\n"," - 部分時系列化・スライディングウィンドウ<br>\n","   - FFTを行う時の前提条件に、「信号が無限に周期的である」という条件があり、FFTでは、入力した信号を複製してつなぎ合わせます。入力信号の最後の値と最初の値は普通同じでないので、つなぎ合わせた場合には急激なギャップが生じてしまいます。「ギャップ」はいわばステップ関数なので、この部分のフーリエ変換は全周波数領域で多くのノイズを生んでしまいます(特に低周波で)。なので、コピーした時間フレームの間をいい感じにつなぎ合わせる必要があります。 窓関数をかけると、区間の両端の値が滑らかになり、周波数特徴を綺麗に取り出せます。<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$ [<font color=\"silver\">…</font>](https://rikei-fufu.com/2020/06/27/post-3237-python-fft/) [<font color=\"silver\">…</font>](https://slidesplayer.net/slide/11519754/)</font><br>\n","<img src=\"https://rikei-fufu.com/wp-content/uploads/2020/06/image-4-1024x725.png\" width=\"320\"> <img src=\"https://player.slidesplayer.net/62/11519754/slides/slide_9.jpg\" width=\"320\"><br><br>\n"],"metadata":{"id":"o77z8RpnAdNz"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MFCC│メル尺度</font><br>\n","> - <font color=\"silver\">Description</font><br>\n"," - メル尺度、\n","   - 人の音声知覚の特徴を考慮した尺度のこと。メル尺度が同じだと、人が感じる音高の差が同じになります。これは「これぐらいの音の高さ」という感覚的なものを、数字に落とし込んで計算できるようにしたものですね。メル尺度を、人が感じる音の周波数に直したものがメル周波数。グラフのように「メル尺度」と「人が感じる音の周波数」は対応しているようですね。この対応関係（メル周波数）をスペクトルに適用すると、メルスペクトルになるようです。<br>\n","<br>\n","　人間の聴覚には、高い音ほど音高（音の高さ）の変化に鈍感になるという特性があります。その知覚特性を考慮した尺度をメル尺度と呼びます。メル尺度における数値の差は、人間が感じる音高の差とほぼ同じになっています。また、スペクトルの周波数をメル尺度に変換したものをメルスペクトルと呼びます。<br><br>\n","<img src=\"https://camo.qiitausercontent.com/547ae065163dcc11f8cc2cb9729e7d7149cfb009/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3233313737372f34326531313266622d633139652d626635642d646632372d3136613731373664613365662e706e67\" width=\"480\">"],"metadata":{"id":"VmALDGGcDPjk"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MFCC│音声信号</font><br>\n","> - <font color=\"silver\">Description</font><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://www.phileweb.com/magazine/audio-course/archives/2008/04/24.html)</font><br>\n","<img src=\"https://www.phileweb.com/magazine/audio-course/uploads/22_02.jpg\" width=\"480\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://rike-kotton.com/g%E6%A4%9C%E5%AE%9A%E3%81%AE%E5%8B%89%E5%BC%B7%EF%BD%9E%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E6%89%8B%E6%B3%95%E3%80%80%E9%9F%B3%E5%A3%B0%E5%87%A6/)</font><br>\n"," - 標本化\n","   - アナログデータを一定の間隔で区切ること。大体は時間で区切り、区切った後は、区切った時間ごとの数値を一つ一つ取り出すこと。<br>\n","   <img src=\"https://rike-kotton.com/wp-content/uploads/2022/10/32eb772c2ac8f71621eee236e83c874d.png\" width=\"240\">\n"," - 量子化\n","   - 標本化されたアナログデータを離散的な値に変換すること。観測された波の強さを予め決められた値に近似すること。bit数は、大きいほど振幅を細やかに表現可能<br>\n","   <img src=\"https://rike-kotton.com/wp-content/uploads/2022/10/911ec5a15219a23e0965b8ecb57cc3bf.png\" width=\"240\">\n"," - 符号化\n","   - 量子化で得られた数値を二進数にすること。量子化では、離散的なデータになっただけで、0と1のみで表現できているわけではないため、符号化でデータを二進数に変更し0と1のみで表現する。<br>\n","   <img src=\"https://rike-kotton.com/wp-content/uploads/2022/10/1d07a19705a98a7ebba80e8beb3c0c9d.png\" width=\"320\"><br>\n","\n","PCM, パルス符号変調, Pulse Code Modulation</font><br>\n","\n","\n"],"metadata":{"id":"keYVPSM9AkP8"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MFCC│スペクトログラム</font><br>\n","> - <font color=\"silver\">Description</font><br>\n","   - 短時間フーリエ変換\n","     - 音声信号に対し、切り出した区間の信号に窓関数を掛けてから高速フーリエ変換を行うことを、短時間フーリエ変換という。<br>\n","     - 短時間フーリエ変換後のスペクトログラムは「時間・周波数・振幅」の3次元の値になる<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/shirowanisan/items/6af2cc4c4be0c57bef06)</font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F651198%2F4d3c35d2-4a16-8c83-f59a-4f2b44bcb9b0.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=430638d2de2931748cab4fc971054431\" width=\"640\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://watlab-blog.com/2019/05/19/python-spectrogram/)<br>\n","<img src=\"https://watlab-blog.com/wp-content/uploads/2019/04/wav-origin.png\" width=\"320\"><img src=\"https://watlab-blog.com/wp-content/uploads/2019/05/spect_with-a.png\" width=\"320\"><br><br>\n","<font color=\"black\">左の図は各ピーク（振幅レベルが卓越している部分）の振幅レベルが小さくなっていく（色が青くなっていく）時間がそれぞれ異なることを示す。\n","右の図は時間が経つにつれピークが周波数方向に変化していくことを示す。<br><br>\n","<img src=\"https://watlab-blog.com/wp-content/uploads/2019/05/explain_spectrogram-variation.png\" width=\"640\"><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://rikei-fufu.com/2020/06/27/post-3237-python-fft/)</font><br>\n","<img src=\"https://rikei-fufu.com/wp-content/uploads/2020/06/image-2-1024x285.png\" width=\"800\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://watlab-blog.com/2019/05/19/python-spectrogram/) [<font color=\"silver\">…</font>](https://nehori.com/nikki/2020/12/06/post-22673/)</font><br>\n","<img src=\"https://watlab-blog.com/wp-content/uploads/2019/05/explain_stft.png\" width=\"480\">"],"metadata":{"id":"ct-hSEF2AppJ"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MFCC│パワースペクトル</font><br>\n","> - <font color=\"silver\">Description</font><br>\n","   - パワースペクトル\n","     - 離散フーリエ変換によって求められた複素数の実部と虚部をそれぞれ二乗して足したものをパワースペクトルという<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://www.skillupai.com/blog/tech/as-tips-2/)</font><br>\n","<img src=\"https://www.skillupai.com/wp-content/uploads/2021/11/as-tips-2_04.png\" width=\"640\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$ [<font color=\"silver\">…</font>](https://nehori.com/nikki/2020/12/06/post-22673/) [<font color=\"silver\">…</font>](https://osdes.com/oms/sample/FFT/FFT.html)</font><br>\n","<img src=\"https://nehori.com/nikki/wp-content/uploads/2020/12/3311485_orig.gif\" width=\"320\"><img src=\"https://osdes.com/oms/sample/FFT/image/fft1.png\" width=\"320\"><br>\n","\n"],"metadata":{"id":"ScZMR8BuAuCV"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MFCC│メル帯域スペクトル</font><br>\n","> - <font color=\"silver\">Description</font><br>\n","   - メル帯域スペクトル\n","     - パワースペクトルを人間の聴覚特性を考慮した尺度であるメル尺度を用いてメルフィルタバンクを適用したものをメル帯域スペクトルという<br><br>\n","   - 対数メル帯域スペクトル\n","     - 人間の知覚は対数スケールなので、メル帯域スペクトルの対数をとったものを対数メル帯域スペクトルという<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/5rQntdiU57zH2Lg/items/c7b9814256c127a13386)<br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2668345%2F03f8c229-e4d8-9cfe-0ec4-1afc1e4bcfba.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=b3973cca89c39ff0c6b5454486c6e624\" width=\"320\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2668345%2Fb2c2d4c4-3db2-9aa4-949d-f594c6f77599.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=dc6528e001a9c9f04280d92350330c84\" width=\"320\"><br>\n","<img src=\"https://camo.qiitausercontent.com/8e3f233020084ee87adfe67207219cf6146de6b4/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f323636383334352f35613930623363352d616532392d613939642d623030382d3239363535353037356363312e706e67\" width=\"640\"><br>\n"],"metadata":{"id":"Oj90WKu7AzoX"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MFCC│メルケプストラム</font><br>\n","> - <font color=\"silver\">Description</font><br>\n","   - メルケプストラム\n","     - 周波数領域のスペクトルを時間信号とみなし、さらにフーリエ変換を施したものをケプストラムと呼びます。このことから、スペクトルのスペクトルとも呼ばれている。また、Cepstrum, ケプストラムの横軸をQuefrency, ケフレンシーという。<br><br>\n","　メルケプストラムは、ケプストラムにメル尺度を適用した特徴量になります。メル化する際に、振幅スペクトルをメルフィルタバンクで畳み込むという操作が行われます。つまり、メルフィルタバンクのフィルタの数だけ、元の振幅スペクトルの次元を減らすことができるということになります。<br><br>\n","　スペクトラムは，ほとんどの場合その低次元部分が利用されます。この低次元部分は，スペクトラムのおおまかな構造を表しているため，「スペクトル包絡」と呼ばれることもあります。つまり，周波数帯の情報を「波形」と捉えて，その低周波数部分のみを抽出することで，波形の大まかな構造を捉えようというのがケプストラムのモチベーションです。<br><br>\n","　周波数領域のおおかな構造は，人それぞれの特性を表すとされています。ですので，音声認識ではケプストラムから抽出された特徴量というのは非常に重要な役割を果たします<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://tips-memo.com/python-cepstrum)</font><br>\n","<img src=\"https://tips-memo.com/wp-content/uploads/2020/01/3_kankei.png\" width=\"480\"><br>"],"metadata":{"id":"ygaHQhXIBBEU"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MFCC│メル周波数ケプストラム係数</font><br>\n","<font color=\"silver\">メル周波数ケプストラム係数, MFCC, Mel Frequency Cepstral Coefficient</font><br>\n","> - <font color=\"silver\">Description</font><br>\n","   - メル周波数ケプストラム係数<br>\n","     - メルスペクトルからケプストラムを算出し、さらに対数をとったあとに、離散コサイン変換を行ったものを MFCC（メル周波数ケプストラム係数）と呼びます。MFCC の低い方から 12～18 次元ほどを取り出したものは、スペクトル包絡のような声道特性（話者の特徴）を表します<br><br>\n","     - <img src=\"https://research.miidas.jp/wp-content/uploads/2019/06/Screen-Shot-2019-06-19-at-12.20.03-768x542.png\" width=\"640\"><br>\n","\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://research.miidas.jp/2019/06/%E9%9F%B3%E5%A3%B0%E8%A7%A3%E6%9E%90%E3%81%AE%E3%81%9F%E3%82%81%E3%81%AE%E6%BA%96%E5%82%99%E3%82%92python%E3%81%A7%E3%82%84%E3%82%8B/)</font><br>"],"metadata":{"id":"CPOUSUSDBIFf"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MFCC│スペクトル包絡</font><br>\n","\n","> - <font color=\"silver\">Description</font><br>\n","   - スペクトル包絡<br>\n","     - 周波数スペクトルの緩やかな変動のこと。スペクトル包絡には、声の特徴 (声道特性) が表れる。MFCC (後述) の低次成分は、スペクトル包絡を表す。Cepstrum, ケプストラムの低い部分を取り出して逆フーリエ変換を行うと、スペクトル包絡（声道の特性を表す特徴量）を取り出すことができます。<br><br>\n"," <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://www.skillupai.com/blog/tech/as-tips-2/)</font><br>\n","<img src=\"https://www.skillupai.com/wp-content/uploads/2021/11/as-tips-2_05.png\" width=\"640\"><br>\n","<br>\n","　スペクトル包絡には、母音の特徴が出やすい。そのため、MFCC が音声認識の特徴量として利用される。スペクトル包絡の時間方向の変化には、子音の特徴が出やすい。 そのため、MFCC の時間方向の変化をとった ΔMFCC も、音声認識の特徴量として利用される。ΔMFCC の時間方向の変化をとった ΔΔMFCC が利用されることもある<br><br>\n","   - フォルマント<br>\n","     - フォルマントとは、スペクトル包絡における周波数のピークのこと。スペクトル包絡を求めると、いくつかの周波数でピークを迎えることがわかる<br>\n","　フォルマント周波数とは、フォルマントに対応する周波数で、フォルマント周波数に音素の特徴が表れる。<br>\n","\n","https://qiita.com/k-maru/items/4f12fd0f8344b9e093bd"],"metadata":{"id":"dIYgtnpMBN6G"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MFCC│appendix,一連の流れ①</font>\n","\n","<font color=\"Blue\">$\\tiny{\\rm Link}$ [<font color=\"Blue\">…</font>](https://rike-kotton.com/g%E6%A4%9C%E5%AE%9A%E3%81%AE%E5%8B%89%E5%BC%B7%EF%BD%9E%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E6%89%8B%E6%B3%95%E3%80%80%E9%9F%B3%E5%A3%B0%E5%87%A6/)</font><br>\n","\n","|    |    |    |\n","| ---- | ---- | ---- |\n","|  1  |  AD変換  |  音声データ, PCM  |\n","|  2  |  STFT+オーバーラップ処理+窓関数処理  |  スペクトログラム  |\n","|  3  |  FFT（複素数の実部と虚部をそれぞれ二乗して足す）  |  パワースペクトル  |\n","|  4  |  メルフィルタバンク |  メル帯域スペクトル  |\n","|  5  |  対数スケール変換 |  対数メル帯域スペクトル  |\n","|  6  |  フーリエ変換≒逆フーリエ変換  |  メルケプストラム  |\n","|  7  |  ローパスリフタ  |  低ケフレンシ領域  |\n","|  8  |  フーリエ変換≒逆フーリエ変換  |  スペクトル包絡  |\n","\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/TaroYoshino/items/861235d034243fe73241) </font><br>\n","<img src=\"https://camo.qiitausercontent.com/5d94038e6106a8b3de79a51c6897013fe64a54e9/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3539383138332f65386532366537632d656463322d633335342d616634612d3736373737386239383539642e706e67\" width=\"640\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2607561%2F6fbf4e98-db02-5245-7927-271a99dc3b3c.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=4546551be156e35e358d662c8e6e85c8) </font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2607561%2F6fbf4e98-db02-5245-7927-271a99dc3b3c.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=4546551be156e35e358d662c8e6e85c8\" width=\"320\">　**音声データ**<br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2607561%2Fd9dee4af-fcd8-5d82-c84f-b6724131faeb.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=3412873701e5ed1d067eb2b94dc2bbec\" width=\"320\">　**フーリエ変換 ➡ パワースペクトル**<br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2607561%2F31473ea4-74da-723c-9bcb-d8203c69ac3a.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=7cbb28f7fd7cb16d1e08a7fd1ea2d19a\" width=\"320\">　**対数スケール変換 ➡ 対数パワースペクトル**<br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2607561%2F802cc9fb-81a6-855d-bdf1-3f153bad4f76.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=a8c5dd34897482de9f695f8f727db4a4\" width=\"320\">　**フーリエ変換≒逆フーリエ変換 ➡ ケプストラム**<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://aidiary.hatenablog.com/entry/20120211/1328964624) </font><br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/a/aidiary/20120211/20120211203519.png\" width=\"320\">　**ローパスリフタ ➡ 低ケフレンシ領域**<br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2607561%2Fae3b58a6-6619-6eb9-6c45-9230dfa74b3b.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=1dff3d327a825e7add5051dacd4f0cb1\" width=\"320\">　**フーリエ変換≒逆フーリエ変換 ➡ スペクトル包絡**<br>"],"metadata":{"id":"ybzofw9j0-bK"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MFCC│appendix,一連の流れ➁</font>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://www.researchgate.net/figure/Data-flow-in-extraction-of-mel-scale-based-features-logMBE-MFCC-block-DCT-MFCC-and_fig4_338359052)</font><br>\n","<img src=\"https://www.researchgate.net/profile/Shefali-Waldekar/publication/338359052/figure/fig4/AS:882390817853441@1587389773562/Data-flow-in-extraction-of-mel-scale-based-features-logMBE-MFCC-block-DCT-MFCC-and.jpg\" width=\"720\"><br>"],"metadata":{"id":"bfW9wLrvt2AP"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MFCC│appendix,一連の流れ③\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/5rQntdiU57zH2Lg/items/c7b9814256c127a13386)</font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2668345%2F052da2ca-3cf9-3e95-1f27-5696cfa5bc13.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=7710f2ca54e79a15b53061cb46acb2dd\" width=\"720\"><br>"],"metadata":{"id":"SbuF0JkEtUiL"}},{"cell_type":"markdown","source":["# <font color=\"silver\">検証手法  [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!5839&cid=b0f01606242a6ed3&CT=1671373463624&OR=ItemsView)</font>"],"metadata":{"id":"2JbQ7DL4n_k2"}},{"cell_type":"markdown","source":["# <font color=\"silver\">検証手法│特徴選択</font>\n","> - <font color=\"silver\">Description</font><br>\n"," <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://aotamasaki.hatenablog.com/entry/2018/04/18/201127)</font><br>\n","①評価指標に則って、➁特徴量を1つ1つランク付けし、③上位のランクの特徴量を選択して使用する<br>\n","<img src=\"https://www.analyticsvidhya.com/wp-content/uploads/2016/11/Filter_1.png\" width=\"640\"><br><br>\n","①特徴量の組み合わせを選択し、②選択した特徴量を使用してモデルを学習させて、③性能を評価する<br>\n","<img src=\"https://www.analyticsvidhya.com/wp-content/uploads/2016/11/Wrapper_1.png\" width=\"640\"><br><br>\n","①モデルを学習させて、➁特徴量の重要度を算出し、③重要でない特徴量を削除する<br>\n","<img src=\"https://www.analyticsvidhya.com/wp-content/uploads/2016/11/Embedded_1.png\" width=\"640\"><br><br>\n",">  - フィルタ法, Filter Method<br>\n","   - データセットのみで完結する手法であり、データの性質に依存するがモデルに依存しない。そのため、どの機械学習モデルに対しても有効であり、なおかつ処理が高速。<br>\n","     - 変数の1つ1つを独立に処理していく手法<br>\n","       - ①評価指標に則って、➁特徴量を1つ1つランク付けし、③上位のランクの特徴量を選択して使用する<br>\n","       - 使用する評価指数は、カイ２乗、フィッシャー、anova、分散値など<br>\n","     - 変数間同士の関係も計算する手法<br>\n","       - 無駄な(説明変数に影響を与えない)特徴量を削除する<br>\n","       - 同じ傾向を示す(y1=x1+ay1=x1+aのような)特徴量を削除する<br>\n","       - 相関が非常に高い特徴量を削除する<br>\n","   - （例）各説明変数と目的関数との相関係数を算出し、その絶対値が大きい10個の説明変数を採用することにした<br>\n",">  - ラッパー法, Wrapper Method<br>\n","   - モデルを使用して特徴量の組み合わせを評価する。モデルを使用することででFilter Methodではわからなかった変数間の関係を見つけ、それぞれのモデルに最適な特徴量の組み合わせを探し出す。<br>\n","   - ①特徴量の組み合わせを選択し、②選択した特徴量を使用してモデルを学習させて、③性能を評価する<br>\n","   - Filter Methodと比較して、計算コストが非常に高い<br>\n","   - Filter Methodよりも精度が高い<br>\n","   - 組み合わせの探索を切り上げるための評価指標は、タスクに合わせて選択する必要がある<br>\n","   - （例）すべての説明変数を用いて線形回帰の学習を行い、各説明変数の重要度を算出し、 重要度が小さい説明変数を取り除いて、 再度、 線形回帰の学習を行う。 これを繰り返すことによって、 採用する説明変数を10個に絞った<br>\n",">  - 埋め込み法 , Emedded Method</font><br>\n",">    - 機械学習アルゴリズムの中で変数選択も同時に行ってくれる方法のこと。<br>\n",">    - 具体的には、Lasso回帰, Ridge回帰, Regularized trees, Memetic algorithm, Random multinomial logitなどがある。<br>\n",">    - Filter Methodでは計算することのできなかった変換の関係も、計算することができ、また学習時に探索するため、Wrapper Methodよりも計算コストは低い。<br>\n","   - ①モデルを学習させて、➁特徴量の重要度を算出し、③重要でない特徴量を削除する<br>\n","例）線形回帰の損失関数にL1ノルム正則化項を加えて学習を行い、その結果を用いて採用する説明変数を10個に絞った<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://i-main.net/emmanote-ai-feature-selection/)</font></font><br>\n","<img src=\"https://i-main.net/wp-content/uploads/2022/02/emmanote-ai-feature-selection-1.jpg\" width=\"640\"><br>\n"],"metadata":{"id":"8veKfqY89lZC"}},{"cell_type":"markdown","source":["# <font color=\"silver\">検証手法│交差検証法</font>\n","> - <font color=\"silver\">Description</font><br>\n",">  - ホールドアウト法</font><br>\n","    - 訓練集合<br>\n","    - 検証集合…ハイパーパラメータの検証に用いる<br>\n","    - テスト集合…モデルの汎化性能の推定に用いる<br>\n","例）データ集合を7:3に分割し、 3割のほうをテスト集合にした。残りの7割のほうをさらに8: 2に分割し、8割のほうを訓練集合、2割 のほうを検証集合にした。 得られた訓練集合を用いて訓練を行い、テスト集合を用いて汎化誤差を推定した。 検証集合は、ハイパーパラメータを決定するために使用した。<br>\n",">  - k-分割交差検証法</font><br>\n","    - データセットの数が少ない場合に用いられる<br>\n","    - ① データをk個の重複しない集合に分割し、➁ そのうちの1つをテストデータ、残りを訓練データとして、訓練、精度計算を行い、③ k回繰り返して平均を取ることで精度評価を行う。<br>\n","    - テスト用のデータを入れ替えながら、ｋ通りの組み合わせで評価を行うことができ、並列処理を実行することで、計算時間を1/ｋ短縮することができる<br>\n",">  - 一つ抜き法</font><br>\n","    - Leave One Out：一つ抜き法、ジャックナイフ法とも呼ばれ、データ全体のうち1つだけをテストデータとすることを繰り返す。Cross Validationと似ている"],"metadata":{"id":"UmrTPk4T-AXh"}},{"cell_type":"markdown","source":["# <font color=\"silver\">検証手法│ハイパラ探索</font>\n","\n","><font color=\"Blue\">グリッドサーチ</font><br>\n","<font color=\"Blue\">ランダムサーチ</font><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/c60evaporator/items/ca7eb70e1508d2ba5359#c%E3%83%99%E3%82%A4%E3%82%BA%E6%9C%80%E9%81%A9%E5%8C%96)</font></font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F610167%2Ffe52e93d-3c7a-9809-e1d3-37650957476d.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=9345b4a01b6771e2498e759d595eec40\" width=\"240\">\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F610167%2Fae2a1d44-5339-0a91-4f95-f95bab640e16.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=b319016cc3919dfaddeb46e54ec9c1bd\" width=\"240\"><br>\n","<font color=\"Blue\">ベイズ最適化</font><br>\n","<font color=\"Blue\">$\\tiny{\\rm Link}$ [<font color=\"Blue\">…</font>](https://qiita.com/c60evaporator/items/ca7eb70e1508d2ba5359#c%E3%83%99%E3%82%A4%E3%82%BA%E6%9C%80%E9%81%A9%E5%8C%96) [<font color=\"Blue\">…</font>](https://qiita.com/Bell-frontier/items/bf6425e40b0f9273cb12)</font><br>\n","　探索を始めてから現時点までの探索結果を用いて評価指標の値を予測する モデルを構築し、その予測モデルと獲得関数を用いて次の探索点を選択する方法。<br>\n","・正答率が良さそうなエリアを優先的に探索<br>\n","・局所解に陥らないように、稀にその他のエリアを探索<br>\n","・帯域的で観測ノイズを考慮した探索が可能<br>\n",">予測モデル：<font color=\"silver\">ガウス過程回帰が多く用いられる</font><br>\n",">獲得関数：<font color=\"silver\">ガウス過程回帰で得た予測値と不確かさ（分散）の情報で、次の探索点を決める関数</font><br>\n","PI：<font color=\"silver\">Probability of Improvement, （改善量はわずかであっても）改善される確率がもっとも高い点を選ぶ</font><br>\n","EI：<font color=\"silver\">Expected Improvement, 改善量の期待値が最も高い点を選ぶ</font><br>\n","UCB：<font color=\"silver\">Upper Confidence Bound, 信頼区間の上限が最も高くなる点を選ぶ。最大化が目的の時</font><br>\n","LCB：<font color=\"silver\">Lower Confidence Bound, 信頼区間の上限が最も低くなる点を選ぶ。最小化が目的の時</font><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/c60evaporator/items/ca7eb70e1508d2ba5359#c%E3%83%99%E3%82%A4%E3%82%BA%E6%9C%80%E9%81%A9%E5%8C%96)</font></font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F610167%2Fc9850209-b7b9-a4eb-626b-661398cc4b54.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=c6e3c17651356d3e4fc71c0e35ff13b6\" width=\"480\"><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://www.albert2005.co.jp/service/case/743.html)</font></font><br>\n","<img src=\"https://www.albert2005.co.jp/new/site/wp-content/uploads/2019/07/%E3%81%A7%E3%81%8D%E3%82%8B%E3%81%93%E3%81%A8%E8%A9%B3%E7%B4%B0%E3%83%9A%E3%83%BC%E3%82%B8%E7%94%A8%E7%94%BB%E5%83%8F.png\" width=\"480\"><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://www.doe-get-started.com/entry/Bayesian-Optimization)</font></font><br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/S/Sturgeon/20230121/20230121173836.png\" width=\"640\">\n","\n"],"metadata":{"id":"sJ59cETY-0Kg"}},{"cell_type":"markdown","source":["# <font color=\"Silver\">開発環境 [<font color=\"silver\">…](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!6359&cid=b0f01606242a6ed3&CT=1672288515436&OR=ItemsView)</font></font><br>"],"metadata":{"id":"vgOGwgIEz13t"}},{"cell_type":"markdown","source":["# <font color=\"Silver\">開発環境│蒸留 </font><br>\n","> - Description<br>\n"," - <font color=\"Blue\">$\\tiny{\\rm Link}$ […</font>](http://codecrafthouse.jp/p/2018/01/knowledge-distillation/)</font>\n",">  -ソフトターゲット損失</font><br>\n",">  -温度付きソフトマックス, Softmax with Temperature<br>\n"],"metadata":{"id":"I88PiLR3z2WW"}},{"cell_type":"markdown","source":["# <font color=\"Silver\">開発環境│プルーニング <br>\n","プルーニング, pruning, 剪定<br></font>\n","> - Description<br>\n"," - 精度に大きく影響を与えないノードや重みを刈りとることによって、データ量と計算負荷を削減する方法。\n","  - 剪定は、学習と同時もしくは学習後に行われます。 学習前に、剪定すべき ノードやエッジなどを知ることはできない<br>\n",">  - 宝くじ仮説</font><br>\n","   - 学習済みのモデルには、一部を切り出して同じ程度学習させても元の学習済みモデルと同程度の性能をもつ部分ネットワーク(当たりくじ)が存在する、という仮説\n","   -「剪定によって得られる疎なネットワークは、なぜ剪定を使わず に最初から学習できないのか?」という動機から研究を始め、この仮説を 提唱するに至った<br>\n"," - 手法の種類<br>\n","      - ニューロンではなく0に近い重み</font>を消していく手法<br>\n","      - 各ユニットの出力に重みをかけるというモデルに対し、その各ユニットの重みをL1正則化付き学習</font>で潰す方法\n","      - 各ユニットの出力にバイナリ値をかけるというモデルに対し、バイナリ値を進化戦略で最適化</font>する方法"],"metadata":{"id":"HP_-Gyhzz2xl"}},{"cell_type":"markdown","source":["# <font color=\"Silver\">開発環境│量子化<br></font>\n","<font color=\"Silver\">量子化, quantization<br></font>\n","> - Description<br>\n"," -  Google TPUは8ビットの計算ユニットで計算を行う\n"," -  最近では重みや中間データを1ビットで表現する手法も提案されている\n"," -  通常、32bitもしくは16bit精度の浮動小数点で表現される\n","ニューラルネットワーク内のデータを、それより少ないビット数で表現することにより、計算の高速化や省メモリ化を図ること。\n"," -  通常、量子化の対象となるデータは、パラメータ、勾配、アクティベーション(中間層の出力)である。\n"," -  学習済みモデルを何らかのハードウェアに組み込む場合は、そのハードウェアの仕様を基に削減後のビット数を決めたりする。\n","- 2値化, binarization</font><br>\n","    - 1bitで表現することを特に2値化(binarization)という。\n","    - 活性化を二値化することは、 活性化にステップ関数に似た形状の関数 ($x < 0$で$f(x)=-1,$x > 0$で $f(x)=1$となる関数) を適用することと同じ であり、ほとんどの勾配が0になるという問題が生じる。 これを回避するため、逆伝播時に、 Straight-Throughを適用すると、 勾配が0でなくなり学習が円滑に進む。 \n","- Straight-Through<br>\n","     - 逆伝播時だけ、 対象となる関数を恒等写像関数として扱う という方法。 順伝播時は通常通り計算し、 逆伝播時は活性化関数を恒 等写像関数とみなす。 恒等写像関数の微分は1であるため、誤差逆伝 指の計算としては、勾配を素通りさせることになる。 勾配を素通りさ せるので、勾配が0になることを回避できる。<br>"],"metadata":{"id":"QGCS4fe_z3Ww"}},{"cell_type":"markdown","source":["# <font color=\"Silver\">開発環境│分散処理 </font><br>\n",">\n","> - モデル並列, model parallelism<br>\n","     - 複数の計算機が1つのデータに対して共同で動作し、それぞれがモデルの異なる部分を実行すること。<br>\n","     - 高解像度画像を入力とするCNNなど、1プロセス上に載りきらないサイズのモデルを訓練したい場合などに用いられる。 <br> \n","     - 訓練時と予測時の両方で可能。<br> \n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://logmi.jp/tech/articles/285424)</font><br>\n","<img src=\"https://i0.wp.com/img.logmi.jp/wp-content/uploads/2018/05/00216.jpg\" width=\"480\"><br>\n","> - データ並列, data parallelism<br>\n","     - 入力データを分割して、複数の計算機で分担する方法。<br>\n","     - 各ワーカーは、それぞれ異なるデータを用いて勾配を計算<br>\n","     - 学習済みモデルを用いて予測する際など。<br>\n"," > - データ並列における同期型, Synchronous<br>\n","     - 各ワーカーが求めた平均勾配を求め、平均勾配を用いて各ワーカーのモデルパラメータを更新する。<br>\n","     - 常に最新の平均勾配を用いて訓練を行い、全ワーカーは常に同じパラメータを持っている。<br>\n","     - 陳腐化した勾配が発生しない。<br>\n","     - 同期更新の方が非同期更新よりも収束速度や正解精度が良い。<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://logmi.jp/tech/articles/285424)</font><br>\n","<img src=\"https://i0.wp.com/img.logmi.jp/wp-content/uploads/2018/05/00221.jpg\" width=\"480\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://logmi.jp/tech/articles/285424)</font><br>\n","<img src=\"https://i0.wp.com/img.logmi.jp/wp-content/uploads/2018/05/00342.jpg\" width=\"480\"><br>\n","> - データ並列における非同期型, Asynchronous<br>\n","     - １つのワーカーが落ちても計算を継続できる。<br>\n","     - 他のワーカーを待たなくて良いので、待ち時間が発生しない。<br>\n","     - 同期を取らずに平均勾配が更新されるため、パラメータサーバー上に古い劣悪な勾配である 陳腐化した勾配</font> より、学習が不安定になることがある。<br>\n","> - 陳腐化した勾配, stale gradient<br>\n","     - 陳腐化した勾配の影響を緩和する方法としては、 <br>\n","         - 学習率を下げる、 <br>\n","         - 陳腐化した勾配を定期的に捨てる、 <br>\n","         - ミニバッチサイズを調整する、 <br>"],"metadata":{"id":"-fTdKkxyz3_u"}},{"cell_type":"markdown","source":["# <font color=\"Silver\">GPU  [<font color=\"silver\">…](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4503&cid=b0f01606242a6ed3&CT=1669556907377&OR=ItemsView)</font></font><br>"],"metadata":{"id":"owR1mqTwQAnz"}},{"cell_type":"markdown","source":["# <font color=\"Silver\">GPU│GPU･TPU･FPGA<br>\n","GPGPU, General-Purpose computing on Graphics Processing Units<br>\n","GPU, Graphics Processing Unit<br>\n","CPU, Central Processing Unit<br>\n","SIMD, Single Instraction, Multiple Data<br>\n","TPU, Tensor Processing Unit<br>\n","FPGA, Field-Programmable Gate Arra<br><br>\n","> - <font color=\"Silver\">Description</font><br></font>\n"," - Docker<br>\n","     -  Docker Engine \n","       - <font color=\"Blue\">GPU</font><br>\n","　GPUは、ディスプレイへの描画処理を高速に行う装置として発達したプロセッサの一種。メモリにデータとプログラムを内蔵しメモリから命令を逐次取り出し実行するノイマン型プロセッサの一種である。<br>\n","　CPUに比べて、GPU はコア当たりの計算性能は低く、それぞれのコアを独立に制御することができない。また、動作周波数が低く、CPUが備えている多くの高速化機能を持たない。複数のGPUを使う場合、通信速度が計算効率のボトルネックになる。<br>\n","　一方で、単一の命令に対して、異なるデータを処理させる形式のSIMDの並列処理に最適化されている。ハイエンドモデルは数千個の演算コアを持ち、複数のコアに対して、同じ命令を実行する並列計算に適している。<br>\n","　GPUでは、 単精度浮動小数点数32ビットの計算しか行えない製品が多いが、 半精度浮動小数点数16ビットを利用する製品もある。半精度浮動小数点数16ビット等に精度を落とす場合、オーバーフローが起こりやすくなることに注意。<br>\n","　GPU の並列計算への特化性を科学計算を含むグラフィクス以外の用途に活用することが増えており、グラフィクス以外の汎用的な科学計算に GPU を利用する技術を総称して、GPGPUという。GPGPUは、特に、ディープラーニングにおける行列演算や暗号通貨のマイニングに利用されることが多い。代表的なディープラーニングのフレームワークである Tensorflow や PyTorch にサポートされている。<br><br>\n","       - <font color=\"Blue\">TPU</font><br>\n","> 　TPUは、Google が開発した深層学習のためのプロセッサ。TPUは、多数の積和演算機が互いに直列に接続された、シストリックアレイと呼ばれるアーキテクチャ\n","によって構成されている。また、TPU では演算結果をメモリではなく次の乗算器に渡すことでメモリアクセス回数を削減しているほか、演算器の計算精度を 8bit や 16bit に限ることで、スループットの向上や低消費電力化を図っている。<br><br>\n","       - <font color=\"Blue\">FPGA</font><br>\n","<font color=\"black\">FPGAは、C/C++ や Python などの高級言語によってその回路を記述することができる、エッジ端末における高速推論に利用されるデバイス。FPGA\n","は、製造後に内部の論理回路の構成を書き換えることができる柔軟性を持ち、深層学習の行列演算を論理回路によって物理的に実装することで処理速度の高速化や省電力化が実現可能。<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://inaccel.medium.com/cpu-gpu-fpga-or-tpu-which-one-to-choose-for-my-machine-learning-training-948902f058e0)</font></font><br>\n","<img src=\"https://miro.medium.com/max/1100/1*86ytqzOii135BOfAv3HYZA.webp\" width=\"320\">\n","https://cloud.google.com/blog/ja/products/gcp/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu\n","\n","https://ja.wikipedia.org/wiki/%E3%83%8E%E3%82%A4%E3%83%9E%E3%83%B3%E5%9E%8B"],"metadata":{"id":"zVMyI-oJkqiK"}},{"cell_type":"markdown","source":["# <font color=\"Silver\">Docker [<font color=\"silver\">…](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!5774&cid=b0f01606242a6ed3&CT=1670918477712&OR=ItemsView)</font></font><br>"],"metadata":{"id":"Lp3ObGs8PqoD"}},{"cell_type":"markdown","source":["# <font color=\"Silver\">Docker│ホスト型とコンテナ型</font><br>\n","\n",">|  ･  |  ホスト型  | コンテナ型  |\n","| :---: | :---: | :---: |\n","|  システム負荷  |  ×  |  〇  |\n","|  起動  |  ×  |  〇  |\n","|  OSの選択性  |  〇  | ×  |\n","|  再現性  |  △  |  〇 |\n","|  インフラ環境の影響  |  〇  | ×  |\n","\n","> - <font color=\"Silver\">Description</font><br>\n"," - Docker<br>\n","<font color=\"Blue\">$\\tiny{\\rm Link}$ [<font color=\"Blue\">…</font>](https://zenn.dev/suzuki_hoge/books/2022-03-docker-practice-8ae36c33424b59/viewer/1-1-readme)</font><br> \n","   - ホスト型とコンテナ型<br> \n","       -  ホスト型<br> \n","         - ハイパーバイザー上に複数の仮想環境を構築でき、仮想マシン上で任意のOSを選択・作動可能<br>\n","         - ハードウェアレベルで仮想化されているため、親元インフラ環境と仮想マシン間が互いに影響を及ぼしあわない。\n","         - コンピュータ内部に仮想環境を構築する必要があるため、負荷が大きく、動作が遅くなる<br>\n","       -  コンテナ型<br> \n","         - ホスト型のようにゲストOSというものはなく、ホストOSのカーネルを使用してアプリケーションを実行する。カーネルとはOSの中核となるプログラムであり、ハードディスクやメモリ等のコンピュータのリソース管理を行う。\n","         - ホスト OS のカーネルを利用するため、コンピュータリソースの使用効率が良い<br> \n","         - ホスト OS のカーネルを利用するため、アプリケーションの起動速度が速い<br>\n","         - ハードウェアや OS の設定が実行環境に依存しないため、設定が容易である<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://zenn.dev/suzuki_hoge/books/2022-03-docker-practice-8ae36c33424b59/viewer/1-2-virtualization)</font></font><br>\n","<img src=\"https://res.cloudinary.com/zenn/image/fetch/s--lxIu9ZQ2--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_1200/https://storage.googleapis.com/zenn-user-upload/deployed-images/64096153514c7abe28ce61c4.jpeg%3Fsha%3D909523393aeda1bf32f887c7c83f5c433276000d\" width=\"320\"><img src=\"https://res.cloudinary.com/zenn/image/fetch/s--mlNvVqaU--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_1200/https://storage.googleapis.com/zenn-user-upload/deployed-images/3bbb28d4124aacbce41fdf90.jpeg%3Fsha%3D43f8380520419f12b8eceda4a953fc1141d901f9\" width=\"320\"><br>"],"metadata":{"id":"zXUV88zDReX0"}},{"cell_type":"markdown","source":["# <font color=\"Silver\">Docker│Docker Engine</font><br>\n","> - <font color=\"Silver\">Description</font><br>\n"," - Docker<br>\n","     -  Docker Engine \n","       -  Docker Daemon, <font color=\"Silver\">コマンドを受け付け、Docker機能に命令を出す</font><br>\n","       -  REST API, <font color=\"Silver\">Docker CLIとDocker Daemonの対話に使用</font>\n","       -  Docker CLI, <font color=\"Silver\">コマンドを発行、REST APIもしくは直接CLIコマンドを使用し、Docker Daemonと対話</font><br> \n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://zenn.dev/ryoatsuta/articles/64dcc2e2b4e0cf)</font></font><br>\n","<img src=\"https://docs.docker.jp/v1.12/_images/engine-components-flow.png\" width=\"480\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://pyteyon.hatenablog.com/entry/2019/12/09/210000)</font></font><br>\n","<img src=\"https://geekflare.com/wp-content/uploads/2019/09/docker-architecture.png\" width=\"640\"><br>\n","\n","\n","\n","\n"],"metadata":{"id":"zrMznHfFSfRC"}},{"cell_type":"markdown","source":["# <font color=\"Silver\">Docker│Docker Containerの実行</font><br>\n","> - <font color=\"Silver\">Description</font><br>\n"," - Docker<br>\n","     -  Docker Containerの実行<br>\n","       -  Dockerfileをbuildする（組み立てる）ことで、Docker Imagesが作成されます。さらにDocker Imagesをrunする（実行可能な状態にする）ことで、Docker Containerが作成される。<br><br>\n","       <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://cpptake.com/archives/491)</font></font><br>\n","<img src=\"https://cpptake.com/wp-content/uploads/2020/08/Docker%E7%90%86%E8%A7%A3-3.png\" width=\"640\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/zembutsu/items/24558f9d0d254e33088f)</font></font><br>\n","       -  Dockerfile<br>\n","         -  Docker Containerの設計内容をコマンド形式でまとめたテキストファイル。\n","         -  Docker Imageの設計図\n","         -  Dockerfileを使って、Docker Imageを作成する\n","         -  ファイル名は、みんな同じで”Dockerfile”\n","         -  Docker Instruction + arguments の形で記述していく（FROM “Docker Image:Tag”から始まる） \n","       -  Docker イメージ<br>\n","         -  Docker Containerの動作環境となるテンプレートファイル。OSやアプリケーションから実行に使われるコマンド、メタデータまで含む。一つの Docker Imagesから複数の Docker Containerの作成が可能。\n","         -  Docker Imagesは、RegistryのDocker Hubからダウンロードして使うことができる他、自分で作って使うこともできる。\n","       -  Docker コンテナ<br>\n","         -  Docker Imagesに基づいてアプリケーションを実行する環境・インスタンス。Docker Imagesを実行するとDocker Containerが作成される。実行環境であるDockerコンテナが作成されると、アプリケーションの実行が開始される。<br><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F13002%2Ff365e304-930c-6888-5ce2-b8032006a58f.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=80cd1c39edf3310cdce41637a0764ee8\" width=\"640\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://www.webtomoblg.net/web/docker-beginner02/)</font></font><br>\n","<img src=\"https://www.webtomoblg.net/wp-content/uploads/2020/08/docker-image-layer-1536x730.png\" width=\"640\"><br>\n"],"metadata":{"id":"kSXNcKWAa15u"}},{"cell_type":"markdown","source":["# <font color=\"Silver\">Docker│Docker コマンド</font><br>\n","> - <font color=\"Blue\">$\\tiny{\\rm Link}$ [<font color=\"Blue\">…</font>](https://www.webtomoblg.net/web/docker-beginner05/)</font><br>\n","https://hara-chan.com/it/infrastructure/docker-command/ <br>\n","- <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://penpen-dev.com/blog/docker-command/)<br><br></font></font>\n","<img src=\"https://www.webtomoblg.net/wp-content/uploads/2020/08/docker-file-1536x779.png\" width=\"480\">\n","- <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://penpen-dev.com/blog/docker-command/)<br></font></font>\n","<img src=\"https://penpen-dev.com/blog/wp-content/uploads/hsthbrt.png\" width=\"480\">\n","> - <font color=\"Blue\">docker pull</font><br> \n","docker pull [docker image name]:[バージョンやタグ名]<br>\n","リポジトリからイメージを取得<br>\n","> - <font color=\"Blue\">docker push</font><br> \n","docker push [アカウント名/image name:バージョンやタグ名]<br>\n","リポジトリへイメージを登録<br>\n","> - <font color=\"Blue\">docker build</font><br>\n","docker build -t [docker image name] <br>\n","Dockerfileからイメージを作成<br>\n","docker build -f [docker file name] -t [docker image name] .<br>\n","Dockerfile以外からイメージを作成<br>\n","> - <font color=\"Blue\">docker create</font><br>\n","docker create [docker image name] <br>\n","Docker コンテナを作成( Docker コンテナのステータスは created に変化)<br>\n","> - <font color=\"Blue\">docker run -d</font><br>\n","docker run -d [docker image name] <br>\n","ビルド済みのdocker imageをバックグラウンドで実行<br>\n","> - <font color=\"Blue\">docker run -d -p</font><br>\n","docker run -d -p [host port]:[container port] [docker image name]<br>\n","コンテナのポートをローカルにポートフォワードする場合は、「-p」オプションを指定<br><br>\n","> - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://penpen-dev.com/blog/docker-command/)</font></font><br>\n","<img src=\"https://penpen-dev.com/blog/wp-content/uploads/hsthbrt.png\" width=\"480\">\n","> - <font color=\"Blue\">docker container start</font><br>\n","docker container start [container id先頭の一部／container name] <br>\n","Docker コンテナを起動(ステータスは running に変換)<br>\n","> - <font color=\"Blue\">docker container stop</font><br>\n","docker container stop [container id先頭の一部／container name] <br>\n","Docker コンテナを停止(ステータスは exited に変化、失敗すると dead ステータスに変化)<br>\n","> - <font color=\"Blue\">docker container rm</font><br>\n","docker container rm [container id先頭の一部／container name] <br>\n","Docker コンテナを破棄(ステータスは removing に変化して、 Docker コンテナは消滅)<br>\n","Docker コンテナが start しているときに rm することはできないので、停止させてから削除する<br>\n","> - <font color=\"Blue\">docker exec -it</font><br>\n","docker exec -it [container id先頭の一部／container name] /bin/bash <br>\n","コンテナの中で作業したり、ビルドした内容を確認する場合に、コンテナ内にログインするコマンド<br>\n","> - <font color=\"Blue\">docker container ls</font><br>\n","docker container ls [-a] <br>\n","停止したコンテナやエラーで実行できなかったコンテナも含めて表示する場合は「-a」オプションを指定<br>"],"metadata":{"id":"SpWWI05D3K39"}},{"cell_type":"markdown","source":["# <font color=\"Silver\">Docker│Docker Compose</font><br>\n","-  Docker Compose<br>\n"," -  複数の Docker コンテナの実行を一括で管理するツール。管理の仕様は、yaml 形式の設定ファイルで定義される。<br><br>\n","Docker Compose は compose up コマンドで Yaml に従いコンテナを構築する<br><br>\n","https://hara-chan.com/it/infrastructure/docker-compose/<br>\n","https://zenn.dev/yukiyohure/articles/d6a7f30fcda1c3f4fb03<br>\n","https://snowsystem.net/container/docker/docker-compose-command/#<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://o2mamiblog.com/docker-beginner-2/)</font></font><br>\n","<img src=\"https://o2mamiblog.com/wp-content/uploads/2022/07/Docker-compose_page-0004-1.jpg\" width=\"640\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://www.docker.com/blog/containerized-python-development-part-2/)</font></font><br>\n","<img src=\"https://www.docker.com/wp-content/uploads/2022/06/containerized-python-development-2-2.png.webp\" width=\"640\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://zenn.dev/suzuki_hoge/books/2022-03-docker-practice-8ae36c33424b59/viewer/3-8-docker-compose)</font></font><br>\n","<img src=\"https://res.cloudinary.com/zenn/image/fetch/s--mBwFuEAo--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_1200/https://storage.googleapis.com/zenn-user-upload/deployed-images/b8007dc077a731fc8cbdfee9.jpeg%3Fsha%3De9c6d13a3bd0138a7ed624780b505823331d4fa9\" width=\"480\">\n","<img src=\"https://res.cloudinary.com/zenn/image/fetch/s--l0sZ7qrc--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_1200/https://storage.googleapis.com/zenn-user-upload/deployed-images/1443ba2569b11a091426ec06.jpeg%3Fsha%3D541b18a0fc14801bcc91eaec9cc68b303d8e0669\" width=\"480\"><br>\n","\n"],"metadata":{"id":"ly9qIJbjnAyX"}},{"cell_type":"markdown","source":["# <font color=\"Silver\">Docker│Dockerfile</font><br>\n","-  <font color=\"Blue\">Dockerfile</font>      \n","FROM <font color=\"Silver\">ベースとなるイメージを決定する。DockerfileはFROMから書き始める。</font><br>\n","RUN <font color=\"Silver\"> Docker イメージをビルドする際に実行したいコマンドを記載。RUN毎にイメージレイヤーが作られる。</font><br>\n","CMD <font color=\"Silver\"> Docker コンテナの起動時に実行したいプロセス（デフォルトコマンド）を記載。Dockerfileの最後に記述。</font><br>\n","ENTRYPOINT <font color=\"Silver\">CMD で記載された値を引数とするプロセスを記載。上書きできないので必ず実行したいコマンドを入れる</font><br>\n","ENV <font color=\"Silver\">Docker コンテナ内での環境変数を設定</font><br>\n","COPY <font color=\"Silver\">Docker イメージのファイルシステムにファイルやフォルダを追加</font><br>\n","ADD <font color=\"Silver\">上記 COPY と同様の機能をもち、tar ファイルは Docker イメージ上で自動展開</font><br>\n","USER <font color=\"Silver\">実行ユーザを指定</font><br>\n","WORKDIR <font color=\"Silver\">RUN や COPY、CMD といったコマンドを実行するディレクトリを指定</font><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/zembutsu/items/24558f9d0d254e33088f)</font></font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F13002%2Ff6b4556b-29c9-f7d3-eecf-2cf13580a9d0.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=d438b78cbe989f1926e9dfbf9776b0aa\" width=\"640\"><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://hara-chan.com/it/infrastructure/docker-cmd-entrypoint-difference/)</font></font><br>\n","<img src=\"https://hara-chan.com/wp-content/uploads/2020/11/edit-2-1.png\" width=\"320\"><br><br>\n","- COPYとADDの違い、CMDとENTRYPOINTの違い<br>\n","https://www.webtomoblg.net/web/docker-beginner08/<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://www.webtomoblg.net/web/docker-beginner08/)</font></font><br>\n","<img src=\"https://www.webtomoblg.net/wp-content/uploads/2020/08/cmd-entrypoint.png\" width=\"640\">"],"metadata":{"id":"W0U0t-bOeW3m"}}]}