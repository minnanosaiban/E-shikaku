{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/group-nai-shomu/00/blob/main/14%E6%95%B0%E5%BC%8F.ipynb","timestamp":1682401821670}],"toc_visible":true,"authorship_tag":"ABX9TyOkd83jxFTb0kQf62liJT1c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"8O4f77rLg3bI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####<font color=\"silver\">固有値分解</font>\n","- 半正定値行列の固有値は非負である<br><br>\n","- <font color=\"silver\">固有値分解<br></font><br>\n","${A}{u}=\\lambda {u} \\qquad P^{-1}AP=Λ \\qquad A=PΛP^{-1}$<br><br>\n","- <font color=\"silver\">特異値分解<br></font><br>\n","$A{v}=\\sigma{u} \\qquad {A}^\\top{u}=\\sigma{v} \\qquad A=UΣV^{T}$<br>\n","$m$次元ベクトル$u$を左特異ベクトル<br>\n","$n$次元ベクトル$v$を右特異ベクトルという<br><br>\n","<img src=\"https://camo.qiitausercontent.com/aea25f6cb0dcc6ca021e54a3d75ddd7e5c6c44d0/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3434383530332f61663636663561662d356236362d656463332d626434372d3564306139353864643062362e706e67\" width=\"320\"><br><br>\n","- <font color=\"silver\">固有値の求め方<br></font><br>\n","$ \\begin{align}det({A}-\\lambda{I})&=0\\\\\\\\\n","\\left[ \\begin{array}{cc} 1 & 4 \\\\ 2 & 3 \\end{array} \\right]-\\lambda \\left[ \\begin{array}{cc}  1 & 0 \\\\ 0 & 1 \\end{array} \\right]&=0\\\\\\\\\n","\\left[ \\begin{array}{cc} 1-\\lambda & 4 \\\\ 2 & 3-\\lambda \\end{array} \\right]&=0\\\\\\\\\n","(1-\\lambda)(3-\\lambda)-4×2 &=0\\\\\\\\\n","\\end{align}$<br>\n","- <font color=\"silver\">固有ベクトルの求め方<br></font><br>\n","$ \\begin{align}\\left[ \\begin{array}{cc} 1 & 4 \\\\ 2 & 3 \\end{array} \\right]\\left[ \\begin{array}{cc}  x \\\\ y \\end{array} \\right]&=5 \\left[ \\begin{array}{cc}  x \\\\ y \\end{array} \\right]\\end{align}$<br><br>\n","$ \\begin{cases} 1x+4y = -x \\\\ 2x+3y = -y \\\\ \\end{cases} \\rightarrow \\begin{cases} 2x+4y = 0 \\\\ 2x+4y = 0 \\\\ \\end{cases}$<br>\n"],"metadata":{"id":"NaRwCxF9tlwt"}},{"cell_type":"markdown","source":["####<font color=\"silver\">距離</font>\n","$ {d(\\pmb{x}, \\pmb{y}) = \\sqrt{\\sum_{i=1}^N (x_i – y_i)^2}} = \\sqrt{\\sum_{i=1}^N (x_i – y_i)^\\top(x_i – y_i)}$\n","<br><br>\n","$ d(\\pmb{x}, \\pmb{y}) = \\sum_{i=1}^N |x_i – y_i|$\n","<br><br>\n","$ d(\\pmb{x}, \\pmb{y}) = \\left(\\sum_{k=1}^n \\left| x_i - y_i \\right|^p \\right)^{\\frac 1 p}$\n","<br><br>\n","$ d(\\pmb{x}, \\pmb{y}) = \\mathop{\\rm max}\\limits_{i}|x_i -y_i|$\n","<br><br>\n","$ {d(\\pmb{x}, \\pmb{y}) = \\displaystyle\\sqrt{ (x – y)^\\top \\displaystyle\\sum^{-1} (x – y) }}$"],"metadata":{"id":"vF9CxZQbuCA8"}},{"cell_type":"markdown","source":["####<font color=\"silver\">ノルム</font>\n","$ \\|\\pmb{x}\\|_1 = \\displaystyle\\sum_{i=1}^{N}|x_i| \\quad $または$ \\quad \\|\\pmb{x}\\|_1=\\pmb{x}\\cdot\\pmb{x}$\n","<br><br>\n","$ {\\|\\pmb{x}\\|_2=\\left(\\displaystyle\\sum_{i=1}^{N}|x_i|^2\\right)^{1/2}}=\\sqrt{\\displaystyle\\sum_{i=1}^{N}x_i^2}$\n","<br><br>\n","$ {\\|\\pmb{x}\\|_\\infty = \\displaystyle\\max_i |x_i|}$\n","<br><br>\n","$ {\\|\\pmb{x}\\|_p=\\left(\\displaystyle\\sum_{i=1}^{N}|x_i|^p\\right)^{1/p}}$"],"metadata":{"id":"aDrnFthiuHrm"}},{"cell_type":"markdown","source":["####<font color=\"silver\">期待値分散</font>\n","$\\mu=\\displaystyle\\frac{1}{N} \\sum_{i=1}^{N} x_{i}\\qquad\\qquad\\sigma^{2}=\\displaystyle\\frac{1}{N} \\sum_{i=1}^{N}\\left(x_{i}-\\mu\\right)^{2}$<br><br>\n","$E[X]=\\displaystyle\\sum_{x} x p(x)\\qquad V[X]=\\displaystyle\\sum_{x}(x-E[X])^{2} p(x)$<br><br>\n","$\\mathbb{V}[X]=\\mathbb{E}[(X-\\mathbb{E}[X])^2]$\n","<br><br>\n","$\\mathbb{V}[X]=\\mathbb{E}[X^2]-(\\mathbb{E}[X])^2$\n","<br><br>\n","$\\sigma_{xy}=\\cfrac{1}{n}\\displaystyle\\sum_{i=1}^n{(x_i-\\bar{x})(y_i-\\bar{y})}\\qquad \\sigma_{xy}=\\cfrac{1}{n}\\displaystyle\\sum_{i=1}^nx_iy_i-\\bar{x}\\bar{y}$</font>\n","<br><br>\n","$ \\begin{align}\n","S\n","= \\begin{pmatrix}\n","s_{1}^2 & s_{12} & \\cdots & s_{1d} \\\\\n","s_{12} & s_{2}^2 & \\cdots & s_{2d} \\\\\n","\\vdots \\\\\n","s_{1d} & s_{2d}  & \\cdots & s_{d}^2\n","\\end{pmatrix}\n","\\end{align}$<br><br>\n","$r=\\cfrac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^n(y_i-\\bar{y})^2}} \\qquad  ρ_{xy} = \\cfrac{\\sigma_{xy}}{\\sigma_{x}\\sigma_{y}}$<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qctoranomaki.com/sqc/statistics/covariance/)</font><br>\n","<img src=\"https://qctoranomaki.com/wp-content/uploads/2021/11/f4a1b0aed5dc02442c433030ff24c031-5-768x626.jpg\" height=\"320\">\n","<img src=\"https://qctoranomaki.com/wp-content/uploads/2021/11/51e8fc29fa61692e70bc4c925b602ad0-2.jpg\" height=\"320\">\n","<br>"],"metadata":{"id":"_Sh68f-IvSxL"}},{"cell_type":"markdown","source":["####<font color=\"silver\">正規分布<br></font>\n","- <font color=\"silver\">期待値分散</font><br><br>\n","$\\mathcal{N}(x | \\mu, \\sigma^2)$：<font color=\"silver\">一変量正規分布, </font><br>\n","$\\mu$：<font color=\"silver\">期待値</font><br>\n","$\\sigma^2$：<font color=\"silver\">分散</font><br><br>\n","- <font color=\"silver\">確率関数</font><br>\n","$f(x | \\mu, \\sigma^2) =\\cfrac{1}{\\sqrt {2\\pi \\sigma^2}}\\exp{\\biggl (}-{\\cfrac {(x-\\mu)^2}{2\\sigma^{2} }}{\\biggr )}$</font><br><br>\n","$f(x | \\mu, \\sigma^2) = \\cfrac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2\\sigma^2}(x - \\mu)^2}$<br><br>\n","- <font color=\"silver\">尤度関数</font><br><br>\n","$\\displaystyle L(\\mu,\\sigma^2|x) = \\prod_{i=1}^n{\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{\\left(-\\frac{(x_i-\\mu)^2}{2\\sigma^2} \\right)}}$<br><br>\n","- <font color=\"silver\">対数尤度関数</font><br><br>\n","$\\displaystyle l(\\mu,\\sigma^2|x) = -n \\log \\sqrt{2\\pi\\sigma^2} - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i-\\mu)^2$<br><br>\n","$\\displaystyle l(\\mu,\\sigma^2|x)=-\\frac{1}{2}n\\log 2\\pi-\\frac{1}{2}n\\log \\sigma^{2}-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(x_{i}-\\mu)^{2}$<br><br>\n","- <font color=\"silver\">最尤推定量</font><br><br>\n","$\\displaystyle{\\hat{\\mu}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}} \\qquad \\displaystyle{\\hat{\\sigma}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}(x_{i}-\\mu)^{2}}$"],"metadata":{"id":"N3uU8-cAvdJb"}},{"cell_type":"markdown","source":["#### <font color=\"silver\">ベルヌーイ分布</font>\n","- <font color=\"silver\">期待値分散</font><br><br>\n"," $\\mathrm{Ber}(x | p)$：<font color=\"silver\">ベルヌーイ分布</font><br>\n","$p$：<font color=\"silver\">期待値</font><br>\n","$p(1-p)$：<font color=\"silver\">分散</font><br><br>\n","- <font color=\"silver\">確率関数</font><br><br>\n","$f(x | p)=p^{x}(1-p)^{1-x}\\qquad p \\in [0,1] \\qquad x \\in \\{0,1\\}$<br><br>\n","- <font color=\"silver\">尤度関数</font><br><br>\n","$L(p|x) =  \\displaystyle\\prod_{i=1}^n p^{x_i}(1-p)^{1-x_i}$<br><br>\n","- <font color=\"silver\">対数尤度関数</font><br><br>\n","$\\displaystyle l(p|x)=\\sum_{i=1}^{n}\\left\\{x_{i}\\log p+(1-x_{i})\\log(1-p)\\right\\}$<br><br>\n","- <font color=\"silver\">最尤推定量</font><br><br>\n","$\\displaystyle{\\hat{p}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}}$"],"metadata":{"id":"nycGQ_ggwytF"}},{"cell_type":"markdown","source":["####<font color=\"silver\">カテゴリカル分布<br>\n","- <font color=\"silver\">期待値分散</font><br><br>\n"," $\\mathrm{Cat}(\\pmb{x}|\\pmb{p})$<font color=\"silver\">ベルヌーイ分布</font><br>\n","$p_k$<font color=\"silver\">期待値</font><br>\n","$p_k(1-p_k)$<font color=\"silver\">分散</font><br><br>\n","- <font color=\"silver\">確率関数</font><br>\n","$\\mathrm{Cat}(\\pmb{x}|\\pmb{p})= \\displaystyle\\prod_{k=1}^K {p_k}^{x_k}$<br><br>\n","$\\displaystyle\\sum\\limits_{k=1}^K p_k = 1 \\qquad 0 \\le p_k \\le 1 \\qquad \\pmb{x} = (x_1, \\dots, x_K) \\qquad \\ x_k \\in \\{0, 1\\} \\ \\text{where} \\displaystyle\\sum\\limits_{k=1}^K x_k = 1$<br><br>\n","- <font color=\"silver\">尤度関数</font><br><br>\n","$\\displaystyle L(p_{j})=\\prod_{i=1}^{n}\\prod_{j=1}^{k}p_{j}^{x_{ij}}$<br><br>\n","- <font color=\"silver\">負の対数尤度関数</font><br><br>\n","$\\displaystyle -\\log L(p_{j})=-\\sum_{i=1}^{n}\\sum_{j=1}^{k}x_{ij}\\log p_{j}$<br><br>\n","- <font color=\"silver\">最尤推定量</font><br><br>\n","$\\displaystyle{\\hat{p}_{j}=\\frac{1}{n}\\sum_{i=1}^{n}x_{ij}}$"],"metadata":{"id":"cqpG4iH1xSE-"}},{"cell_type":"markdown","source":["#### <font color=\"silver\">最小二乗法テンプレ</font>\n","- <font color=\"silver\">回帰問題のためのモデルを考える。</font><br><br>\n","$y=f(\\pmb{x}:\\pmb{θ})+\\epsilon$<br><br>\n","- <font color=\"silver\">$\\epsilon$ は、一変量正規分布 $N(0, \\sigma^2)$ に従う確率変数である</font><br><br>\n","$\\epsilon_i=y_i-f(\\pmb{x}_i:\\pmb{θ})$<br><br>\n","$ f(\\epsilon | 0, \\sigma^2)  =\\cfrac{1}{\\sqrt {2\\pi \\sigma^2}}\\exp{\\biggl (}-{\\cfrac {\\epsilon^2}{2\\sigma^{2} }}{\\biggr )}$<br><br>\n","$\\displaystyle -\\log L(p \\mid \\mathcal{D})=\\frac{1}{2\\sigma^2}\\sum_{i=1}^{N} (y_i-f(\\pmb{x}_i:\\pmb{θ}))^2 + N\\log\\sqrt {2\\pi \\sigma^2}$<br><br>\n","- <font color=\"silver\">最小二乗法の目的関数</font><br><br>\n","$\\displaystyle\\pmb{θ}^*, σ^* =\\mathop{\\rm argmax}\\limits_{\\pmb{θ},σ}\\frac{1}{2\\sigma^2}\\sum_{i=1}^{N} (y_i-f(\\pmb{x}_i:\\pmb{θ}))^2 + N\\log\\sqrt {2\\pi \\sigma^2}$<br><br>\n","- <font color=\"silver\">$\\pmb{θ}$を解くために$θ$を固定したときの目的関数</font><br><br>\n","$\\displaystyle\\pmb{θ}^* =\\mathop{\\rm argmax}\\limits_{\\pmb{θ}}\\frac{1}{2}\\sum_{i=1}^{N} (y_i-f(\\pmb{x}_i:\\pmb{θ}))^2$<br><br>\n","- <font color=\"silver\">一変量正規分布 $N(\\mu, 1)$ に従う確率変数である</font><br><br>\n","$f(x | \\mu, 0) =\\cfrac{1}{\\sqrt {2\\pi}}\\exp{\\biggl (}-{\\cfrac {(x-\\mu)^2}{2}}{\\biggr )}$\n"],"metadata":{"id":"UFAkWAsHx2H2"}},{"cell_type":"markdown","source":["#### <font color=\"silver\">ベイズの定理</font>\n","- <font color=\"silver\">加法定理</font><br><br>\n","$P(A \\cup B) = P(A) + P(B) – P(A \\cap B)$<br><br>\n","- <font color=\"silver\">乗法定理, 同時確率</font><br><br>\n","$P(A \\cap B)=P(A,B)=P(A) \\times P(B)=P(A\\mid B)P(B)=P(B\\mid A)P(A)$<br><br>\n","- <font color=\"silver\">周辺確率</font><br><br>\n","$P(A)=\\displaystyle\\sum_{B}P(A, B)= \\displaystyle\\sum_{B}P(A\\mid B)P(B)$<br><br>\n","- <font color=\"silver\">ベイズの定理</font><br><br>\n","$\\begin{eqnarray} \\overset{\\small (事後確率)}{P(A \\mid B)} = \\dfrac{ \\overset{\\small (尤度関数)}{P(B \\mid A)} \\overset{\\small (事前確率)}{P(A)} } { \\underset{(周辺尤度・エビデンス・規格化定数)}{P(B)} } \\end{eqnarray}$<br>\n","<br><br>\n","<font color=\"black\">$\\displaystyle P(A \\mid B)=\\cfrac{P(A \\cap B)}{P(B)}=\\cfrac{P(A\\mid B)P(B)}{P(B)}=\\cfrac{P(B \\mid A)P(A)}{P(B)}$<br><br>\n"," <img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F582794%2F6d2be4f5-892d-cd55-db41-82897302bc3b.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=b54b5f1cb263bffad207f74920bd9a70\" width=\"320\">\n"],"metadata":{"id":"cabkhTqIyzsX"}},{"cell_type":"markdown","source":["#### <font color=\"silver\">生成モデルテンプレ</font>\n","- 問題<br><br>\n","$\\mathcal{C}_{\\rm pred}=\\mathop{\\rm argmax}\\limits_{\\mathcal{C}}p(\\mathcal{C} \\mid x)$<br><br>\n","- 識別モデル<br><br>\n"," $p(\\mathcal{C} \\mid x)$ を直接モデリングして $\\mathop{\\rm argmax}\\limits_{\\mathcal{C}}p(\\mathcal{C} \\mid x)$ を解くアプローチ<br><br>\n","- 生成モデル<br><br>\n"," $\\displaystyle P(\\mathcal{C} \\mid x)=\\cfrac{ P(x \\mid \\mathcal{C})P(\\mathcal{C})}{P(x)}=\\cfrac{P(x , \\mathcal{C})}{P(x)}$<br><br>\n"," であることから、$P(x \\mid \\mathcal{C})$と$P(\\mathcal{C})$をそれぞれモデリングするか、 $P(x , \\mathcal{C})$ をモデリングして、$\\mathop{\\rm argmax}\\limits_{\\mathcal{C}}p(\\mathcal{C} \\mid x)$ を解くアプローチ<br><br>\n"," 生成モデルに基づくアプローチで得たモデルを用いて<br><br>\n","$P(x)=\\displaystyle\\sum_{\\mathcal{C}}P(x \\mid \\mathcal{C})P(\\mathcal{C})$<br><br>\n","と周辺化することで、入力 $x$ が従う分布 $p(x)$ を求めることが可能であるため、予測のみならず欠損値の補完やデータ拡張への応用が期待でできる。<br><br>\n","- 規格化定数<br><br>\n"," 規格化定数$P(x)$は積分によって算出できるが、様に計算することができないため、マルコフ連鎖モンテカルロ法等によって近似する。<br><br>\n","$P({x})= \\displaystyle\\int P({x}\\mid \\mathcal{C})P(\\mathcal{C}) d\\mathcal{C}$<br>"],"metadata":{"id":"nvrEOZX00nzn"}},{"cell_type":"markdown","source":["#### <font color=\"silver\">ベイズ推定例題</font>\n"," - <font color=\"silver\">Description</font><br>\n"," -  <font color=\"silver\">例題</font>\n","   -  　過去の調査結果から、全てのメールのうち10%が迷惑メールであることが分かっています。また、迷惑メールの中で「広告」という単語の含まれる確率が30%、一般メールの中では5%であることが分かっています。無作為に選んだメールに「広告」という単語が含まれていた場合、このメールが迷惑メールである確率はいくつでしょうか？</font><br>\n","<font color=\"silver\">$\\scriptsize \\cfrac{0.3\\times0.1}{0.3\\times0.1＋0.05×0.9}=0.4$</font><br>\n","   -  　1万人に1人の割合で罹患する病気があったとします。この病気の陽性/陰性を判定する検査において、誤判定する割合が1%であったとします。Aさんが陽性と判定されたとき、本当に病気にかかっている確率はいくつでしょうか？</font><br>\n","<font color=\"silver\">$\\scriptsize \\cfrac{0.99\\times0.0001}{0.99\\times0.0001＋0.01×0.9999}=0.0098$</font><br>\n","   -  　とある製品は、工場A、工場B、工場Cの3つの工場で作られています。生産数量の比率は、それぞれ50%、30%、20%です。また、それぞれの工場における不良の発生率は、1%、2%、5%という調査結果が得られています。ここで、とある不良品がどの工場で製造されたのか不明な場合において、工場Cで生産されたものである確率はいくつでしょうか？</font><br>\n","<font color=\"silver\">$\\scriptsize \\cfrac{0.05\\times0.2}{0.01×0.5 ＋ 0.02×0.3 ＋ 0.05\\times0.2}=0.0476$</font><br>\n","   -  　工場A,B,C,Dで生産される製品があり、ある日不良品が発見された。 各工場の生産割合が30%, 40%, 20%, 10%、不良品率が5%, 6%, 4%, 10%であるとしたとき、不良品が生産された可能性が高い工場はどれでしょうか？<br>\n","<font color=\"silver\">$\\scriptsize \\cfrac{？\\times？}{0.05×0.3 ＋ 0.06×0.4 ＋ 0.04\\times0.2＋ 0.10\\times0.1}$</font><br>\n","   -  　疾患 X に罹患している確率が 0.001 であるとする。検査Yは、疾患 X に罹患している人に適用した場合に確率 0.95 で陽性を示し、疾患 X に感染していない人に適用した場合に確率 0.05 で陽性を示すことが知られている。Z氏 に対して検査薬 Y を適用したところ、陽性を示した。Z氏 が疾患 X に罹患している確率はいくつでしょうか？<br>\n","<font color=\"silver\">$\\scriptsize \\cfrac{0.95\\times0.001}{0.95\\times0.001＋0.05×0.999}=0.019$</font><br>\n","   -  　母集団に属する人が疾患Xに罹患している確率を0.010 とする。 簡易検 査薬Yは、疾患Xに感染している人に適用した場合に確率 0.90で陽性を 示し、疾患Xに感染していない人に適用した場合に確率0.10で陽性を示 すことが知られている。 母集団に属する人のうち、ある1名に対して 簡易検査薬Yを適用したところ、 陽性を示した。 このとき、 zが疾患Xに 罹患している確率。<br>\n","<font color=\"silver\">$\\scriptsize \\cfrac{0.90\\times0.010}{0.99\\times0.010＋0.10×0.990}=0.083$</font>"],"metadata":{"id":"SUrcMxN-1l5n"}},{"cell_type":"markdown","source":["#### <font color=\"silver\">情報理論</font>\n","- <font color=\"silver\">Information</font><br><br>\n","$ I(x) = -\\log P(x)$<br><br>\n","$I(x) = \\log\\cfrac{1}{p(x)} = -\\log p(x)$<br><br>\n","<img src=\"https://camo.qiitausercontent.com/4294035b14106783a536dd1344dc0e3d0664559d/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f323531353532372f30313766346164342d613436652d363362622d626336372d6132613038623431303332632e706e67\" width=\"200\">\n","- <font color=\"silver\">Entropy</font><br><br>\n","$ H(p(x))  =  \\mathbb{E}_{p(x)}[I(x)] = \\mathbb{E}_{p(x)}[-\\log p(x)]$<br><br>\n","$ H(X) = \\displaystyle\\sum_{x} I(x) P(x)$<br><br>\n","$ H(X) = -\\displaystyle\\sum_{x} P(x) logP(x)$<br><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2515527%2F47f415f6-1859-f987-69c1-cdc6d9c6b01c.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=d77b5fe90ae5641fe30ebfc0a9a789aa\" width=\"200\"><br><br>\n","- <font color=\"silver\">相互情報量</font><br><br>\n","$I(X;Y) = \\displaystyle\\sum_x \\sum_y p(x,y) \\log_2 \\left( \\cfrac{p(x,y)}{p(x) p(y)}   \\right)$\n","<br><br>\n","$\\begin{eqnarray}\n","I(X;Y) & = & H(X) - H(X|Y) \\\\\\\\\n","& = & H(Y) - H(Y|X) \\\\\\\\\n","& = & H(X) + H(Y) - H(X,Y)\n","\\end{eqnarray}$<br><br>\n","- <font color=\"silver\">バイナリクロスエントロピー</font><br><br>\n","$\\begin{align}\n","H(P,Q) = - \\sum_{i}^{2} y_i \\log \\hat{y_i} &= - (y_1 \\log \\hat{y_1} + y_2 \\log \\hat{y_2} ) \\\\\n","&= - (y_1 \\log \\hat{y_1} + (1-y_1) \\log (1-\\hat{y_1}) )\n","\\end{align}$<br><br>\n","- <font color=\"silver\">Cross-Entropy</font><br><br>\n","$\\displaystyle H(P,Q) = -\\sum_{x \\sim P} P(x) \\log Q(x)$<br><br>\n","$H(p(x), q(x))  =  \\mathbb{E}_{p(x)}[-\\log q(x)]$<br><br>\n","- <font color=\"silver\">KL Divergence<br><br></font>\n","$ D_{KL}(P\\|Q) = \\displaystyle\\sum_{x} P(x) \\log\\cfrac{P(x)}{Q(x)}$\n","<br><br>\n","$ D_{KL}(P\\|Q) = -\\displaystyle\\sum_{x} P(x) \\log\\cfrac{Q(x)}{P(x)}$\n","<br><br>\n","$\\begin{eqnarray} KL(p(x)||q(x)) & = & H(p(x), q(x)) - H(p(x))\\\\ &=& \\mathbb{E}_{p(x)}[-\\log q(x)] - \\mathbb{E}_{p(x)}[-\\log p(x)]\\\\ &=& \\mathbb{E}_{p(x)}[-\\log q(x) + \\log p(x)]\\\\ &=& \\mathbb{E}_{p(x)}\\left[\\log\\left(\\frac{p(x)}{q(x)}\\right)\\right]\\end{eqnarray}$<br><br>\n","（KL Divergence）=（Cross-Entropy）-（Entropy）</font><br><br>\n","- $KL(p||q) \\neq KL(q||p)$であり$P$と$Q$が非対称<br>\n","- $p(x)$を調整して、$p(x)$と$q(x)$の誤差（カルバックライブラー情報量）を最小化するためには、第２項（クロスエントロピー）を最小化することと同じ<br><br>\n","- <font color=\"silver\">JS Divergence</font><br><br>\n","$M(x) = \\cfrac{P(x)+Q(x)}{2}$<br><br>\n","$ D_{JS}(P\\|Q) = \\cfrac{1}{2}(D_{KL}(P\\|M) + D_{KL}(Q\\|M))$<br><br>\n","- $D_{KL}(P\\|M)$と$D_{KL}(Q\\|M)$との平均、対象で交換可能</font><br>\n","- 真の分布と学習モデルの分布が重ならないとき、勾配消失問題が生じる</font><br><br>\n"],"metadata":{"id":"WssREySj2alA"}},{"cell_type":"markdown","source":["####<font color=\"silver\">バイアスバイリアンス</font>\n","<font color=\"silver\">$\\quad$E$\\qquad$=$\\quad$バイアス + バイリアンス$\\quad$+$\\qquad$ノイズ</font><br>\n","<font color=\"silver\">$\\quad$$\\qquad$$\\qquad$$\\qquad$予測値と理想値$\\qquad$+$\\qquad$理想値と実データ</font><br>\n","<br>\n","$\\displaystyle E\\left( L\\right) =\\int \\left\\{ y\\left( x\\right) -h\\left( x\\right) \\right\\} ^{2}p\\left( x\\right) dx + \\int \\int \\left\\{ h\\left( x\\right) -t\\right\\} ^{2}p\\left( x,t\\right) dxdy$<br><br>\n","<font color=\"silver\">$\\qquad$バイアス$\\qquad$$\\qquad$$\\qquad$$\\qquad$+$\\qquad$$\\qquad$バイリアンス</font><br>\n","<font color=\"silver\">$\\qquad$予測値の期待値と理想値$\\qquad$+$\\qquad$$\\qquad$予測値と予測値の期待値</font><br><br>\n","$\\displaystyle \\int \\left\\{ E_{D}\\left[ y\\left( x;D\\right) \\right] -h\\left( x\\right) \\right\\} ^{2}p(x)dx +  \\int E_{D}\\left[ \\left\\{ y\\left( x;D\\right) -E_{D},\\left[ y\\left( x;D\\right) \\right] \\right\\} ^{2}\\right] p\\left( x\\right) dx$"],"metadata":{"id":"ckDQ-4Uy5PVi"}},{"cell_type":"markdown","source":["####<font color=\"silver\">回帰分析</font>\n","$\\hat{y} = \\pmb{x}^\\top \\pmb{w}$<br><br>\n","$\\displaystyle L = \\frac{1}{2}\\sum_{n=1}^N (y^{(n)}-t^{(n)})^2+λ \\sum_{i=1}^Kw_i^2$<br><br>\n","$\\displaystyle \\nabla {\\mathcal{L}}_{\\mathcal{D}}(\\pmb{w})=\\frac{\\partial L}{\\partial w}=\\pmb{x}_n^{\\top}(y - t)+2λw$<br><br>"],"metadata":{"id":"a2tqYpvzOCcz"}},{"cell_type":"markdown","source":["####<font color=\"silver\">損失</font>\n","$\\textrm{MAE} = \\displaystyle\\cfrac{1}{N}\\sum_{i=1}^{N}|y_{i}-\\hat{y}_{i}|$\n","<br><br>\n","$\\textrm{MSE} = \\displaystyle\\cfrac{1}{N}\\sum_{i=1}^{N}(y_{i}-\\hat{y}_{i})^{2}$<br><br>\n","$\\displaystyle\\textrm{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(y_{i}-\\hat{y}_{i})^{2}}$<br><br>\n","$ \\textrm{R}^{2} = 1 - \\cfrac{\\sum(y_{i}-\\hat{y_{i}})^{2}}{\\sum(y_{i}-\\bar{y_{i}})^{2}}$<br><br>\n","$L_1^\\text{smooth}(x) = \\begin{cases}\n","    0.5 x^2             & \\text{if } \\vert x \\vert < 1\\\\\n","    \\vert x \\vert - 0.5 & \\text{otherwise}\n","\\end{cases}$<br><br>\n","<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*0eoiZGyddDqltzzjoyfRzA.png\" width=\"480\"><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://heartbeat.comet.ml/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0)</font></font><br>\n","<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*8BQhdKu1nk-tAAbOR17qGg.png\" width=\"240\">\n","<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*EqTaoCB1NmJnsRYEezSACA.png\" width=\"240\">\n","<img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*jxidxadWSMLvwLDZz2mycg.png\" width=\"240\">\n"," - Huber Loss\n","  - 損失が大きいとMAEに似た機能をし、損失が小さいとMSEの機能になる。<br>\n","  - MSEとMAEの切り替わりは𝛿で設定する<br><br>\n"," - $\\rm R^2$\n","- 予測データの分散が教師データの分散に相対して、何割占めているかを表す。<br>\n","- 精度依存する評価と違い、分散が低くても、精度が高くない場合があり得る。<br>\n","- 回帰分析の当てはまりの良さ<br>\n","- 目的変数のスケールに依存せず評価可能<br>\n","- 0から1の値をとる<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://miro.medium.com/v2/resize:fit:720/format:webp/1*EqTaoCB1NmJnsRYEezSACA.png)</font></font><br>\n","<img src=\"https://cor.tokyo/wp-content/uploads/2019/04/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2019-04-18-23.22.06.png\" width=\"320\"><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/ironball/items/6acb3546312f4c65ec54)</font></font><br>\n","<img src=\"https://camo.qiitausercontent.com/4b2a8b68fcf74c56350ef368cdbc7b5e4ef73f2e/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3438383537372f35333564336439302d343035302d313862392d316263372d3465373066346666623233322e706e67\" width=\"480\"><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://www.xiaowenying.com/machine-learning/2019/11/18/svm.html)</font><br>\n","<img src=\"https://www.xiaowenying.com/assets/post_img/svm/7824BB24CA8355AFBD5B9BF42521096D.jpg\" width=\"320\"><br><br>\n","\n"],"metadata":{"id":"TrPXhicbAnyW"}},{"cell_type":"markdown","source":["####<font color=\"silver\">オッズ比</font>\n","- <font color=\"silver\">確率, $\\quad 0 < p(x) < 1$</font><br><br>\n","発生確率$p$, 発生しない確率$1-p$<br><br>\n","- <font color=\"silver\">オッズ比, $\\quad 0 < {\\rm Odds} < ∞$</font><br><br>\n","$\\cfrac{p}{1-p}=\\exp({w^{\\mathrm{T}}x)}$<br><br>\n","- <font color=\"silver\">ロジット関数, $\\quad-∞ < {\\rm logit}  < ∞$</font><br><br>\n","$ f(p) = \\log \\cfrac{p}{1-p}$<br><br>\n","- <font color=\"silver\">ロジスティック関数, $\\quad 0 < {\\rm logistic}  < 1$</font><br><br>\n","$g(x) = \\cfrac{1}{1+e^{-x}}$<br>"],"metadata":{"id":"haWR7LIA6pCs"}},{"cell_type":"markdown","source":["####<font color=\"silver\">連鎖率</font>\n","- $\\overset{x}{\\longrightarrow}\n","{f}\n","\\overset{y=f(x)}{\\longrightarrow}\n","{g}\n","\\overset{z=g(y)}{\\longrightarrow}$<br><br>\n","$\\overset{\\frac{\\partial L}{\\partial x}= \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial x}= \\frac{\\partial L}{\\partial z} \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x}}{\\Longleftarrow}\n","{f}\n","\\overset{\\frac{\\partial L}{\\partial y}= \\frac{\\partial L}{\\partial z} \\frac{\\partial z}{\\partial y}}{\\Longleftarrow}\n","{g}\n","\\overset{\\frac{\\partial L}{\\partial z}}{\\Longleftarrow}$<br><br>\n","- $\\overset{x}{\\longrightarrow}\n","{f}\n","\\overset{y=f(x)}{\\longrightarrow}\n","{g}\n","\\overset{z=g(y)}{\\longrightarrow}$<br><br>\n","$\\overset{\\frac{\\partial L}{\\partial x}= \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial x}= \\frac{\\partial L}{\\partial z}  g'(y)  f'(x)}{\\Longleftarrow}\n","{f}\n","\\overset{\\frac{\\partial L}{\\partial y}= \\frac{\\partial L}{\\partial z} g'(y)}{\\Longleftarrow}\n","{g}\n","\\overset{\\frac{\\partial L}{\\partial z}}{\\Longleftarrow}$<br><br>\n","- <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://becominghuman.ai/back-propagation-in-convolutional-neural-networks-intuition-and-code-714ef1c38199)</font></font><br>\n","<img src=\"https://miro.medium.com/max/1100/1*q1M7LGiDTirwU-4LcFq7_Q.webp\" width=\"640\"><br>\n"],"metadata":{"id":"-POXXHGEsvxE"}},{"cell_type":"markdown","source":["####<font color=\"silver\">損失関数</font>\n"," - $ L = - \\cfrac{1}{N}\\displaystyle\\sum_{n=1}^{N}\\sum_{k=1}^{K} t_{nk} \\log y_{nk}$<br><br>\n","$\\scriptsize\\begin{bmatrix}\n","1 \\\\\n","0 \\\\\n","\\vdots \\\\\n","0\n","\\end{bmatrix}^T\n","\\times\n","\\ \\begin{bmatrix}\n","0.6 \\\\\n","0.01 \\\\\n","\\vdots \\\\\n","0.4\n","\\end{bmatrix} = -1 \\times \\log 0.6 = 0.51$<br><br><br><br>\n"," - $\\overset{x}{\\longrightarrow}\n","{\\rm sigmoid}\n","\\overset{t,y}{\\longrightarrow}\n","{\\rm Binary Cross Entropy}\n","\\overset{\\mathtt{L}}{\\longrightarrow}$<br><br>\n","$\\overset{\\frac{\\partial L}{\\partial x}= \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial x} = y-t}{\\Longleftarrow}\n","{\\rm sigmoid}\n","\\overset{\\frac{\\partial L}{\\partial y} = -\\frac{t}{y} + \\frac{1 - t}{1 - y}}{\\Longleftarrow}\n","{\\rm Binary Cross Entropy}\n","\\overset{\\mathtt{1}}{\\Longleftarrow}$<br><br>\n","$\\scriptsize\\displaystyle\\frac{\\partial L}{\\partial x}= \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial x} =  -\\frac{t}{y} + \\frac{1 - t}{1 - y}･y(1-y)= y-t$<br><br><br><br>\n"," - $\\overset{\\mathtt{a}}{\\longrightarrow}\n","{\\rm softmax}\n","\\overset{t,y}{\\longrightarrow}\n","{\\rm Cross Entropy}\n","\\overset{\\mathtt{L}}{\\longrightarrow}$<br><br>\n","$\\overset{\\frac{\\partial L}{\\partial x}= \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial x} = y-t}{\\Longleftarrow}\n","{\\rm softmax}\n","\\overset{\\frac{\\partial L}{\\partial y} = -\\frac{t}{y}}{\\Longleftarrow}\n","{\\rm Cross Entropy}\n","\\overset{\\mathtt{1}}{\\Longleftarrow}$<br><br>\n","$\\scriptsize\\displaystyle\\frac{\\partial L}{\\partial x}= \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial x} =  -\\frac{t}{y}･\\frac{\\partial y}{\\partial x}= y-t$<br><br><br><br>\n"," - $\\overset{x}{\\longrightarrow}\n","{\\rm Liner}\n","\\overset{t,y}{\\longrightarrow}\n","{\\rm Mean Squared Error}\n","\\overset{\\mathtt{L}}{\\longrightarrow}$<br><br>\n","$\\overset{\\frac{\\partial L}{\\partial x}= \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial x} = y-t}{\\Longleftarrow}\n","{\\rm Liner}\n","\\overset{\\frac{\\partial L}{\\partial y} = y-t}{\\Longleftarrow}\n","{\\rm Mean Squared Error}\n","\\overset{\\mathtt{1}}{\\Longleftarrow}$<br><br>"],"metadata":{"id":"s0-pjdu2wGqg"}},{"cell_type":"markdown","source":["####<font color=\"silver\">BatchNorm</font>\n","<font color=\"silver\">学習時<br></font><br>\n","$\\displaystyle \\mu = \\frac{1}{n}\\sum_{i=1}^{n}x_i\\qquad\\displaystyle \\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i-\\mu)^2\\qquad\\displaystyle \\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2+\\epsilon}}\\qquad\\displaystyle y_i = \\gamma \\hat{x}_i + \\beta$<br><br>\n","<font color=\"silver\">予測時<br></font><br>\n","$\\displaystyle\\mathbb{E}[x]=\\mathbb{E}_\\mathcal{B}[\\mu_\\mathcal{B}]$<br><br>\n","$\\displaystyle\\text{Var}[x]=\\frac{m}{m-1}\\mathbb{E}_\\mathcal{B}[\\sigma^2_\\mathcal{B}]$<br><br>\n","$\\mathbb{E}[x] \\leftarrow \\eta \\times \\mathbb{E}[x] + (1-\\eta)\\times \\mu_{\\mathcal{B}}$<br><br>\n","$\\text{Var}[x] \\leftarrow \\eta \\times \\text{Var}[x] + (1-\\eta)\\times \\sigma^2_{\\mathcal{B}}$<br><br>\n","$\\begin{align} y&=\\gamma\\cdot \\widehat{x}+\\beta\\\\ &=\\gamma\\frac{x-\\mathbb{E}[x]}{\\sqrt{\\text{Var}[x]+\\epsilon}}+\\beta\\\\ &=\\frac{\\gamma}{\\sqrt{\\text{Var}[x]+\\epsilon}}\\cdot x+\\left(\\beta – \\frac{\\gamma \\mathbb{E}[x]}{\\sqrt{\\text{Var}[x]+\\epsilon}} \\right) \\end{align}$<br><br>\n","- <font color=\"silver\">Layer Norm</font><br>\n","$\\displaystyle \\bar{x}^{(l)}_n=\\frac{1}{K}\\sum^K_{k=1}{x}^{(l, k)}_n\\qquad\\displaystyle s^{(l)}_n=\\sqrt{\\frac{1}{K}\\sum^K_{k=1}\\left({x}^{(l, k)}_n-\\bar{x}^{(l, k)}_n\\right)^2}$<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://konchangakita.hatenablog.com/entry/2021/01/12/210000)<br></font>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/k/konchangakita/20210111/20210111154320.png\" width=\"800\"><br>\n","N, D = x.shape<br>\n","mu = np.mean(x, axis=0) <font color=\"silver\"> # 入力xをNの方向に平均 (D, ) <font color=\"blue\">axis=0</font><br></font>\n","mu = np.broadcast_to(mu, (N, D)) <font color=\"silver\"> # Nの方向にブロードキャスト (N, D)<br></font>\n","x_mu = x - mu <font color=\"silver\"> # 入力xから平均値を引く (N, D)<br></font>\n","var = np.mean(x_mu ** 2, axis=0) <font color=\"silver\"> # 入力xの分散, 分散は(x - mu) ** 2を平均したもの, (D, )  <font color=\"blue\">axis=0</font> <br></font>\n","std = np.sqrt(var + epsilon) <font color=\"silver\"> # 入力xの標準偏差, 標準偏差は分散の平方根, (D, ) <br></font>\n","std_inv = 1 / std <font color=\"silver\"> # 標準偏差の逆数 (D, ) <br></font>\n","std_inv = np.broadcast_to(std_inv, (N, D)) <font color=\"silver\"> # Nの方向にブロードキャスト (N, D)  <br></font>\n","x_std = x_mu * std_inv <font color=\"silver\"> # 標準化 (N, D)<br></font>\n","out = gamma * x_std + beta <font color=\"silver\"> # gammaでスケール、betaでシフト (N, D)<br><br></font>\n","moving_mean = rho * moving_mean + (1-rho) * mu<br>\n","moving_var = rho * moving_var + (1-rho) * var<br>\n","x_mu = x - moving_mean <font color=\"silver\"> # (N, D)<br></font>\n","x_std = x_mu / np.sqrt(moving_var + epsilon)  # (N, D)<br></font>\n","out = gamma * x_std + beta <font color=\"silver\"> # gammaでスケール、betaでシフト (N, D)<br><br></font>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://rikei-logistics.com/batch-normalization1)<br></font>\n","<img src=\"https://rikei-logistics.com/wp-content/uploads/2021/11/1-3-5.png\" height=\"240\">\n","<img src=\"https://rikei-logistics.com/wp-content/uploads/2021/11/1-5-5.png\" height=\"240\"><br><br>\n","out = self.gamma * x_std + self.beta <font color=\"silver\"> # gammaでスケール、betaでシフト (N, D)<br></font>\n","<font color=\"red\">dbeta = np.sum(dout, axis=0) </font><font color=\"silver\"> # (D, )<br></font></font>\n","<font color=\"red\">dgamma = np.sum(self.x_std * dout, axis=0)<font color=\"silver\"># (D, )<br></font></font>\n","<font color=\"red\">dx_std = self.gamma * dout  </font><font color=\"silver\">  # (N, D)<br></font></font>\n","x_std = x_mu * std_inv <font color=\"silver\"> # 標準化 (N, D)<br></font>\n","<font color=\"red\">dx_mu_1 = dx_std / std  </font><font color=\"silver\"> # a2, Xmuの勾配(1つ目)<br></font></font>\n","<font color=\"red\">dstd_inv = dx_std * x_mu  </font><font color=\"silver\">  # a3, 標準偏差の逆数の勾配<br></font></font>\n","std_inv = np.broadcast_to(std_inv, (N, D)) <font color=\"silver\"> # Nの方向にブロードキャスト (N, D)  <br></font>\n","<font color=\"red\">dstd_inv = np.sum(dstd_inv, axis=0)  </font><font color=\"silver\">  #  a3, Nの方向に合計<br></font></font>\n","std_inv = 1 / std <font color=\"silver\"> # 標準偏差の逆数 (D, ) <br></font>\n","<font color=\"red\">dstd = -(dstd_inv) / (std * std)  </font><font color=\"silver\">  #   a4, 標準偏差の勾配<br></font></font>\n","std = np.sqrt(var + epsilon) <font color=\"silver\"> # 入力xの標準偏差 (D, ) <br></font>\n","<font color=\"red\">dvar = 0.5 * dstd / std  </font><font color=\"silver\">  # a5, 分散の勾配<br></font></font>\n","var = np.mean(x_mu**2, axis=0) <font color=\"silver\"> # 入力xの分散 (D, )  <br></font>\n","<font color=\"red\">a6 = dvar / batch_size  </font><font color=\"silver\">  # Xmuの2乗の勾配<br></font></font>\n","<font color=\"red\">a6 = np.broadcast_to(a6, (N, D))  </font> </font><font color=\"silver\"> # Nの方向にブロードキャスト\" <br></font></font>\n","<font color=\"red\">dx_mu_2 = 2.0  * x_mu * a6  </font><font color=\"silver\">  # a7, Xmuの勾配(2つ目)<br></font></font>\n","x_mu = x - mu <font color=\"silver\"> # 入力xから平均値を引く (N, D)<br></font>\n","<font color=\"red\">dmu = -(dx_mu_1+dx_mu_2)  </font><font color=\"silver\"> # a8, a2+a7, muの勾配<br></font></font>\n","mu = np.broadcast_to(mu, (N, D)) <font color=\"silver\"> # Nの方向にブロードキャスト (N, D)<br></font>\n","<font color=\"red\">dmu = np.sum(dmu, axis=0)  </font><font color=\"silver\">   a8, # Nの方向に合計<br></font></font>\n","<font color=\"black\">mu = np.mean(x, axis=0) <font color=\"silver\"> # 入力xをNの方向に平均 (D, )<br></font>\n","<font color=\"red\">a9 = dmu / batch_size  </font><font color=\"silver\">  # Xの勾配<br></font></font>\n","<font color=\"red\">a9 = np.broadcast_to(a9, (N, D))  </font><font color=\"silver\">  # Nの方向にブロードキャスト<br></font></font>\n","<font color=\"red\">dx = a2 + a7 + a9<br></font>"],"metadata":{"id":"9_tUC6pzE8tr"}},{"cell_type":"markdown","source":["####<font color=\"silver\">Attention</font>\n"," - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/ac86f1992b7beefa1f0c)</font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F1a019fc5-7cc8-5c24-f896-78c5ac20b478.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=7999c583133fcf245a0449025b3cbaa2\" width=\"800\"><br>\n","hr = h.reshape(N, 1, H).repeat(T, axis=1)<br>\n","t = hs × hr<br>\n","s = np.sum(t, axis=2)<br>\n","a = softmax.forward(s)<br>\n","ar = a.reshape(N, T, 1).repeat(H, axis=2)<br>\n","t = hs * ar<br>\n","c = np.sum(t, axis=1)<br>\n","<br>\n","c = np.sum(t, axis=1)<br>\n","<font color=\"red\">dt = dc.reshape(N, 1, H).repeat(T, axis=1)</font><br>\n","t = hs * ar<br>\n","<font color=\"red\">dar = dt * hs</font><br>\n","<font color=\"red\">dhs = dt * ar</font><br>\n","ar = a.reshape(N, T, 1).repeat(H, axis=2)<br>\n","<font color=\"red\">da = np.sum(dar, axis=2)</font><br>\n","a = softmax.forward(s)<br>\n","<font color=\"red\">ds = softmax.backward(da)</font><br>\n","s = np.sum(t, axis=2)<br>\n","<font color=\"red\">dt = ds.reshape(N, T, 1).repeat(H, axis=2)</font><br>\n","t = hs × hr<br>\n","<font color=\"red\">dhs = dt * hr</font><br>\n","<font color=\"red\">dhr = dt * hs</font><br>\n","hr = h.reshape(N, 1, H).repeat(T, axis=1)<br>\n","<font color=\"red\">dh = np.sum(dhr, axis=1)<br></font>"],"metadata":{"id":"7YnufqgfI2lt"}},{"cell_type":"markdown","source":["####<font color=\"silver\">活性化関数</font>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/kwi0303/items/e43efa6657ff4b6b59ad)</font></font><br>\n","<img src=\"https://camo.qiitausercontent.com/909ba915324bf98753519587673d4dc449a80ea6/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313532393339332f31636261353432302d653034392d313033312d356532632d3961646134633133613238642e706e67\" width=\"800\"><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/kwi0303/items/e43efa6657ff4b6b59ad)</font></font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F1529393%2F3b0ade38-d3a5-83da-9733-c1eb0fec5ddb.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=2b73d489c67e6e752dd4e4e40f1a0347\" width=\"800\"><br><br>\n","<img src=\"https://camo.qiitausercontent.com/2c0dc659bcfb529a0cedc4eae99b720a17cd2ef8/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313532393339332f36306138366533322d316534362d336537622d356265662d3764323339643364656361352e706e67\" width=\"800\"><br>"],"metadata":{"id":"Qqk-jwVlpZ_K"}},{"cell_type":"markdown","source":["####<font color=\"silver\">Dropout</font>\n","torch.nn.Dropout(p=0.5, inplace=False)<br><br>\n","class Dropout:<br>\n","$\\quad$def __ init __(self, dropout_ratio=0.5):<br>\n","$\\quad$$\\quad$self.dropout_ratio = dropout_ratio<br>\n","$\\quad$$\\quad$self.mask = None<br>\n","$\\quad$def forward(self, x, train_flg=True):<br>\n","$\\quad$$\\quad$if train_flg: <font color=\"silver\"> # 0〜1の乱数で x.shape の行列を生成</font><br>\n","$\\quad$$\\quad$$\\quad$self.mask = np.random.rand(*x.shape) > self.dropout_ratio <font color=\"blue\">$~\\leftarrow\\ast$</font><br>\n","$\\quad$$\\quad$$\\quad$return x * self.mask<br>\n","$\\quad$$\\quad$else:<br>\n","$\\quad$$\\quad$$\\quad$return x * (1.0 - self.dropout_ratio) <font color=\"blue\">$~\\leftarrow\\ast$</font><br>\n","$\\quad$ def backward(self, dout):<br>\n","$\\quad$$\\quad$return dout * self.mask <font color=\"blue\">$~\\leftarrow\\ast$</font><br>"],"metadata":{"id":"9iMeeGkPQzp6"}},{"cell_type":"markdown","source":["####<font color=\"silver\">重みの初期値</font>\n","torch.nn.init.normal_(tensor, mean=0.0, std=1.0)<br>\n","torch.nn.init.xavier_normal_(tensor, gain=1.0)<br>\n","torch.nn.init.kaiming_normal_(tensor, a=0, mode=\"fan_in\", nonlinearity=\"leaky_relu\") <br><br>\n","Xavierの初期値</font><br>\n","sigmoid関数やtanh関数のように点対称で中央付近で線形関数としてみなせる活性化関数に向いている。<br><br>\n","$\\sqrt {\\dfrac {1}{n}},\\quad\\displaystyle \\sqrt \\frac{2}{n1+n2}$を標準偏差とするガウス分布<br><br>\n","n1：<font color=\"silver\">前の層のノード数,</font> n2：<font color=\"silver\">後ろの層のノード数</font><br><br>\n","Heの初期値</font><br>\n","Xavierより広がりを持った初期値<br><br>\n","<font color=\"Black\">$\\sqrt {\\dfrac {2}{n}},\\quad\\displaystyle \\sqrt \\frac{2}{n1}$を標準偏差とするガウス分布<br><br>\n","n1：<font color=\"silver\">前の層のノード数</font><br>"],"metadata":{"id":"Hq5eruwsO6_Y"}},{"cell_type":"markdown","source":["####<font color=\"silver\">WeightDecay</font>\n","l2_loss = l2_loss + torch.norm(w) ** 2<br>\n","l1_loss = l1_loss + torch.norm(w, 1)<br><br> \n","$\\displaystyle L = L' + \\frac{1}{2} \\lambda \\sum_{l=1}^{layers} \\sum_{i, ~~j} (w_{ij}^{l})^2$<br>  \n","$\\displaystyle \\frac{\\partial L}{\\partial {\\bf W}_t} = \\frac{\\partial L'}{\\partial {\\bf W}_t} + \\lambda {\\bf W}_{t}$<br>  \n","$\\bf W$ :  <font color=\"silver\">重み行列  </font><br>$w_{i,j}^l$ :<font color=\"silver\"> $l$層目の重み行列${\\bf W}$の$i, j $成分<br></font>\n","$L$ :<font color=\"silver\"> 正則化項を加えた後の損失<br></font>\n","$L'$ : <font color=\"silver\">正則化項を加える前の損失 <br></font>\n","$layers$ :<font color=\"silver\"> 層番号<br></font>\n","$\\lambda$ :<font color=\"silver\"> 係数<br></font><br>\n","<font color=\"silver\"># 損失の補正</font><br>\n","weight_decay = 0<br>\n","for idx in range(1, self.hidden_layer_num + 2):<br>\n","$\\qquad$W = self.params['W' + str(idx)]<br>\n","$\\qquad$weight_decay += 0.5 * lmd * np.sum(W**2) <font color=\"silver\"> # 全ての行列Wについて積算していく</font><br>\n","return self.lastLayer.forward(y, t) + weight_decay <br>\n","<font color=\"silver\"># 勾配の補正</font><br>\n","grads = {}<br>\n","for idx in range(1, self.hidden_layer_num+2):<br>\n","$\\qquad$grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + lmd * self.layers['Affine' + str(idx)].W<br>\n","$\\qquad$grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db<br>\n","return grads<br>"],"metadata":{"id":"c3cuHpl6Nkbi"}},{"cell_type":"markdown","source":["#### <font color=\"silver\">勾配クリッピング</font>\n","torch.nn.utils.clip_grad_norm_(parameters, max_norm, norm_type=2.0, error_if_nonfinite=False)<br>\n","torch.nn.utils.clip_grad_norm_(model.parameters(), clip)<br><br> \n","勾配のL2ノルムがしきい値を超えた場合に、$g$ に$\\cfrac{v}{\\|g\\|}$を掛けて勾配を修正する。<br>\n","<br>\n","$if \\parallel{g}\\parallel > v:$<br><br>\n","$\\displaystyle g← \\frac{v}{\\|g\\|}g$<br><br>\n","$g$ ：<font color=\"silver\">すべてのパラメータに対する勾配をひとひとつにまとめたもの。</font><br><br>\n","$\\rm -threshold < \\parallel gradient \\parallel < threshold$<br><br>\n","$\\displaystyle\\|g\\|= \\sqrt{ \\Bigl(\\frac{\\partial L}{\\partial \\mathbf{W}_1}\\Bigr)^2  + \\Bigl(\\frac{\\partial L}{\\partial \\mathbf{W}_2}\\Bigr)^2}$<br><br>\n","def clip_grads(grads, max_norm):<br>\n","$\\quad$total_norm = 0<br>\n","$\\quad$for key, grad in grads.items():<br>\n","$\\quad$$\\quad$total_norm += np.sum(grad ** 2, axis=None)<br>\n","$\\quad$total_norm = np.sqrt(total_norm)<br>\n","$\\quad$rate = max_norm / (total_norm + 1e-6)<br>\n","$\\quad$if rate < 1:<br>\n","$\\quad$$\\quad$for key, grad in grads.items():<br>\n","$\\quad$$\\quad$$\\quad$grads[key] *= rate<br>\n","def gradient_clipping(grad, threshold):<br>\n","$\\quad$norm = np.linalg.norm(grad)<br>\n","$\\quad$rate = threshold / norm<br>\n","$\\quad$if rate < 1:<br>\n","$\\quad$$\\quad$return grad * rate<br>\n","$\\quad$return grad<br>\n","<font color=\"silver\">returnを指定しない関数のため値を返さず、引数に渡したgradsの値を直接更新する</font><br>"],"metadata":{"id":"Nl8wAP9DP5Yo"}},{"cell_type":"markdown","source":["####<font color=\"silver\">ラベル平滑化</font>\n","正解ラベルとして１が、不正解ラベルとして０が設定されている教師データについて、 正解ラベルを1 – εに、不正解ラベルをε/(k − 1)として設定する。</font><br><br>\n","$\\begin{align}\n","t_k^{LS} = (1 – \\epsilon) t_k + \\epsilon u(k) \n","\\end{align}$\n","<br><br>\n","$\\begin{align}\n","L \n"," &= – \\sum_{i=1}^N \\sum_{k=1}^K t_k^{LS} \\log p_{\\theta}(\\pmb{y}^{(i)} | \\pmb{x}^{(i)}) \\\\\n","&= – \\sum_{i=1}^N \\sum_{k=1}^K [ (1 – \\epsilon) t_k + \\epsilon u(k)] \\log p_{\\theta}(\\pmb{y}^i |\\pmb{x}^i) \n","\\end{align}$"],"metadata":{"id":"ZKuMv4Lt85pT"}},{"cell_type":"markdown","source":["####<font color=\"silver\">オプティマイザー</font>\n"," - <font color=\"silver\">SGD</font><br><br>\n","$\\displaystyle {\\boldsymbol \\theta}_{t+1} = {\\boldsymbol \\theta}_t - \\eta \\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t}$<br><br>\n","while True:<br>\n","$\\quad$dx = compute_gradient(x)<br>\n","$\\quad$x -= learning_rate * dx<br><br>\n"," - <font color=\"silver\">Momentum</font><br><br>\n","$\\displaystyle {\\boldsymbol v}_{t+1} = \\alpha {\\boldsymbol v}_t - \\eta \\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t}$\n","<br>\n","$\\displaystyle {\\boldsymbol \\theta}_{t+1} = {\\boldsymbol \\theta}_t + {\\boldsymbol v}_{t+1}$<br>\n","<br>\n","$\\alpha$ :  <font color=\"silver\">モーメンタム係数(0以上1未満)</font><br>\n","<br>\n","vx = 0<br>\n","rho = 0.9<br>\n","while True:<br>\n","$\\quad$dx = compute_gradient(x)<br>\n","$\\quad$vx = rho * vx - learning_rate * dx<br>\n","$\\quad$x += vx<br><br>\n"," - <font color=\"silver\">AdaGrad</font><br><br>\n","$\\displaystyle {\\boldsymbol h}_{t+1} = \\rho{\\boldsymbol h}_{t}+ (1-\\rho)\\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t} \\odot \\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t}$<br>\n","$\\displaystyle {\\boldsymbol \\theta}_{t+1} =  {\\boldsymbol \\theta}_t - \\eta \\frac{1}{\\sqrt{{\\boldsymbol h}_{t+1}+\\epsilon}} \\odot \\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t}$<br><br>\n","delta = 1e-7<br>\n","grad_squared = 0<br>\n","while True:<br>\n","$\\quad$dx = compute_gradient(x)<br>\n","$\\quad$grad_squared += dx * dx<br>\n","$\\quad$x -= learning_rate * dx / (np.sqrt(grad_squared) + delta)<br><br>\n"," - <font color=\"silver\">RMSprop</font><br><br>\n","$\\displaystyle {\\boldsymbol h}_{t+1} = \\rho{\\boldsymbol h}_{t}+ (1-\\rho)\\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t} \\odot \\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t}$<br>\n","$\\displaystyle {\\boldsymbol \\theta}_{t+1} =  {\\boldsymbol \\theta}_t - \\eta \\frac{1}{\\sqrt{{\\boldsymbol h}_{t+1}+\\epsilon}} \\odot \\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t}$<br><br>\n","$\\rho$ : <font color=\"silver\">減衰率. 0.9など<br></font>\n","<br>\n","delta = 1e-7<br>\n","grad_squared = 0<br>\n","decay_rate = 0.9<br>\n","while True:<br>\n","$\\quad$dx = compute_gradient(x)<br>\n","$\\quad$grad_squared = decay_rate * grad_squared + (1-decay_rate) * dx * dx<br>\n","$\\quad$x -= learning_rate * dx / (np.sqrt(grad_squared) + delta)<br><br>\n"," - <font color=\"silver\">Adam</font><br><br>\n","$\\displaystyle {\\boldsymbol m}_{t+1} = \\rho_1 {\\boldsymbol m}_{t} + (1 - \\rho_1)\\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t}$\n","<br>\n","$\\displaystyle {\\boldsymbol v}_{t+1} = \\rho_2 {\\boldsymbol v}_{t} + (1 - \\rho_2)\\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t} \\odot \\frac{\\partial L}{\\partial {\\boldsymbol \\theta}_t}$\n","<br>\n","$\\displaystyle {\\hat{\\boldsymbol m}}_{t+1} = \\frac{{\\boldsymbol m}_{t+1}}{1-\\rho_1^t}$<br>\n","$\\displaystyle {\\hat{\\boldsymbol v}}_{t+1} = \\frac{{\\boldsymbol v}_{t+1}}{1-\\rho_2^t}$<br>\n","$\\displaystyle {\\boldsymbol \\theta}_{t+1} = {\\boldsymbol \\theta}_t - \\eta \\frac{1}{\\sqrt{\\hat{\\boldsymbol v}_{t+1}}+\\epsilon} \\odot {\\hat{\\boldsymbol m}}_{t+1}$<br>\n","<br>\n","$\\rho_1$:  <font color=\"silver\">減衰率. 0.9が推奨されている</font><br>\n","$\\rho_2$: <font color=\"silver\">減衰率. 0.99が推奨されている</font><br>\n","<br>\n","first_moment = 0<br>\n","second_moment = 0<br>\n","beta1, beta2 = 0.9, 0.999<br>\n","for i in range(num_iterations):<br>\n","$\\quad$dx = compute_gradient(x)<br>\n","$\\quad$first_moment = beta1 *  first_moment + (1 - beta1)  * dx<br>\n","$\\quad$second_moment = beta2 *  second_moment + (1 - beta2)  *  dx * dx<br>\n","$\\quad$first_unbias = first_moment / (1 - beta1 * * i)<br>\n","$\\quad$second_unbias = second_moment / (1 - beta2 * * i)<br>\n","$\\quad$x -= learning_rate * first_unbias / (np.sqrt(second_unbias) + delta)<br><br>"],"metadata":{"id":"iCDXwcqi_0Q9"}},{"cell_type":"markdown","source":["####<font color=\"silver\">混合行列</font>\n","\n","|    |  実際にはポジティブ（P）  |  実際にはネガティブ（N）  |\n","| :----: | :----: | :----: |\n","|  予測がポジティブ（P） |TP|FP|\n","|  予測がネガティブ（N） |FN|TN|\n","\n","<br>\n","\n","|    |  迷惑なメールだった（P）  |  普通のメールだった（N）  |\n","| :----: | :----: | :----: |\n","|  迷惑メールに分類（P） |迷惑メールと的中（TP）|迷惑メールと誤認（FP）|\n","|  普通メールに分類（N） |普通メールと誤認（FN）|普通メールと的中（TN）|\n","\n","<br>\n","\n","正解率, $\\scriptsize\\rm{Accuracy} = \\cfrac{TP + TN}{TP + TN + FP + FN}= \\cfrac{迷惑メールと的中したもの}{すべてのメール}$\n","<br><br>\n","適合率, 誤検知に強い, $\\scriptsize\\rm{Precision} = \\cfrac{TP}{TP + FP}  = \\cfrac{迷惑メールと的中したもの}{迷惑メールに分類したもの}$\n","<br><br>\n","再現率, 見逃しに強い, $\\scriptsize\\rm{Recall} = \\cfrac{TP}{TP + FN}= \\cfrac{迷惑メールと的中したもの}{迷惑メールだったもの}$\n","<br><br>\n","$\\scriptsize\\rm{F1₋score} = \\cfrac{2 \\times \\textrm{recall} \\times \\textrm{precision}}{\\textrm{recall} + \\textrm{precision}} = \\cfrac{2 TP}{2 TP+FP+FN} $\n","<br><br>\n","Precision（正解と判断したものが本当に正解か）は正直どうでもいい。精密検査をすればいいだけ。それよりもRecallが1より小さい方がまずい。ガンじゃないと判断して、ガンだったら訴訟モノなので、是が非でもRecallを1に持っていきたい。"],"metadata":{"id":"waim0FQLLygc"}},{"cell_type":"markdown","source":["####<font color=\"silver\">ROC曲線とPR曲線</font>\n","\n","|    |  実際にはポジティブ（P）  |  実際にはネガティブ（N）  |\n","| :----: | :----: | :----: |\n","|  予測がポジティブ（P） |TP|FP|\n","|  予測がネガティブ（N） |FN|TN|\n","\n","<br>\n","\n","|    |  迷惑なメールだった（P）  |  普通のメールだった（N）  |\n","| :----: | :----: | :----: |\n","|  迷惑メールに分類（P） |迷惑メールと的中（TP）|迷惑メールと誤認（FP）|\n","|  普通メールに分類（N） |普通メールと誤認（FN）|普通メールと的中（TN）|\n","\n","<br>\n","$~~~$真陽性率, $\\scriptsize\\rm{TPR}= \\cfrac{TP}{TP + FN}= \\cfrac{迷惑メールと的中したもの}{迷惑メールだったもの}$\n","<br><br>\n","$~~~$偽陽性率, $\\scriptsize\\rm{FPR} = \\cfrac{FP}{FP + TN}= \\cfrac{迷惑メールと誤認したもの}{普通メールだったもの}$\n","<br><br>\n","\n","| ランキング | 真実 | TPR=Recall | FPR | Precision|\n","| :------: | :------: | :------: | :------: | :------: |\n","|1|1|1÷3=0.333|0.000|1.000|\n","|2|0|1÷3=0.333|0.250|0.500|\n","|3|1|2÷3=0.666|0.250|0.666|\n","|4|1|3÷3=1.000|0.250|0.750|\n","|5|0|3÷3=1.000|0.250|0.600|\n","|6|0|3÷3=1.000|0.250|0.500|\n","|7|0|3÷3=1.000|0.250|0.286|\n","\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/skyshk/items/016cd1820650ea78d101)</font></font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F51256%2F1c77dedc-bce7-46eb-05ca-5e745c890522.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=6e3792eeee199decaf4b8cb997396145\" width=\"240\">\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F51256%2Fdfd20912-ea95-9a8e-b1b9-4b2b1a6be97a.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=dd9c26a928a4ffd02ca83cf3326a121d\" width=\"240\">"],"metadata":{"id":"JyZhK5W3XzrR"}},{"cell_type":"markdown","source":["#### <font color=\"silver\">mAP</font>\n","\n","|    |  実際にはポジティブ（P）  |  実際にはネガティブ（N）  |\n","| :----: | :----: | :----: |\n","|  予測がポジティブ（P） |TP|FP|\n","|  予測がネガティブ（N） |FN|TN|\n","\n","<br>\n","\n","|    |  迷惑なメールだった（P）  |  普通のメールだった（N）  |\n","| :----: | :----: | :----: |\n","|  迷惑メールに分類（P） |迷惑メールと的中（TP）|迷惑メールと誤認（FP）|\n","|  普通メールに分類（N） |普通メールと誤認（FN）|普通メールと的中（TN）|\n","\n","<br>\n","\n","|    |  IoU ≥ 0.5（P）  |  IoU < 0.5（N）  |\n","| :----: | :----: | :----: |\n","|  Predicted BBoxに対してGround-truth BBoxが（P） |十分に重なっている（TP）|重ならなかったPrediction Bboxの数（FP）|\n","|  Ground-truth BBoxに対してPredicted BBoxが（N） |重ならないことは予測していないので無し|重ならなかったGT Bboxの数（TN）|\n","\n","<br>\n","\n","$~~~$$\\scriptsize{\\rm Precision} = \\cfrac{\\rm TP}{\\rm TP + FP}= \\cfrac{それまで見てきた十分に重なっている{\\rm True}の数}{それまで見てきた{\\rm Predicted BBoxの総数}}$\n","<br><br>\n","$~~~$$\\scriptsize{\\rm Recall} = \\cfrac{\\rm TP}{\\rm TP + FN}= \\cfrac{それまで見てきた十分に重なっている{\\rm True}の数}{全ての{\\rm GT BBox}の数}$\n","<br><br>\n","\n","| Sorted Number | Confidence Score(%) | Correct? | Precision | Recall |\n","|------------------:|:-----------------------:|:------------:|:-------------:|:----------:|\n","|                 #1|           96            |     True     |    1/1 = 1    | 1/5 = 0.2|  \n","|                 #2|           92            |     True     |    2/2 = 1    | 2/5 = 0.4 |\n","|                 #3|           89            |     False    |    2/3 = 0.667| 2/5 = 0.4 |\n","|                 #4|           88            |     False    |    2/4 = 0.5  | 2/5 = 0.4 |\n","|                 #5|           84            |     False    |    2/5 = 0.4  | 2/5 = 0.4 |\n","|                 #6|           83            |     True     |    3/6 = 0.5  | 3/5 = 0.6 |\n","|                 #7|           80            |     True     |    4/7 = 0.571| 4/5 = 0.8 |\n","|                 #8|           78            |     False    |    4/8 = 0.5  | 4/5 = 0.8 |\n","|                 #9|           74            |     False    |    4/9 = 0.444| 4/5 = 0.8 |\n","|                #10|           72            |     True     |    5/10 = 0.5| 5/5 = 1.0 |\n","\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/skyshk/items/016cd1820650ea78d101)</font></font><br>\n","<img src=\"https://camo.qiitausercontent.com/258ee1e3dc7cdd87faa0f8f011fb0a4b0cad2956/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3139313331332f31656539653364392d353762332d656464642d313461642d3532633432333361366231342e6a706567\" width=\"480\">\n","<br><br>\n","$\\begin{align}\n","\\rm{AP}\n","&= \\displaystyle\\int_0^1 p(r) dr\\\\\n","&= \\cfrac{1}{11}*(p(0) + p(0.1) + ・・・ + p(1.0))\\\\\n","&= \\frac{1}{11}*(1.0 * 5 + 0.571 * 4 + 0.5 * 2) = \\frac{1}{11}*8.284 = 0.753\n","\\end{align}$\n","<br><br>\n","$\\rm AP_{StopSign} = 0.753,\\;AP_{TrafficLight} = 0.990,\\;AP_{Car} = 0.683\\;$のとき\n","<br><br>\n","$\\rm mAP = \\frac{1}{3} * (0.753 + 0.990 + 0.683) = 0.8086$"],"metadata":{"id":"XH0zwqxCYMCw"}},{"cell_type":"markdown","source":["#### <font color=\"silver\">コサイン類似度</font>\n","※ コサイン類似度とは、ベクトルどうしの類似度を測る指標。</font><br>\n","※ 埋め込みベクトルどうしの類似度を測る際にも使用される。</font><br><br>\n","$d(\\pmb{x}, \\pmb{y}) = \\cfrac{x\\,y}{||x||\\,||y||} = \\cfrac{x_1y_1 + x_2y_2 + \\cdots + x_ny_n}{\\sqrt{x_1^2\\cdots+x_n^2}\\sqrt{y_1^2\\cdots+y_n^2}}$<br>\n","<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://yolo-kiyoshi.com/2020/08/26/post-2234/)</font></font><br>\n","<img src=\"https://yolo-kiyoshi.com/wp/wp-content/uploads/2020/08/20200825_cosine-1.jpeg\" width=\"320\">"],"metadata":{"id":"sH1T7bGMY4kJ"}},{"cell_type":"markdown","source":["#### <font color=\"silver\">パープレキシティ</font>\n","※ 言語モデルを評価するための指標。<br>\n","※ 言語モデルは次の単語を明確に判別できる方が精度が高いとみなせる</font><br>\n","※ perplextityは理論的には「次の単語として絞り込めた候補単語数」と言える</font><br>\n","→ perplexityが低いほど、単語を絞り込めているのでいいモデル</font><br>\n","<br>\n","※ 確率の逆数という意味を持っており、正解となる単語を選ぶ難しさ、と解釈できる。<br>\n","※ $y$ = 0.1のとき、その確率の逆数は10。これは、 10個の単語候補の中から正解\n","となる単語を見つけるくらい難しい、と解釈できる。<br>\n","※ 正解の単語に対応する出力値(確率)を$y$とすると、$e^{-\\log y}=(e^{\\log y})^{-1}=\\cfrac{1}{y}$であるため、パープレキシティは、確率の逆数という意味になる。<br><br>\n","$\\mathrm{perplexity} = \\exp(-\\log y_k) = y_k^{-1} = \\cfrac{1}{y_k}$<br><br>\n","\n","※ Hはある文の単語あたりのエントロピーである。</font><br>\n","※ cross entropyに対して最適化することとperplexityに対して最適化することは同じ。</font><br>\n","※ 1以上の値をとり、値が小さいほどモデルの性能が良い。</font><br><br>\n","$\\displaystyle \\text{ppl}=\\exp\\left(-\\frac{1}{N}\\sum_{n}\\sum_k t_{n,k}\\log p_\\text{model}(y_{n, k})\\right)$ <br><br>\n","$\\displaystyle \\begin{align} \\text{ppl}&=\\exp (L)\\\\ L&=-\\frac{1}{N}\\sum_{n}\\sum_k t_{n,k}\\log p_\\text{model}(y_{n, k}) \\end{align}$<br>\n","<br>\n","y = np.arange(0.1, 1.01, 0.01)<br>\n","L = - np.log(y)<br>\n","perplexity = np.exp(L)<br><br>\n","<font color=\"silver\">$\\text{ppl}=\\exp\\left(-\\frac{1}{N}\\sum_{n} \\log (\\text{正解となる単語の予測確率})\\right)$ </font><br>"],"metadata":{"id":"btu05goFZnfR"}},{"cell_type":"markdown","source":["#### <font color=\"silver\">BLEU</font>\n","$\\displaystyle{\\rm BLEU}={\\rm BP}\\exp\\left(\\sum_{n=1}^N w_n\\log p_n\\right)$\n","<br><br>\n","$\\displaystyle{\\rm BLEU}=\\min\\Big(1, \\exp\\big(1-\\frac{\\text{r}}{\\text{c}}\\big)\\Big)\\exp\\left(\\frac{1}{N}\\sum_{n=1}^N \\log p_n\\right)$<br><br>\n","$\\rm BP$：<font color=\"silver\">翻訳文が短い文のときに$P_n$が高くなってしまうことに対するペナルティ</font><br>\n","$w_n$：<font color=\"silver\">重み, 1/4とすることが多い</font><br>\n","$N$：<font color=\"silver\">N-gramまで考慮する, 4とすることが多い</font><br>\n","$p_n$：<font color=\"silver\">修正されたn-gramの精度, 適合率</font><br>\n","$c$：<font color=\"silver\">candidate, 翻訳タスクなどによって生成された文の長さ</font><br>\n","$r$：<font color=\"silver\">reference, 正解の文の長さ</font><br><br>\n"," - Brevity Penalty</font><br><br>\n","$\\text{BP} \n","\\begin{cases} \n","  1 & c > r \\\\ \n","  e^{1-r/c} & c \\leqq  r \n","\\end{cases}$<br><br>\n"," - nグラム適合率, Modified Precision</font><br><br>\n","$\\displaystyle p_n=\\frac{\\sum_{C\\in\\{\\rm Candidates\\}}\\sum_{\\text{n-gram}\\in C}{\\rm Count}_{\\rm clip}(\\text{n-gram})}{\\sum_{C'\\in \\{\\rm Candidates\\}}\\sum_{\\text{n-gram}'\\in C'}{\\rm Count}(\\text{n-gram}')}$<br><br>\n","<img src=\"https://machinelearninginterview.com/wp-content/uploads/2021/11/image-3.png\" width=\"320\"><br><br>\n","<img src=\"https://i0.wp.com/kikaben.com/wp-content/uploads/2022/04/image-372.png?w=800&ssl=1\" width=\"480\"><br><br>\n","<img src=\"https://i0.wp.com/kikaben.com/wp-content/uploads/2022/04/image-374.png?w=800&ssl=1\" width=\"480\"><br><br>\n","<img src=\"https://i0.wp.com/kikaben.com/wp-content/uploads/2022/04/image-371.png?w=800&ssl=1\" width=\"480\"><br><br>\n","<img src=\"https://i0.wp.com/kikaben.com/wp-content/uploads/2022/04/image-367.png?w=800&ssl=1\" width=\"480\"><br>\n","<img src=\"https://i0.wp.com/kikaben.com/wp-content/uploads/2022/04/image-366.png?w=800&ssl=1\" width=\"480\"><br>"],"metadata":{"id":"3CD92nmZrn-s"}},{"cell_type":"markdown","source":["####<font color=\"silver\">Affine</font>\n","$\\pmb{Y} = \\pmb{X} \\pmb{W}+ \\pmb{B} \\qquad \\cfrac{\\partial L}{\\partial \\pmb{X}}= \\cfrac{\\partial L}{\\partial \\pmb{Y}}\\pmb{W}^{\\top} \\qquad \\cfrac{\\partial L}{\\partial \\pmb{W}}= \\pmb{X}^{\\top} \\cfrac{\\partial L}{\\partial \\pmb{Y}}  \\qquad \\cfrac{\\partial L}{\\partial \\pmb{B}} = \\cfrac{\\partial L}{\\partial \\pmb{Y}}$<br><br>\n","out = np.dot(x, W) + b<br>\n","<font color=\"red\">dx = np.dot(dout, W.T)<br>\n","dW = np.dot(x.T, dout)<br>\n","db = np.sum(dout, axis=0)</font><br>\n"],"metadata":{"id":"ojUZLlWUKRDB"}},{"cell_type":"markdown","source":["####<font color=\"silver\">Conv</font>\n","$\\displaystyle (I*K)(i,j) = \\sum_{m}\\sum_{n}I_{I+m,j+n}K_{m,n}$<br><br>\n","$\\scriptsize \\rm OH = \\cfrac{H + 2P - FH}{S} + 1 \\qquad OW = \\cfrac{W + 2P - FW}{S} + 1$<br><br>\n","(input_size + 2*pad - filter_size) / stride + 1<br><br>\n","np.pad(img, [(上, 下), (左, 右)], 'constant')<br><br></font>"],"metadata":{"id":"2SCzJbFbL2uI"}},{"cell_type":"markdown","source":["####<font color=\"silver\">RNN</font>\n","$\\mathbf{H}_t^{(l)} = \\tanh(\\mathbf{H}_t^{(l-1)} \\mathbf{W}_{xh}^{(l)} + \\mathbf{H}_{t-1}^{(l)} \\mathbf{W}_{hh}^{(l)}  + \\mathbf{b}_h^{(l)})$<br><br>\n","t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b<br>\n","h_next = np.tanh(t)<br><br>\n","h_next = np.tanh(t)<br>\n","<font color=\"red\">dt = dh_next * (1 - h_next ** 2)</font><br>\n","t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b<br>\n","<font color=\"red\">db = np.sum(dt, axis=0)<br>\n","dWh = np.dot(h_prev.T, dt)<br>\n","dh_prev = np.dot(dt, Wh.T)<br>\n","dWx = np.dot(x.T, dt)<br>\n","dx = np.dot(dt, Wx.T)</font><br>"],"metadata":{"id":"Y7Vmp6y4W-Hr"}},{"cell_type":"markdown","source":["####<font color=\"silver\">LSTM</font>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://d2l.ai/chapter_recurrent-modern/gru.html)</font><br>\n","<img src=\"https://d2l.ai/_images/lstm-3.svg\" width=\"480\"><br><br>\n","$\\mathbf{F}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xf} + \\mathbf{H}_{t-1} \\mathbf{W}_{hf} + \\mathbf{b}_f)$<br>\n","$\\mathbf{I}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xi} + \\mathbf{H}_{t-1} \\mathbf{W}_{hi} + \\mathbf{b}_i)$<br>\n","$\\mathbf{O}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xo} + \\mathbf{H}_{t-1} \\mathbf{W}_{ho} + \\mathbf{b}_o)$<br>\n","$\\tilde{\\mathbf{C}}_t = \\text{tanh}(\\mathbf{X}_t \\mathbf{W}_{xc} + \\mathbf{H}_{t-1} \\mathbf{W}_{hc} + \\mathbf{b}_c)$<br>\n","$\\mathbf{C}_t = \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t$<br>\n","$\\mathbf{H}_t = \\mathbf{O}_t \\odot \\tanh(\\mathbf{C}_t)$"],"metadata":{"id":"eOSTddtH8T3L"}},{"cell_type":"markdown","source":["####<font color=\"silver\">LSTM</font>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/35f6f0d26f9e58f01e4e)</font><br>\n","<img src=\"https://camo.qiitausercontent.com/b9387ed37d1f247f8f679184f8dd2efd1e7a709a/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f35306164343264652d643433642d333162392d333761652d3637666462313032393939612e706e67\" width=\"480\"><br><br>\n","N, H = h_prev.shape<br>\n","A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b<br>\n","f = A[:, :H]<br>\n","g = A[:, H:2H]<br>\n","i = A[:, 2H:3H]<br>\n","o = A[:, 3H:]<br>\n","f = sigmoid(f)<br>\n","g = np.tanh(g)<br>\n","i = sigmoid(i)<br>\n","o = sigmoid(o)<br>\n","c_next = f * c_prev + g * i<br>\n","tanh_c_next = np.tanh(c_next)<br>\n","h_next = o * tanh_c_next<br><br>"],"metadata":{"id":"EKETunYbVjuC"}},{"cell_type":"markdown","source":["####<font color=\"silver\">LSTM</font>\n","<img src=\"https://camo.qiitausercontent.com/bd2fad9749a2dd322a1dd9f8dcfbcf3ee209107a/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f62366631386438372d613436662d643762302d303435382d3733656163373434663465662e706e67\" width=\"480\"><br><br>\n","N, H = h_prev.shape<br>\n","A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b<br>\n","f = A[:, :H]<br>\n","g = A[:, H:2H]<br>\n","i = A[:, 2H:3H]<br>\n","o = A[:, 3H:]<br>\n","f = sigmoid(f)<br>\n","g = np.tanh(g)<br>\n","i = sigmoid(i)<br>\n","o = sigmoid(o)<br>\n","c_next = f * c_prev + g * i<br>\n","tanh_c_next = np.tanh(c_next)<br>\n","h_next = o * tanh_c_next<br>\n","<font color=\"red\">A2 = (dh_next * o) * (1 - tanh_c_next ** 2)<br>\n","ds = dc_next + A2<br>\n","dc_prev = ds * f<br>\n","di = ds * g<br>\n","df = ds * c_prev<br>\n","do = dh_next * tanh_c_next<br>\n","dg = ds * i<br>\n","di * = i * (1 - i)<br>\n","df * = f * (1 - f)<br>\n","do * = o * (1 - o)<br>\n","dg * = (1 - g * * 2)<br>\n","dA = np.hstack((df, dg, di, do))<br>\n","dWh = np.dot(h_prev.T, dA)<br>\n","dWx = np.dot(x.T, dA)<br>\n","db = dA.sum(axis=0)<br>\n","dx = np.dot(dA, Wx.T)<br>\n","dh_prev = np.dot(dA, Wh.T)</font><br>"],"metadata":{"id":"q8Km0Jn4V1hU"}},{"cell_type":"markdown","source":["####<font color=\"silver\">GRU</font>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://d2l.ai/chapter_recurrent-modern/gru.html)</font><br>\n","<img src=\"https://d2l.ai/_images/gru-3.svg\" width=\"480\"><br><br>\n","$\\mathbf{R}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xr} + \\mathbf{H}_{t-1} \\mathbf{W}_{hr} + \\mathbf{b}_r)$<br>\n","$\\tilde{\\mathbf{H}}_t = \\tanh(\\mathbf{X}_t \\mathbf{W}_{xh} + \\left(\\mathbf{R}_t \\odot \\mathbf{H}_{t-1}\\right) \\mathbf{W}_{hh} + \\mathbf{b}_h)$<br>\n","$\\mathbf{Z}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xz} + \\mathbf{H}_{t-1} \\mathbf{W}_{hz} + \\mathbf{b}_z)$<br>\n","$\\mathbf{H}_t = \\mathbf{Z}_t \\odot \\mathbf{H}_{t-1}  + (1 - \\mathbf{Z}_t) \\odot \\tilde{\\mathbf{H}}_t$<br>"],"metadata":{"id":"I4YgbPVH-nWV"}},{"cell_type":"markdown","source":["####<font color=\"silver\">GRU</font>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/4ca1645210e5fbfafa1f)</font></font><br>\n","<img src=\"https://camo.qiitausercontent.com/f80caaadacd1c4ded8d1190d0fd964f3786f8fe5/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f34356163383866642d393137662d373661352d633562662d3733633466396164616666352e706e67\" width=\"320\"><br><br>\n","def forward(self, x, h_prev):<br>\n","$\\qquad$Wx, Wh, b = self.params<br>\n","$\\qquad$N, H = h_prev.shape<br>\n","$\\qquad$Wxz, Wxr, Wxh = Wx[:, :H], Wx[:, H:2 * H], Wx[:, 2 * H:]<br>\n","$\\qquad$Whz, Whr, Whh = Wh[:, :H], Wh[:, H:2 * H], Wh[:, 2 * H:]<br>\n","$\\qquad$bhz,   bhr,  bhh =  b[:H], b[H:2 * H], b[2 * H:]<br>\n","$\\qquad$z = sigmoid(np.dot(x, Wxz) + np.dot(h_prev, Whz) + bhz)<br>\n","$\\qquad$r = sigmoid(np.dot(x, Wxr) + np.dot(h_prev, Whr) + bhr)<br>\n","$\\qquad$h_hat = np.tanh(np.dot(x, Wxh) + np.dot(r*h_prev, Whh) + bhh)<br>\n","$\\qquad$h_next = z * h_prev + (1-z) * h_hat<br>\n","$\\qquad$self.cache = (x, h_prev, z, r, h_hat)<br>\n","$\\qquad$return h_next<br>"],"metadata":{"id":"XH194VvCWJuF"}},{"cell_type":"markdown","source":["####<font color=\"silver\">Peep Hole</font><br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/k/konchangakita/20210130/20210130013304.png\" width=\"400\"><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F22648%2Fba2dc6b7-6d94-75c6-8e94-033a56708249.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=83e7b7658918128c25c826a989f76f55\" width=\"320\"><br>"],"metadata":{"id":"nizFCwvi-fjU"}},{"cell_type":"markdown","source":["####<font color=\"silver\">torch.nn.Conv</font><br>\n","torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)<br>\n","torch.nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True, device=None, dtype=None)<br>\n","torch.nn.InstanceNorm2d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False, device=None, dtype=None)<br>\n","torch.nn.GroupNorm(num_groups, num_channels, eps=1e-05, affine=True, device=None, dtype=None)<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://cvml-expertguide.net/terms/dl/layers/convolution-layer/grouped-convolution/)</font></font><br>\n","<img src=\"https://i0.wp.com/cvml-expertguide.net/wp-content/uploads/2022/05/ac755532ff53c6fe84815b4b0bf83ac0.png?resize=1024%2C615&ssl=1\" width=\"320\"><br>\n","[<font color=\"silver\">$\\tiny{…}$</font>](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)<br>\n","torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, …)<br>\n","Shape：input$\\rm (N, C_{in}, H_{in}, W_{in})$<br>\n","Shape：output$\\rm (N, C_{out}, H_{out}, W_{out})$<br>\n","Variables：weight$\\rm (out_{channels}, in_{channels}/groups, kernel[0],kernel[1])$<br>\n","Variables：bias$\\rm (out_{channels})$<br>\n","[<font color=\"silver\">$\\tiny{…}$</font>](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)<br>\n","torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1,…)<br>\n","Shape：input$\\rm (N, C_{in}, H_{in}, W_{in})$<br>\n","Shape：output$\\rm (N, C_{out}, H_{out}, W_{out})$<br>\n","Variables：weight$\\rm (out_{channels}, in_{channels}/groups, kernel[0],kernel[1])$<br>\n","Variables：bias$\\rm (out_{channels})$<br>\n","[<font color=\"silver\">$\\tiny{…}$</font>](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d)<br>\n","torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)<br>\n","Shape：input$\\rm (N, C_{in}, H_{in}, W_{in})$<br>\n","Shape：output$\\rm (N, C_{out}, H_{out}, W_{out})$<br>\n","[<font color=\"silver\">$\\tiny{…}$</font>](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)<br>\n","torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)<br>\n","Shape：input$\\rm (N, in_{features})$<br>\n","Shape：output$\\rm (N, out_{features})$<br>\n","Variables：weight$\\rm (out_{features}, in_{features})$<br>\n","Variables：bias$\\rm (out_{features})$<br>"],"metadata":{"id":"suU7nauLYhAQ"}},{"cell_type":"markdown","source":["####<font color=\"silver\">torch.nn.RNN</font><br>\n","\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/35f6f0d26f9e58f01e4e)</font><br>\n","<img src=\"https://camo.qiitausercontent.com/32654fed87f53f2b82b773d50c36e94a97f9bec0/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f31363033633765302d653532392d346237632d653936312d3039396561653661663433632e706e67\" width=\"400\">\n","<img src=\"https://camo.qiitausercontent.com/3d6925879a4d36d8cd7562a9adf75d76f43f281e/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f65363761373333612d623134662d373232362d646262382d3061393135396663396434312e706e67\" width=\"480\"><br>\n","[<font color=\"silver\">$\\tiny{…}$</font>](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)<br>\n","torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None, device=None, dtype=None)<br>\n","Shape：input$\\rm (N,)$, (T, N) <font color=\"silver\"> # (sequence length, batch size) </font><br>\n","Shape：output$\\rm (N, embedding_{dim})$, (T, N, D) <font color=\"silver\"> # (sequence length, batch size, embedding_dim) </font><br>\n","Variables：weight$\\rm (num_{embeddings}, embedding_{dim})$, (V, D) <font color=\"silver\"> # (num_embeddings, embedding_dim) </font><br>\n","[<font color=\"silver\">$\\tiny{…}$</font>](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN)<br>\n","torch.nn.RNN(*args, **kwargs)<br>\n","input_size <font color=\"silver\">各時刻における入力ベクトルのサイズ<br></font>\n","hidden_size <font color=\"silver\">隠れ層ベクトルのサイズ<br></font>\n","num_layers = 1<br></font>\n","nonlinearity = 'tanh'<br></font>\n","bias = True<br></font>\n","batch_first = False <font color=\"silver\">テンソルのTNDをNTDで入力する<br></font>\n","dropout = 0<br></font>\n","bidirectional = False<br></font><br>\n","Shape：input$\\rm (L, N, H_{in})$ (T, N, D) <font color=\"silver\"> # (sequence length, batch size, embedding_dim) </font><br>\n","Shape：h_0$\\rm (num_{layers}, N, H_{out})$ <font color=\"silver\"> # (num_layers, batch size, hidden_size) </font><br>\n","Shape：h_n$\\rm (num_{layers}, N, H_{out})$ <font color=\"silver\"> # (num_layers, batch size, hidden_size) </font><br>\n","Shape：output$\\rm (L, N, H_{out})$ (T, N, H) <font color=\"silver\"> # (sequence length, batch size, hidden_size) </font><br>\n","<br>\n","Shape：input$\\rm (L, N, H_{in})$ (T, N, D) <font color=\"silver\"> # (sequence length, batch size, embedding_dim) </font><br>\n","Shape：h_0$\\rm (2×num_{layers}, N, H_{out})$ (Bidirectional×num_layers, N, H) <font color=\"silver\"> # (Bidirectional×num_layers, batch size, hidden_size) </font><br>\n","Shape：h_n$\\rm (2×num_{layers}, N, H_{out})$ (Bidirectional×num_layers, N, H) <font color=\"silver\"> # (Bidirectional×num_layers, batch size, hidden_size) </font><br>\n","Shape：output$\\rm (L, N, 2×H_{out})$ (T, N, Bidirectional×H) <font color=\"silver\"> # (sequence length, batch size, 2×hidden_size) </font><br><br>\n","class BiLSTM(nn.Module):<br>\n","$\\qquad$def __ init __ (self):<br>\n","$\\qquad$$\\qquad$super(BiLSTM, self). __ init __ ()<br>\n","$\\qquad$$\\qquad$self.lstm = nn.LSTM(1, 1024, 2, batch_first = True, bidirectional = True)<font color=\"silver\">  # (input, hidden, num_layers) </font><br>\n","$\\qquad$$\\qquad$self.fc = nn.Linear(2 * 1024, 1)<font color=\"silver\">  # (Bidirectional×hidden, output)</font><br>\n","$\\qquad$def forward(self, x):<br>\n","$\\qquad$$\\qquad$num_layers = 2 <font color=\"silver\">  # (Bidirectional×num_layers)</font><br>\n","$\\qquad$$\\qquad$h0 = torch.zeros(2 * num_layers, x.size(0), 1024).requires_grad_().to(device)<font color=\"silver\">  # hの初期化</font><br>\n","$\\qquad$$\\qquad$c0 = torch.zeros(2 * num_layers, x.size(0), 1024).requires_grad_().to(device)<font color=\"silver\">  # cの初期化</font><br>\n","$\\qquad$$\\qquad$output, (h_n, c_n) = self.lstm(x, (h0.detach(), c0.detach())<font color=\"silver\">  # .detach()は勾配を計算しない</font><br>\n","$\\qquad$$\\qquad$output = self.fc(output[ :, -1, : ]) <font color=\"silver\">  # 最後のLSTMの値</font><br>\n","$\\qquad$$\\qquad$return output<br>"],"metadata":{"id":"twHriv7UxZjL"}},{"cell_type":"markdown","source":["####<font color=\"silver\"> CNNの定義</font><br>\n","<font color=\"silver\"># TwoLayerNet</font><br>\n","self.params = {}<br>\n","self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)<br>\n","self.params['b1'] = np.zeros(hidden_size)<br>\n","self.params['W2'] = weight_init_std * np.random.randn(hidden_size,output_size) <br>\n","self.params['b2'] = np.zeros(output_size)<br>\n","self.layers = OrderedDict() <br>\n","self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])<br>\n","self.layers['Relu1'] =  ReLU()<br>\n","self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])<br>\n","self.lastLayer = MeanSquaredLoss()<br>\n","<font color=\"silver\"># SimpleConv</font><br>\n","self.params = {}<br>\n","std = weight_init_std<br>\n","self.params['W1'] = std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)<br>\n","self.params['b1'] = np.zeros(filter_num)<br>\n","self.params['W2'] = std *  np.random.randn(pool_output_pixel, hidden_size)<br>\n","self.params['b2'] = np.zeros(hidden_size)<br>\n","self.params['W3'] = std *  np.random.randn(hidden_size, output_size)<br>\n","self.layers = OrderedDict()<br>\n","self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], conv_param['stride'], conv_param['pad'])<br>\n","self.layers['ReLU1'] = ReLU()<br>\n","self.layers['Pool1'] = MaxPooling(pool_h=pool_size, pool_w=pool_size, stride=pool_stride, pad=pool_pad)<br>\n","self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])<br>\n","self.layers['ReLU2'] = ReLU()<br>\n","self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])<br>\n","self.last_layer = SoftmaxWithLoss()<br>\n"],"metadata":{"id":"zYGMaBMUQYGn"}},{"cell_type":"markdown","source":["####<font color=\"silver\"> SimpleRNNLMの定義</font><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://d2l.ai/chapter_recurrent-modern/deep-rnn.html)</font><br>\n","<img src=\"https://d2l.ai/_images/deep-rnn.svg\" width=\"240\"><br><br>\n","$\\mathbf{H}_t^{(l)} = \\phi_l(\\mathbf{H}_t^{(l-1)} \\mathbf{W}_{xh}^{(l)} + \\mathbf{H}_{t-1}^{(l)} \\mathbf{W}_{hh}^{(l)}  + \\mathbf{b}_h^{(l)})$<br><br>\n","$\\mathbf{O}_t = \\mathbf{H}_t^{(L)} \\mathbf{W}_{hq} + \\mathbf{b}_q$<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/35f6f0d26f9e58f01e4e)</font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F19b35dea-8cc6-35f1-da5f-9849395053c3.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=3de1c04d0aca2bc134b64bff46c63d40\" width=\"480\"><br><br>\n","rn = np.random.randn<br>\n","embed_W = rn(V, D) / 100 <font color=\"silver\"> # 小さな値で初期化するため100で割る<br></font>\n","rnn_Wx = rn(D, H) * np.sqrt(2/(D+H)) <font color=\"silver\"> # Xavierの初期値<br></font>\n","nn_Wh = rn(H, H) * np.sqrt(2/(H+H)) <font color=\"silver\"> # Xavierの初期値<br></font>\n","rnn_b = np.zeros(H)<br>\n"," affine_W = rn(H, V) * np.sqrt(2/(H+V)) <font color=\"silver\"> # Xavierの初期値<br></font>\n","affine_b = np.zeros(V)<br>\n","self.layers = [<br>\n","TimeEmbedding(embed_W),<br>\n","TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True), <font color=\"silver\"> # 中間層を引き継ぐためTrueにする<br></font>\n","TimeAffine(affine_W, affine_b) <br>\n","]<br>\n","self.loss_layer = TimeSoftmaxWithLoss()</font><br>"],"metadata":{"id":"cS6MCWX6Wt_j"}},{"cell_type":"markdown","source":["####<font color=\"silver\"> BetterRNNLMの定義</font><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/35f6f0d26f9e58f01e4e)</font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F254c6004-ae48-ff43-a409-edd820d86dd0.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=7ef03e00836c07be1d96d46948cf20cf\" width=\"640\"><br><br>\n","rn = np.random.randn <font color=\"silver\"> # 正規分布からランダム</font><br>\n","embed_W = (rn(V, D) / 100).astype('f') <font color=\"silver\"> # 小さな値で初期化するため100で割る</font><br>\n","lstm_Wx1 = (rn(D, 4 * H) / np.sqrt(D)).astype('f') <font color=\"silver\"> # Xavier, 4×H, biasはゼロで初期化</font><br>\n","lstm_Wh1 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')<br>\n","lstm_b1 = np.zeros(4 * H).astype('f')<br>\n","lstm_Wx2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')<br>\n","lstm_Wh2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')<br>\n","lstm_b2 = np.zeros(4 * H).astype('f')<br>\n","affine_b = np.zeros(V).astype('f')<br>\n","self.layers = [<br>\n","TimeEmbedding(embed_W),<br>\n","TimeDropout(dropout_ratio),<br>\n","TimeLSTM(lstm_Wx1, lstm_Wh1, lstm_b1, stateful=True),<br>\n","TimeDropout(dropout_ratio),<br>\n","TimeLSTM(lstm_Wx2, lstm_Wh2, lstm_b2, stateful=True),<br>\n","TimeDropout(dropout_ratio),<br>\n","TimeAffine(embed_W.T, affine_b) <font color=\"silver\"> # weight tying!!</font><font color=\"blue\">$~\\leftarrow\\ast$</font><br>\n","]<br>\n","self.loss_layer = TimeSoftmaxWithLoss()<br>"],"metadata":{"id":"oKXz17B9Xztx"}},{"cell_type":"markdown","source":["####<font color=\"silver\"> Seq2Seqの定義</font><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://d2l.ai/chapter_recurrent-modern/seq2seq.html)</font><br>\n","<img src=\"https://d2l.ai/_images/seq2seq.svg\" width=\"400\"><br>\n","<img src=\"https://d2l.ai/_images/seq2seq-predict.svg\" width=\"400\"><br>\n","<font color=\"silver\">Encoder</font><br>\n","rn = np.random.randn<font color=\"silver\"> # 正規分布からランダム</font><br>\n","embed_W = rn(V, D) / 100 <font color=\"silver\"> # 小さな値で初期化<br></font>\n","lstm_Wx = rn(D, 4 * H) * np.sqrt(2/(D+H)) <font color=\"silver\"> # Xavier, 4×H, biasはゼロで初期化<font color=\"blue\">$~\\leftarrow\\ast$</font></font><br>\n","lstm_Wh = rn(H, 4 * H) * np.sqrt(2/(H+H))<br>\n","lstm_b = np.zeros(4 * H)<br>\n","self.embed = TimeEmbedding(embed_W)<br>\n","self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)<br><br>\n","<font color=\"silver\">Decoder</font><br>\n","rn = np.random.randn<font color=\"silver\"> # 正規分布からランダム</font><br>\n","embed_W = rn(V, D) / 100 <font color=\"silver\"> # 小さな値で初期化<br></font>\n","lstm_Wx = rn(D, 4 * H) * np.sqrt(2/(D+H)) <font color=\"silver\"> # Xavier, 4×H, biasはゼロで初期化<font color=\"blue\">$~\\leftarrow\\ast$</font></font><br>\n","lstm_Wh = rn(H, 4 * H) * np.sqrt(2/(H+H))<br>\n","lstm_b = np.zeros(4 * H)<br>\n","affine_W = rn(H, V) * np.sqrt(2/(H+V))<br>\n","affine_b = np.zeros(V)<br>\n","self.embed = TimeEmbedding(embed_W)<br>\n","self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)<br>\n","self.affine = TimeAffine(affine_W, affine_b)<br><br>\n","<font color=\"silver\">Seq2seq</font><br>\n","self.encoder = Encoder(V, D, H) <br>\n","self.decoder = Decoder(V, D, H)<br>\n","self.softmax = TimeSoftmaxWithLoss()<br>"],"metadata":{"id":"fUOPSIk5aLgZ"}},{"cell_type":"markdown","source":["####<font color=\"silver\">AttentionBiSeq2seqの定義</font><br>\n","\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://d2l.ai/chapter_recurrent-modern/bi-rnn.html)</font><br>\n","<img src=\"https://d2l.ai/_images/birnn.svg\" width=\"320\"><br><br>\n","$\\overrightarrow{\\mathbf{H}}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh}^{(f)} + \\overrightarrow{\\mathbf{H}}_{t-1} \\mathbf{W}_{hh}^{(f)}  + \\mathbf{b}_h^{(f)})$<br><br>\n","$\\overleftarrow{\\mathbf{H}}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh}^{(b)} + \\overleftarrow{\\mathbf{H}}_{t+1} \\mathbf{W}_{hh}^{(b)}  + \\mathbf{b}_h^{(b)})$<br><br>\n","$\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q$<br><br>\n","<font color=\"silver\">AttentionBiEncoder</font><br>\n","rn = np.random.randn<font color=\"silver\"> # 正規分布からランダム</font><br>\n","embed_W = rn(V, D) / 100 <font color=\"silver\"> # 小さな値で初期化<br></font>\n","lstm_Wx_f = rn(D, 4 * H) * np.sqrt(2/(D+H))<font color=\"silver\"> # Xavier, 4×H, biasはゼロで初期化<font color=\"blue\">$~\\leftarrow\\ast$</font></font><br>\n","lstm_Wh_f = rn(H, 4 * H) * np.sqrt(2/(H+H))<br>\n","lstm_b_f = np.zeros(4 * H)<br>\n","lstm_Wx_b = rn(D, 4 * H) * np.sqrt(2/(D+H))<br>\n","lstm_Wh_b = rn(H, 4 * H) * np.sqrt(2/(H+H))<br>\n","lstm_b_b = np.zeros(4 * H)  <br>\n","self.embed = TimeEmbedding(embed_W)<br>\n","self.lstm = TimeBiLSTM(lstm_Wx_f, lstm_Wh_f, lstm_b_f, lstm_Wx_b, lstm_Wh_b, lstm_b_b, stateful=False)<br><br>\n","<font color=\"silver\">AttentionDecoder</font><br>\n","rn = np.random.randn<font color=\"silver\"> # 正規分布からランダム</font><br>\n","embed_W = rn(V, D) / 100 <font color=\"silver\"> # 小さな値で初期化<br></font>\n","lstm_Wx = rn(D, 4 * H) * np.sqrt(2/(D+H)) <font color=\"silver\"> # Xavier, 4×H, biasはゼロで初期化<font color=\"blue\">$~\\leftarrow\\ast$</font></font><br>\n","lstm_Wh = rn(H, 4 * H) * np.sqrt(2/(H+H))<br>\n","lstm_b = np.zeros(4 * H)<br>\n","affine_W_c = rn(2* H, V) * np.sqrt(2/(2*H+V))<font color=\"silver\"> # $c^t+h^t$を入力するため2×H<font color=\"blue\">$~\\leftarrow\\ast$</font></font><br>\n","affine_b_c = np.zeros(V)<br>\n","affine_W_s = rn(V, V) * np.sqrt(2/(V+V))<br>\n","affine_b_s = np.zeros(V)<br>\n","self.embed = TimeEmbedding(embed_W)<br>\n","self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)<br>\n","self.attention = TimeAttention()<br>\n","self.affine_c = TimeAffine(affine_W_c, affine_b_c)<br>\n","self.tanh = Tanh()<br>\n","self.affine_s = TimeAffine(affine_W_s, affine_b_s)<br>\n","layers = [self.embed, self.lstm, self.attention, self.affine_c, self.tanh, self.affine_s]<br><br>\n","<font color=\"silver\">AttentionBiSeq2seq</font><br>\n","self.encoder = AttentionBiEncoder(V, D, H)<br>\n","self.decoder = AttentionDecoder(V, D, H*2) <font color=\"silver\"> # 双方向の中間層を引数に取るためH×2</font><font color=\"blue\">$~\\leftarrow\\ast$</font><br>\n","self.softmax = TimeSoftmaxWithLoss()<br>"],"metadata":{"id":"b4SQbcsicCO1"}},{"cell_type":"markdown","source":["####<font color=\"silver\">ResNet</font>\n","$H(x)=F(x)+x$<br><br>\n","CNN：${x}^{(l)} = H^{(l)}({x}^{l-1})$<br><br>\n","  ResNet：${x}^{(l)} = H^{(l)}({x}^{l-1},{x}) + {x}^{l-1}$<br><br>\n","  DenseNet：${x}^{(l+1)} = H^{(l)}([{x}^{l-1},{x}^{l-2},…,{x}^{0}]) + {x}^{l}$<br><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F38555%2F1557d5bd-50bc-a65e-cf9a-dad10649a6c0.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=21774d16a1f02b2759fa9fb941212d13\" width=\"800\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/be48afbdd9da19f1e43e)<br></font>\n","<img src=\"https://camo.qiitausercontent.com/310591eea55adba318520a682e19baac8ab64d19/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3331353036332f36303336393736332d643734382d386334382d396262322d3734616564623262663263392e706e67\" width=\"480\">"],"metadata":{"id":"I854aonqccha"}},{"cell_type":"markdown","source":["####<font color=\"silver\">MobileNet</font>\n","${\\rm Depthwise}+{\\rm Pointwise} = c_{in}･k^2 + c_{in}･c_{out} = c_{in}(c_{out}+k^2)$<br><br>\n","$\\cfrac{c_{in}(c_{out}+k^2)}{c_{in}･c_{out}･k^2} ＝\\cfrac{c_{out}＋k^2}{c_{out}･k^2} ＝\\cfrac{1}{k^2} ＋\\cfrac{1}{c_{out}}$<br><br>\n","${\\cfrac{D_{K} · D_{K} · M · D_{F} · D_{F} ＋M · N · D_{F} · D_{F} }{D_{K} · D_{K} · M · N · D_{F} · D_{F} } = \\cfrac{1}{N} + \\cfrac{1}{D_{K}^2}}$<br><br>\n","$D_{K} · D_{K} · αM · \\rho D_{F} · \\rho D_{F} ＋αM · αN · \\rho D_{F} · \\rho D_{F} $<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://muscle-programmer.hatenablog.com/entry/2018/06/07/190221)</font></font><br>\n","<img src=\"https://qiita-image-store.s3.amazonaws.com/0/108729/5e4bb20f-127e-4d9e-10fb-110ba4694360.png\" width=\"640\"><br>\n","<img src=\"https://qiita-image-store.s3.amazonaws.com/0/108729/72ca6fe6-f6a0-7dd3-3b24-7aa3aa185ab6.png\" width=\"640\"><br>"],"metadata":{"id":"SKUfu8jRdld3"}},{"cell_type":"markdown","source":["####<font color=\"silver\">GNMT</font>\n","<img src=\"https://norman3.github.io/papers/images/gnmt/f01.png\" width=\"640\"><br><br>\n","双方向LSTM<br></font>\n","エンコーダの1層目（1段目と2段目を合わせた部分）が 双方向LSTM<br>\n","もし、全層を双方向LSTMにした場合、それぞれの層は前の層の順方向と逆方向 の計算が終わるのを待たなければならず、 2GPU (順方向に1、 逆方向に1) し か並列に動かすことができない。<br><br>\n","Attention<br></font>\n","エンコーダ第8層の出力とデコーダ第1層の出力でAttentionを計算し、デコーダの全層に送る。<br>\n","エンコーダの第8層とデコーダの第1層で計算するのは、並列化の効率を高めるため。 エンコーダの第8層とデコーダの第8層とでアテンションを計算すると待ち時間が発生する。<br>"],"metadata":{"id":"biE28s51gfme"}},{"cell_type":"markdown","source":["####<font color=\"silver\">Transformer Embedding</font>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/35f6f0d26f9e58f01e4e)</font><br>\n","<img src=\"https://camo.qiitausercontent.com/32654fed87f53f2b82b773d50c36e94a97f9bec0/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f31363033633765302d653532392d346237632d653936312d3039396561653661663433632e706e67\" width=\"400\"><br><br>\n","class Embedding:<br>\n","$\\qquad$def __ init __ (self, W): <font color=\"silver\"> # W（語彙数、埋め込みベクトルの要素数）</font><br>\n","$\\qquad$$\\qquad$self.params = [W] <font color=\"silver\"> # 要素は1つだけであるが、他のレイヤと仕様を揃えるため、リストで定義</font><br>\n","$\\qquad$$\\qquad$self.grads = [np.zeros_like(W)] <font color=\"silver\"> # 要素は1つだけであるが、他のレイヤと仕様を揃えるため、リストで定義</font><br>\n","$\\qquad$$\\qquad$self.idx = None<br>\n","$\\qquad$def forward(self, idx):<br>\n","$\\qquad$$\\qquad$W = self.params[0]<br>\n","$\\qquad$$\\qquad$self.idx = idx<br>\n","$\\qquad$$\\qquad$out = W[idx] <font color=\"silver\"> # 埋め込み行列から埋め込みベクトルを取り出す</font><br>\n","$\\qquad$$\\qquad$return out<br>\n","$\\qquad$def backward(self, dout):<br>\n","$\\qquad$$\\qquad$dW = self.grads[0]  <font color=\"silver\"> # gradsというリストの1要素目を参照する </font><br>\n","$\\qquad$$\\qquad$dW.fill(0)  <font color=\"silver\"># 配列の全ての要素に0を代入する</font><br>\n","$\\qquad$$\\qquad$np.add.at(dW, self.idx, dout) <font color=\"silver\"> # dWのidxの場所にdoutを加える</font><br>\n","$\\qquad$$\\qquad$return None<br><br>\n","class Embedder(nn.Module):<br>\n","$\\qquad$def  __ init __ (self, text_embedding_vectors):<br>\n","$\\qquad$$\\qquad$super(Embedder, self). __ init __ ()<br>\n","$\\qquad$$\\qquad$self.embeddings = nn.Embedding.from_pretrained(text_embedding_vectors, freeze=True) <font color=\"silver\"># freeze=Trueで更新しない</font><br>\n","$\\qquad$def forward(self, x): <font color=\"silver\">(T, ) = (256, )</font><br>\n","$\\qquad$$\\qquad$x_vec = self.embeddings(x)<br>\n","$\\qquad$$\\qquad$return x_vec <font color=\"silver\">(T, D) = (256, 300)</font><br>"],"metadata":{"id":"JzPh481qIl49"}},{"cell_type":"markdown","source":["####<font color=\"silver\">Transformer PositionalEncoding</font>\n","$\\rm \\displaystyle PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)$<br><br>\n","$\\rm \\displaystyle PE_{(pos, 2i + 1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)$<br><br>\n","$pos$：<font color=\"silver\">位置</font><br>\n","$i$：<font color=\"silver\">次元</font><br><br>\n","class PositionalEncoder(nn.Module):<br>\n","$\\qquad$def  __ init __ (self, d_model=300, max_seq_len=256): <font color=\"silver\"># 埋め込みベクトルの次元と入力長</font><br>\n","$\\qquad$$\\qquad$super(). __ init __ ()<br>\n","$\\qquad$$\\qquad$self.d_model = d_model<br>\n","$\\qquad$$\\qquad$pe = torch.zeros(max_seq_len, d_model) <font color=\"silver\"># PositionalEncodeを格納する受け皿, 零テンソル</font><br>\n","$\\qquad$$\\qquad$for pos in range(max_seq_len):<br>\n","$\\qquad$$\\qquad$$\\qquad$for i in range(0, d_model, 2):<br>\n","$\\qquad$$\\qquad$$\\qquad$$\\qquad$pe[pos, i] = math.sin(pos / (10000 ** ((2*i)/d_model)))<br>\n","$\\qquad$$\\qquad$$\\qquad$$\\qquad$pe[pos, i+1] = math.cos(pos / (10000 ** ((2*i)/d_model)))<br>\n","$\\qquad$$\\qquad$$\\qquad$self.pe = pe.unsqueeze(0)  <font color=\"silver\"># peの先頭に、ミニバッチを表す次元を追加</font><br>\n","$\\qquad$$\\qquad$$\\qquad$self.pe.requires_grad = False  <font color=\"silver\"># 勾配を計算しないようにする</font><br>\n","$\\qquad$def forward(self, x): <font color=\"silver\"># 入力xとpeを足し合わせる, xがpeよりも小さいため次元数の平方根を掛けて拡大</font><br>\n","$\\qquad$$\\qquad$$\\qquad$ret = math.sqrt(self.d_model)*x + self.pe<br>\n","$\\qquad$$\\qquad$$\\qquad$return ret  <font color=\"silver\">(T, D) = (256, 300)</font><br>"],"metadata":{"id":"73LSCSkNRf1X"}},{"cell_type":"markdown","source":["####<font color=\"silver\">Transformer Attention</font>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://nlpillustration.tech/?p=2171)</font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F68cb794b-92b2-9c4a-55e6-b4696545efd7.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=8042404aad0e06e993a9568404500a92\" width=\"480\"><br>\n","$\\rm{ScaledDotProductAttention}(\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V}) = \\rm{softmax}\\left(\\cfrac{\\boldsymbol{QK}^T}{\\sqrt{d_k}}\\right)\\boldsymbol{V}$<br><br>\n","$\\boldsymbol{Q}$：デコーダー, 検索文, 計算するターゲット<br>\n","$\\boldsymbol{K}$：エンコーダー, 辞書のインデックス, 類似度の計算に使う単語ベクトルの集まり<br>\n","$\\boldsymbol{V}$：エンコーダー, 辞書の本文, 重み付け和計算に使うベクトルの集まり<br>\n","$\\sqrt{d_k}$：query, keyの単語分散表現の次元数で論文では512<br><br>\n","$\\sqrt{d_k}$で割る理由は、確率が低い部分の勾配情報を保持するためで、$\\sqrt{d_k}$ が大きくなると逆伝播時のソフトマックス関数の勾配が小さくなるため、 学習が円滑に進まなくなる。</font><br><br>\n","class Attention(nn.Module):<br>\n","$\\qquad$def  __ init __ (self, d_model=300):<br>\n","$\\qquad$$\\qquad$super(). __ init __ ()<br>\n","$\\qquad$$\\qquad$self.q_linear = nn.Linear(d_model, d_model)<br>\n","$\\qquad$$\\qquad$self.k_linear = nn.Linear(d_model, d_model)<br>\n","$\\qquad$$\\qquad$self.v_linear = nn.Linear(d_model, d_model)<br>\n","$\\qquad$$\\qquad$self.out = nn.Linear(d_model, d_model)<br>\n","$\\qquad$$\\qquad$self.d_k = d_model<br>\n","$\\qquad$def forward(self, q, k, v, mask):<br>\n","$\\qquad$$\\qquad$q = self.q_linear(q)<br>\n","$\\qquad$$\\qquad$k = self.k_linear(k)<br>\n","$\\qquad$$\\qquad$v = self.v_linear(v)<br>\n","$\\qquad$$\\qquad$weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.d_k)<br>\n","$\\qquad$$\\qquad$mask = mask.unsqueeze(1)<br>\n","$\\qquad$$\\qquad$weights = weights.masked_fill(mask == 0, -1e9)<br>\n","$\\qquad$$\\qquad$normlized_weights = F.softmax(weights, dim=-1)<br>\n","$\\qquad$$\\qquad$output = torch.matmul(normlized_weights, v)<br>\n","$\\qquad$$\\qquad$output = self.out(output)<br>\n","$\\qquad$$\\qquad$return output, normlized_weights<br>"],"metadata":{"id":"PkHBBWoc4u-V"}},{"cell_type":"markdown","source":["####<font color=\"silver\">Transformer Why Self-Attention</font>\n","- Why Self-Attention<br></font>\n","計算量が小さい<br>\n","並列計算可能<br>\n","広範囲の依存関係を学習可能<br>\n","高い解釈可能性<br><br>\n","- 計算コスト比較<br></font>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://zenn.dev/attentionplease/articles/4e09c41d7a85db)</font><br>\n","Complexity per Layer：1層あたりの計算複雑性<br>\n","Sequential Operations：系列を処理する最小の操作回数<br>\n","Maximum Path Length：入力から出力の経路の長さ、長期依存性の学習しやすさ<br>\n","$n$：シーケンスの長さ<br>\n","$d$：モデルの次元<br>\n","<img src=\"https://camo.qiitausercontent.com/493038d5bbfdd7e5a27858e84f288bf99c398d85/68747470733a2f2f696d6775722e636f6d2f667478666255682e706e67\" width=\"640\"><br>"],"metadata":{"id":"Wx51F2AWR4aH"}},{"cell_type":"markdown","source":["####<font color=\"silver\">WaveNet</font>\n","$f(x_{t}) = \\rm{sign}(x_{t}) \\displaystyle \\frac{\\log \\{1 + \\mu |x_{t}|\\}}{\\log \\{1 + \\mu \\}}$<br><br>\n","$-1 < x_{t} < 1, \\mu = 255$<br><br>\n","$\\mathbf{z} = \\tanh (W_{f, k} * \\mathbf{x} + V_{f, k}^{T}\\mathbf{h}) \\odot \\sigma (W_{g, k} * \\mathbf{x} + V_{g, k}^{T}\\mathbf{h})$<br><br>\n","$\\mathbf{z} = \\tanh (W_{f, k} * \\mathbf{x} + V_{f, k}^{T} * f(\\mathbf{h})) \\odot \\sigma (W_{g, k} * \\mathbf{x} + V_{g, k}^{T} * f(\\mathbf{h}))$"],"metadata":{"id":"rk_JR_1e2uhz"}},{"cell_type":"markdown","source":["####<font color=\"silver\">グラフ</font>\n","<font color=\"silver\">ラプラシアン行列</font><br><br>\n","$\\boldsymbol{L}=\\boldsymbol{D}-\\boldsymbol{A}$<br><br>\n","<font color=\"silver\">正規化ラプラシアン行列</font><br><br>\n","$\\boldsymbol{\\mathcal{L}} = \\boldsymbol{D}^{- \\frac{1}{2}}\\boldsymbol{L}\\boldsymbol{D}^{- \\frac{1}{2}} = \\boldsymbol{I}-\\boldsymbol{D}^{- \\frac{1}{2}}\\boldsymbol{A}\\boldsymbol{D}^{- \\frac{1}{2}}$<br><br>\n","<font color=\"silver\">グラフフーリエ変換</font><br>\n","<font color=\"silver\">正規化グラフラプラシアン行列の固有ベクトルを並べた行列の転置とデータの 積をとる</font><br><br>\n","$\\mathcal{F}_G[\\boldsymbol{x}] = \\boldsymbol{Q}^{\\top}\\boldsymbol{x}$<br><br>\n","<font color=\"silver\">逆グラフフーリエ変換</font><br>\n","<font color=\"silver\">正規化グラフラプラシアン行列の固有ベクトルを並べた行列とデータの積をとる\n","</font><br><br>\n","$\\mathcal{F}_G^{-1}[\\boldsymbol{x}] = \\boldsymbol{Q}\\boldsymbol{x}$<br><br>\n","<font color=\"silver\">グラフフーリエ変換の逆グラフフーリエ変換</font><br><br>\n","$\\mathcal{F}_G^{-1}[\\mathcal{F}_G[\\boldsymbol{x}]] = \\boldsymbol{Q}\\boldsymbol{Q}^{\\rm{T}}\\boldsymbol{x} = \\boldsymbol{x}$<br><br>\n","<font color=\"silver\">畳み込み定理<br>\n","2つの関数の畳み込みのフーリエ変換は、それらのフーリエ変換の各要素ごとの積である<br><br></font>\n","$\\mathcal{F}_G[\\boldsymbol{x}_1*\\boldsymbol{x}_2] =\\mathcal{F}_G[\\boldsymbol{x}_1]\\odot \\mathcal{F}_G[\\boldsymbol{x}_2]$<br>\n","$\\boldsymbol{x}_1*\\boldsymbol{x}_2=\\mathcal{F}_G^{-1}[\\mathcal{F}_G[\\boldsymbol{x}_1] \\odot \\mathcal{F}_G[\\boldsymbol{x}_2]]$<br><br>\n","<font color=\"silver\">導出</font><br><br>\n","$\\begin{align}\n","\\boldsymbol{x}*_G\\boldsymbol{g}_\\boldsymbol{θ}&=\\mathcal{F}_G^{-1}[\\mathcal{F}_G[\\boldsymbol{x}] \\odot \\mathcal{F}_G[\\boldsymbol{g}_\\boldsymbol{θ}]]\\\\\n","&=\\boldsymbol{Q}(\\boldsymbol{Q}^\\top\\boldsymbol{x}\\odot\\boldsymbol{Q}^\\top\\boldsymbol{g}_\\boldsymbol{θ})\\\\\n","&=\\boldsymbol{Q}(\\boldsymbol{Q}^\\top\\boldsymbol{g}_\\boldsymbol{θ}\\odot\\boldsymbol{Q}^\\top\\boldsymbol{x})\\\\\n","&=\\boldsymbol{Q}\\boldsymbol{Q}^\\top\\boldsymbol{g}_\\boldsymbol{θ}\\odot\\boldsymbol{Q}^\\top\\boldsymbol{x}\n","\\end{align}$<br><br>\n","$\\theta=\\boldsymbol{Q}^\\top\\boldsymbol{g}_\\boldsymbol{θ}$<font color=\"silver\">をパラメータと考えて</font><br><br>\n","$\\boldsymbol{\\Theta}=\\mathrm{diag}(\\boldsymbol{Q}^\\top\\boldsymbol{g}_\\boldsymbol{θ})$<font color=\"silver\">をパラメータを対角に並べた行列とおくと、<br></font><br>\n","$\\begin{align}\n","\\boldsymbol{x}*_G\\boldsymbol{g}_\\boldsymbol{\\Theta}&=\\boldsymbol{Q}\\boldsymbol{Θ}\\boldsymbol{Q}^\\top\\boldsymbol{x}\\\\\\\\\n","&=(逆グラフフーリエ変換)(パラメータ)(グラフフーリエ変換)\n","\\end{align}$<br><br>\n","$\\boldsymbol{x}$ ：<font color=\"silver\">全ノードの特徴(各ノードの特徴が1要素)を並べた特徴ベクトル</font><br>\n","$\\boldsymbol{g}_\\boldsymbol{θ}$ ：<font color=\"silver\">フィルタ</font><br>\n","$*_G$ ：<font color=\"silver\">グラフ畳み込み</font><br>\n","$\\boldsymbol{Q}$ ：<font color=\"silver\">正規化グラフラプラシアンの固有ベクトル行列</font><br><br>\n","<font color=\"silver\">グラフにおける畳み込みを多層かつマルチチャンネルに拡張</font><br><br>\n","$\\displaystyle\\boldsymbol{x}_j^{l+1} = \\rho \\Biggl( \\boldsymbol{Q} \\sum_{i=1}^{f_l}\\boldsymbol{\\Theta}_{i,j}^l\\boldsymbol{Q}^{\\rm{T}}\\boldsymbol{x}_i^l \\Biggr)= \\rho \\Biggl( \\sum_{i=1}^{f_l}\\boldsymbol{Q}\\boldsymbol{\\Theta}_{i,j}^l\\boldsymbol{Q}^{\\rm{T}}\\boldsymbol{x}_i^l \\Biggr),\n","~~~~~j=1,...,f_{l+1}$<br><br>\n","<font color=\"silver\">Spatial Conv</font><br><br>\n","$\\displaystyle\\boldsymbol{h}_i^{l+1} = \\sigma \\Biggl( \\sum_{j \\in \\mathcal{N_i}} \\frac{1}{c_{ij}}\\boldsymbol{h}_j^l \\boldsymbol{W}^l \\Biggr)$<br>"],"metadata":{"id":"QBFuX6Ng_00J"}},{"cell_type":"markdown","source":["####<font color=\"silver\">SVM</font><br>\n","- <font color=\"silver\">推論</font><br><br>\n","$ \\hat{y} = \\text{sgn}(\\pmb{w}^\\top\\pmb{x} + b)\n","= \n","\\begin{cases} \n"," 1,  & \\pmb{w}^\\top\\pmb{x} + b \\geqq 0 \\\\ \n"," -1, & \\pmb{w}^\\top\\pmb{x} + b< 0 \n","\\end{cases}$\n","<br><br>\n","$ \\mathcal{D} = \\left\\{({\\pmb x_i}, y_i , y_i \\in \\{ -1, 1 \\}\\right\\}_{i=1}^{N}$<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://watlab-blog.com/2019/12/22/svm/)</font><br></font>\n","<img src=\"https://watlab-blog.com/wp-content/uploads/2019/12/Regularization-parameter.png\" width=\"480\"><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://www.xiaowenying.com/machine-learning/2019/11/18/svm.html)</font><br>\n","<img src=\"https://www.xiaowenying.com/assets/post_img/svm/7824BB24CA8355AFBD5B9BF42521096D.jpg\" width=\"320\"><br><br></font>\n","- <font color=\"silver\">目的関数</font><br><br>\n","$ \\begin{align} \n","\\pmb{w}^*, b^* &=\\mathop{\\rm argmax}\\limits_{\\pmb{w},b}d+C\\displaystyle\\sum_{i=1}^{n}ξ_i =\\mathop{\\rm argmax}\\limits_{\\pmb{w},b}\\cfrac{|\\pmb{w}^\\top\\pmb{x}^*+b|}{\\|\\pmb{w}\\|} +C\\displaystyle\\sum_{i=1}^{n}ξ_i\n","\\end{align}$\n","<br><br>\n","$\\begin{cases}\n","\\pmb{w}^\\top\\pmb{x}^*+b \\ge +1, \\quad y_i = +1の場合\\\\\n","\\pmb{w}^\\top\\pmb{x}^*+b \\le -1, \\quad y_i = -1の場合\\\\\n","\\end{cases}$\n","<br><br>\n","$y_i(\\pmb{w}^\\top\\pmb{x}^*+b)-1 \\ge 0, \\quad i \\in \\{1...n\\}$\n","<br><br>\n","$y_i(\\pmb{w}^\\top\\pmb{x}^*+b) - 1 + \\xi_i \\ge 0 $<br><br>\n","- <font color=\"silver\">主問題</font><br><br>\n","$\\pmb{w}^*, b^*, \\pmb{\\xi} ^* = \\displaystyle\\mathop{\\rm argmin} _{\\pmb{w},b,\\pmb{\\xi} }\\frac{1}{2}{{\\|\\pmb{w}\\|}^2+C\\displaystyle\\sum_{i=1}^{n}ξ_i  }$\n","<br><br>\n","$\\mbox{ s.t. } y_i(\\pmb{w^{\\top}x_i}+b) \\geq 1-ξ_i, \\quad ξ_i \\geq 0, \\quad i \\in \\{1...n\\}$<br><br>\n","- <font color=\"silver\">双対問題（ラグランジュ関数＋KKT条件）</font><br><br>\n","$\\pmb{\\alpha}^* =\\mathop{\\rm argmin}\\limits_{\\pmb{\\alpha}} \\displaystyle\\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j \\pmb X_i^\\top \\pmb X_j $<br><br>\n","$\\mbox{ s.t. } \\displaystyle\\sum_{i=1}^{n} \\alpha_i y_i = 0 , \\quad C \\geq \\alpha_i \\geq 0 , \\quad i \\in \\{1...n\\}$<br><br>\n","- <font color=\"silver\">カーネルトリック</font><br><br>\n","$\\pmb{\\alpha}^* =\\mathop{\\rm argmin}\\limits_{\\pmb{\\alpha}} \\displaystyle\\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\displaystyle\\sum_{i=1}^{n}\\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j K(\\pmb X_i,\\pmb X_j)$<br><br>\n","$\\mbox{ s.t. } \\displaystyle\\sum_{i=1}^{n} \\alpha_i y_i = 0 ,\\quad C \\geq \\alpha_i \\geq 0 \\quad i \\in \\{1...n\\}$<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://cochineal19.hatenablog.com/entry/2021/05/20/193516)</font><br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/c/cochineal19/20210520/20210520021551.png\" width=\"800\"></font><br><br>"],"metadata":{"id":"EZcpeuVLoMcH"}},{"cell_type":"markdown","source":["####<font color=\"silver\">k-ｍeans</font><br>\n","- <font color=\"silver\">目的関数</font><br><br>\n","クラスタ内誤差平方和を最小化する$\\pmb{\\mu_k}^*$を求める<br><br>\n","$\\begin{align}\\pmb{\\mu_k}^*&=\\mathop{\\rm argmin}\\limits_{\\pmb{u}}J(q_{ik}, \\mu_k)\\\\&= \\mathop{\\rm argmin}\\limits_{\\pmb{u}}\\displaystyle \\sum_{i=1}^{N}\\sum_{k=1}^{K}q_{ik}||x_i-\\mu_k||^2 \n","\\end{align}$><br><br>\n","- <font color=\"silver\">k-ｍeansアルゴリズム</font><br><br>\n","クラスタ数$\\,k\\,$と各クラスタに対応するセントロイド$\\,\\mu_k\\,$の初期値を設定する<br>\n","  while：do<br>\n","  $\\qquad$各入力データ$\\,x_i\\,$とセントロイド$\\,\\mu_k\\,$との距離を求める<br>\n","  $\\qquad$距離が最も小さいクラスタを求めて、データ$\\,x_i\\,$の新しいクラスタを割り当てる<br>\n","  $\\qquad$クラスタ内のデータ$\\,x_i\\,$の平均ベクトルを求めて、新しいセントロイド$\\,\\mu_k\\,$を設定する<br>\n","  $\\qquad$新旧クラスタを比較して、クラスタが変わらなかったら終了する<br>\n","  end while：<br><br>\n","- <font color=\"silver\">k-ｍeans++のAlgorithm</font><br><br>\n","データ$\\,x_i\\,$からランダムに1つデータを選び、それをセントロイド$\\,\\mu_1\\,$とする<br>\n","データ$\\,x_i\\,$とセントロイド$\\,\\mu_1\\,$との一番近い距離$\\,D(x)\\,$をとる<br>\n","重み付き確率からセントロイドをランダムに設定する<br>\n","$\\phi(x) = \\cfrac{D(x_i)}{\\sum_k D(x_k)}$<br>\n","代表点の合計数が$\\,k\\,$個集まればk-meansを実行する<br><br>\n","- <font color=\"silver\">X-ｍeansアルゴリズム</font><br><br>\n","クラスタの個数を決定するAlgorithm<br>\n","2個のセントロイドをk-means++で決める<br>\n","上記セントロイドによりクラスタリング<br>\n","クラスタに対してk-meansでクラスタリングして分割前と後のＢＩＣを計算する<br>\n","分割前のＢＩＣよりも分割後のＢＩＣの方が大きければ分割を適用する<br>\n","クラスタサイズが一定より小さくなるか、分割するクラスタが無くなったら終了<br>"],"metadata":{"id":"reiCvMDTqcDB"}},{"cell_type":"markdown","source":["####<font color=\"silver\">PCA</font><br>\n"," - <font color=\"silver\">目的関数<br></font><br>\n","$N$個のデータ点$\\pmb{x}_1, \\dots, \\pmb{x}_N$をベクトル$\\pmb{u}$で射影したデータ点$\\pmb{u}^\\top\\pmb{x}_i$の分散$s_y^2$が最大となる$\\pmb{u}^{*}$を求める<br><br>\n","$\\begin{align}\\pmb{u}^*&=\\mathop{\\rm argmax}\\limits_{\\pmb{u}}s_y^2\\\\&= \\mathop{\\rm argmax}\\limits_{\\pmb{u}}\\cfrac{1}{N} \\sum_{i=1}^N \\left(\\pmb{u}^\\top \\pmb{x}_i - \\pmb{u}^\\top \\pmb{\\bar{x}}\\right)^2 \\\\\n","&=\\mathop{\\rm argmax}\\pmb{u}^\\top \\left[\\frac{1}{N} \\sum_{i=1}^N (\\pmb{x}_i - \\pmb{\\bar{x}})(\\pmb{x}_i - \\pmb{\\bar{x}})^\\top\\right] \\pmb{u} \\\\\n","&=\\mathop{\\rm argmax}\\limits_{\\pmb{u}}\\pmb{u}^\\top \\pmb{S} \\pmb{u}\n","\\end{align}$\n"],"metadata":{"id":"1yeNZ9TUrmh5"}},{"cell_type":"markdown","source":["####<font color=\"silver\">AE</font><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://lilianweng.github.io/posts/2018-08-12-vae/)</font></font><br>\n","<img src=\"https://lilianweng.github.io/posts/2018-08-12-vae/autoencoder-architecture.png\" width=\"320\"><br>\n","$\\mathbf{x} \\approx \\mathbf{x}'$<br><br>\n","$\\mathbf{x}' = f_\\theta(g_\\phi(\\mathbf{x}))$<br><br>\n","$\\mathbf{x} \\approx f_\\theta(g_\\phi(\\mathbf{x}))$<br><br>\n","$\\displaystyle L_\\text{AE}(\\theta, \\phi) = \\frac{1}{n}\\sum_{i=1}^n (\\mathbf{x}^{(i)} - f_\\theta(g_\\phi(\\mathbf{x}^{(i)})))^2$<br>\n","<font color=\"silver\">DAE<br></font>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://lilianweng.github.io/posts/2018-08-12-vae/)</font></font><br>\n","<img src=\"https://lilianweng.github.io/posts/2018-08-12-vae/denoising-autoencoder-architecture.png\" width=\"320\"><br>\n","$\\mathbf{x} \\approx \\mathbf{x}'$<br><br>\n","$\\mathbf{x}' = f_\\theta(g_\\phi(\\tilde{\\mathbf{x}}^{(i)}))$<br><br>\n","$\\mathbf{x} \\approx f_\\theta(g_\\phi(\\tilde{\\mathbf{x}}^{(i)}))$<br><br>\n","$\\tilde{\\mathbf{x}}^{(i)} \\sim \\mathcal{M}_\\mathcal{D}(\\tilde{\\mathbf{x}}^{(i)} \\vert \\mathbf{x}^{(i)})$<br><br>\n","$\\displaystyle L_\\text{DAE}(\\theta, \\phi) = \\frac{1}{n} \\sum_{i=1}^n (\\mathbf{x}^{(i)} - f_\\theta(g_\\phi(\\tilde{\\mathbf{x}}^{(i)})))^2$<br>\n","<font color=\"silver\">VAE<br></font>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://lilianweng.github.io/posts/2018-08-12-vae/)</font></font><br>\n","事前分布$p_\\theta(\\mathbf{z})$から$\\mathbf{z}$をサンプリングしたあと、デコーダーの尤度分布$p_\\theta(\\mathbf{x}\\vert\\mathbf{z})$から$\\mathbf{x}$をサンプリングする。$\\mathbf{z}$がサンプリングされる空間を絞るために$\\mathbf{z} \\sim p(\\mathbf{z})=  \\mathcal{N}(\\mathbf{0},\\mathbf{I})$とする。<br>\n","<img src=\"https://lilianweng.github.io/posts/2018-08-12-vae/vae-gaussian.png\" width=\"320\"><img src=\"https://lilianweng.github.io/posts/2018-08-12-vae/VAE-graphical-model.png\" width=\"320\"><br>\n","<font color=\"silver\">Reparametrization Trick</font><br><br>\n","   <font color=\"black\">$\\mathbf{z} = \\boldsymbol{\\mu} + \\boldsymbol{\\sigma} \\odot \\boldsymbol{\\epsilon} $<br><br>\n","<font color=\"black\">$\\text{where } \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\boldsymbol{I}) $"],"metadata":{"id":"eKiyYytVauML"}},{"cell_type":"markdown","source":["####<font color=\"silver\">VAE</font><br>\n"," - <font color=\"silver\">最尤推定を考える<br></font><br>\n","$\\displaystyle\\theta^{*} = \\arg\\max_\\theta \\prod_{i=1}^n p_\\theta(\\mathbf{x})$<br><br>\n"," - <font color=\"silver\">周辺化を考える<br></font><br>\n","$\\begin{align} \n","p_{\\mathbf{\\theta}}(\\mathbf{x}) =  \\int  p_{\\mathbf{\\theta}}(\\mathbf{x} \\mid \\mathbf{z}) p_{\\mathbf{\\theta}}(\\mathbf{z}) d \\mathbf{z}\n","\\end{align}$<br>\n","$\\begin{align} \n","\\log p_{\\mathbf{\\theta}}(\\mathbf{x}) \n","= \\log \\int  p_{\\mathbf{\\theta}}  (\\mathbf{x} \\mid \\mathbf{z}) p_{\\mathbf{\\theta}}(\\mathbf{z}) d \\mathbf{z}\n","\\end{align}$<br><br>\n"," - <font color=\"silver\">$p_\\theta(\\mathbf{z}\\vert\\mathbf{x})$を$q_\\phi(\\mathbf{z}\\vert\\mathbf{x})$で近似<br></font><br>\n","$\\begin{align} \\log p_\\theta({\\bf{x}})&=\\int_{\\bf{z}} q_\\phi({\\bf{z}}|{\\bf{x}}) \\log p_\\theta({\\bf{x}}) d{\\bf{z}} \\end{align}$<br><br>\n"," - <font color=\"silver\">変分ベイズ<br></font><br>\n","$\\log p_{\\mathbf{\\theta}}(\\mathbf{x}) \n","= \\underbrace{\\mathcal{L}_{\\mathbf{\\theta},\\mathbf{\\phi}}(\\mathbf{x})}_{\\text{ELBO}} \n","+\\underbrace{\\mathcal{D}_{KL}( q_{\\mathbf{\\phi}}(\\mathbf{z} \\mid \\mathbf{x}) || p_{\\mathbf{\\theta}}(\\mathbf{z} \\mid \\mathbf{x}))}_{\\text{KLダイバージェンス}}$<br><br>\n"," - <font color=\"silver\">ELBO<br></font><br>\n","$\\mathcal{L}({\\bf{x}}, \\phi, \\theta) =\\mathbb{E}_{z \\sim q_\\phi (z|x)}\\left[\\log p_\\theta({\\bf{x}}|{\\bf{z}})\\right] -D_{KL}\\left( q_\\phi({\\bf{z}}|{\\bf{x}})||p_\\theta({\\bf{z}}) \\right)$<br>\n","$\\mathcal{L}({\\bf{x}}, \\phi, \\theta) \n","= \\underbrace{\\mathbb{E}_{z \\sim q_\\phi (z|x)}\\left[\\log p_\\theta({\\bf{x}}|{\\bf{z}})\\right]}_{\\text{再構成誤差項、大きくする}} \n","-\\underbrace{D_{KL}\\left( q_\\phi({\\bf{z}}|{\\bf{x}})||p_\\theta({\\bf{z}}) \\right)}_{\\text{正則化項、小さくする}}$<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/kenmatsu4/items/b029d697e9995d93aa24)</font></font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F50670%2F2fe60ff8-2015-2b3a-0b78-ce5d7ce8f79a.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=9b431fe5bfafe35667f88432821b9a67\" width=\"240\"><br>\n"," - <font color=\"silver\">再構成誤差項<br></font><br>\n","<font color=\"black\">$\\mathbb{E}_{z \\sim q_\\phi (z|x)} \\left[\\log p_\\theta({\\bf{x}}|{\\bf{z}})\\right] =\\displaystyle\\sum_{i = 1}^D x_{i} \\log (f(z_{i}))+(1-x_{i}) \\log(1-f(z_{i}))$<br><br>\n"," - <font color=\"silver\">正則化項<br></font><br>\n","$\\displaystyle\\begin{align} D_{KL}\\left(q(z|x)||p_\\theta(z) \\right) &=D_{KL} (N(μ,Σ) || N(0,\\boldsymbol{I}))\\\\\\\\&= -\\displaystyle \\cfrac{1}{2}\\sum_{j = 1}^J(1 + \\log(\\sigma_{j}^2)-\\mu_{j}^2 - \\sigma_{j}^2) \n","\\\\\\\\&= -\\frac{1}{2}(1+2\\log \\sigma_{j}-\\mu_{j}^{2} - \\sigma_{j}^{2})\\end{align}$<br>\n"," - <font color=\"silver\">損失関数<br></font><br>\n","$\\mathcal{L}({\\bf{x}}, \\phi, \\theta) \n","= \\underbrace{\\mathbb{E}_{z \\sim q_\\phi (z|x)}\\left[\\log p_\\theta({\\bf{x}}|{\\bf{z}})\\right]}_{\\text{再構成誤差項、大きくする}} \n","-\\underbrace{D_{KL}\\left( q_\\phi({\\bf{z}}|{\\bf{x}})||p_\\theta({\\bf{z}}) \\right)}_{\\text{正則化項、小さくする}}$<br>\n","<font color=\"black\">$L = -\\displaystyle\\sum_{i = 1}^D x_{i} \\log (f(z_{i}))+(1-x_{i}) \\log(1-f(z_{i}))  -\\displaystyle \\frac{1}{2}\\sum_{j = 1}^J(1 + \\log(\\sigma_{j}^2)-\\mu_{j}^2 - \\sigma_{j}^2)$\n","<br>\n","$D$：<font color=\"silver\">出力層のノード数<br></font>\n","$J$：<font color=\"silver\">正規分布の次元数<br>"],"metadata":{"id":"H0HAkn5s4e6B"}},{"cell_type":"markdown","source":["####<font color=\"silver\">VQ-VAE</font><br>\n","<font color=\"silver\">サンプリング</font><br><br>\n","<font color=\"black\">$\\begin{align} q(z=k|x)=\\left\\{\\begin{array}{ll}1&\\text{for }k=\\arg \\min_j \\| z_e(x)-e_j \\|_2, \\\\ 0&\\text{otherwise}\\end{array}\\right. \\end{align}$<br><br>\n","$z_q(x)=e_k, \\hspace{10pt}\\text{where   } k=\\arg\\min_j\\|z_e(x)-e_j\\|_2$\n","<br><br>\n","$j$：インデックス<br></font>\n","$k$：インデックス、$K$は埋め込み空間のサイズ<br></font>\n","$e_j$：インデックス$j$の埋め込みベクトル</font><br><br>\n","<font color=\"silver\">損失関数</font><br><br>\n","<font color=\"black\">$L=\\log p\\left(x|z_q(x)\\right) + \\left\\|\\text{sg}[z_e(x)]-e \\right \\|^2_2 + \\beta \\left\\|z_e(x)-\\text{sg}[e]\\right\\|^2_2$\n","<br><br>"],"metadata":{"id":"Ol1nGVNB-iox"}},{"cell_type":"markdown","source":["####<font color=\"silver\">GAN</font><br>\n"," - <font color=\"silver\">GAN<br></font><br>\n","$\\displaystyle \\min_G \\max_D V(D, G) = \\displaystyle \\mathbb{E}_{\\boldsymbol{x} \\mathtt{\\sim} p_{data}(\\boldsymbol{x})}[\\log D(\\boldsymbol{x})] + \\displaystyle \\mathbb{E}_{\\boldsymbol{z} \\mathtt{\\sim} p_{\\boldsymbol{z}}(\\boldsymbol{z})}[\\log (1 - D(G(\\boldsymbol{z})))]$<br><br>\n"," - <font color=\"silver\">CGAN<br></font><br>\n","$\\displaystyle \\min_G \\max_D V(D, G) = \\displaystyle \\mathbb{E}_{\\boldsymbol{x} \\mathtt{\\sim} p_{data}(\\boldsymbol{x})}[\\log D(\\boldsymbol{x}|\\boldsymbol{y})] + \\displaystyle \\mathbb{E}_{\\boldsymbol{z} \\mathtt{\\sim} p_{\\boldsymbol{z}}(\\boldsymbol{z})}[\\log (1 - D(G(\\boldsymbol{z}|\\boldsymbol{y})))]$<br><br>\n"," - <font color=\"silver\">Pix2Pix（CGAN）<br></font></font><br>\n","$\\mathcal{L}_{cGAN}(G,D) = \\mathbb{E}_{x,y \\sim p_{data}(x,y)}[\\log D(x,y)] +  \\mathbb{E}_{x \\sim p_{data}(x), z \\sim p_z(z)}[\\log (1-D(x,G(x,z)))] \n","$<br><br>\n"," - <font color=\"silver\">Pix2Pix<br></font></font><br>\n","$\\mathcal{L}_{L1}(G) = \\mathbb{E}_{x,y \\sim p_{data}(x,y),z \\sim p_z(z)}[||y-G(x,z)||_1]$<br>\n","$G^*={\\rm arg}\\displaystyle \\min_G \\max_D \\mathcal{L}_{cGAN}(G,D)+ \\lambda \\mathcal{L}_{L1}(G)$<br>"],"metadata":{"id":"d2PGILYAW5pW"}},{"cell_type":"markdown","source":["####<font color=\"silver\">CTC</font><br>\n","-  <font color=\"silver\">定義</font><br><br>\n","$\\boldsymbol{x}$：<font color=\"silver\">入力音声系列<br></font>\n","$\\boldsymbol{l}$：<font color=\"silver\">ブランクを含まないラベル系列（教師）</font><br>\n","$\\pi$：<font color=\"silver\">パス<br></font>\n","$\\pi_t$：<font color=\"silver\">あるパスにおける時刻$t$のクラス番号<br></font>\n","$y_{\\pi_t}^ t$：<font color=\"silver\">$\\boldsymbol \\pi_t$のクラスに対する RNN の出⼒（確率）\n","<br></font>\n","$\\mathcal{B}$：<font color=\"silver\">パスを⽂字列に変換する縮約関数\n","</font><br><br>\n","-  <font color=\"silver\">パス$\\boldsymbol \\pi$の確率</font><br><br>\n","$\\displaystyle p(\\boldsymbol \\pi \\mid \\boldsymbol {x}) = \\prod_{t=1}^T y_{\\pi_t}^t$<br><br>\n","-  <font color=\"silver\">ラベル列$\\boldsymbol l$の確率</font><br><br>\n","$\\displaystyle p(\\boldsymbol{l} \\mid \\boldsymbol{x})=\\sum_{\\pi \\in \\mathcal{B}^{-1}(\\boldsymbol{l})}p(\\pi \\mid \\boldsymbol{x}) ＝\\sum_{\\pi \\in \\mathcal{B}^{-1}(\\boldsymbol{l})}\\prod_{t=1}^ T y_{\\pi_t}^ t$<br><br>\n","-  <font color=\"silver\">損失関数</font><br><br>\n","$\\mathcal{L} =\\displaystyle -\\log(p(\\boldsymbol{l} \\mid  \\boldsymbol{x}))$<br><br>\n","-  <font color=\"silver\">勾配</font><br><br>\n","$\\cfrac{\\partial L}{\\partial y_k^t} = \\cfrac{-\\log(p(\\boldsymbol{l} \\mid  \\boldsymbol{x}))}{\\partial y_k^t} = -\\cfrac{1}{p(\\boldsymbol{l} \\mid \\boldsymbol{x})}\\cfrac{\\partial p(\\boldsymbol{l} \\mid  \\boldsymbol{x})}{\\partial y_k^t}$<br>\n"],"metadata":{"id":"PoCDQXIUB7Tl"}},{"cell_type":"markdown","source":["####<font color=\"silver\">CTC</font><br>\n","- <font color=\"silver\">ラベル列$\\boldsymbol{l}'$の定義</font><br><br>\n","$\\boldsymbol{l}'$：<font color=\"silver\"> $\\boldsymbol{l}$の両端と各ラベルの間にブランクを追加したブランク入りラベル系列<br></font>\n","$|\\boldsymbol{l}'|$：<font color=\"silver\"> $2|\\boldsymbol{l}|+1$<br></font>\n","$\\cal B(\\boldsymbol \\pi_{1:t}) = \\boldsymbol l_{1:\\lfloor s/2 \\rfloor}$：<font color=\"silver\"> $\\alpha_t(s)$の総和の範囲<br></font>\n","$\\cal B(\\boldsymbol \\pi_{t:T}) = \\boldsymbol l_{\\lfloor s/2 \\rfloor:\\mid \\boldsymbol l \\mid}$：<font color=\"silver\">$\\beta_t(s)$の総和の範囲<br><br></font>\n","- <font color=\"silver\">再帰的性質</font><br><br>\n","$\\begin{align}\t\n","  \t\\alpha_t(s) = \n","\t  \\begin{cases}\n","\t    (\\alpha_{t-1}(s) + \\alpha_{t-1}(s-1)) y_{\\boldsymbol l'_s}^t & \\text{if } \\boldsymbol l'_s = blank \\text{ or } \\boldsymbol l'_{s-2} = \\boldsymbol l'_s\\\\\n","\t    \\left( \\alpha_{t-1}(s) + \\alpha_{t-1}(s-1) + \\alpha_{t-1}(s-2) \\right)y_{\\boldsymbol l'_s}^t & \\text{otherwise}\n","\t  \\end{cases}\n","  \\end{align}$<br><br>\n","  $\\boldsymbol l’_{s-2} = \\boldsymbol l’_s$ は例外のケースに相当する。例えば、cutter の tt ような同じ文字が続く場合は、間に必ずブランクを入れる。<br><br>\n","- <font color=\"silver\">パス$\\boldsymbol \\pi$の確率の総和</font><br><br>\n","$\\displaystyle \\alpha_t(s) \\overset{\\rm def}{=} \\sum_{\\cal B(\\boldsymbol \\pi_{1:t}) = \\boldsymbol l_{1:\\lfloor s/2 \\rfloor}} \\prod_{t'=1}^t y_{\\boldsymbol \\pi_{t'}}^{t'}$<br><br>\n","$\\displaystyle\\beta_t(s) \\overset{\\rm def}{=} \\sum_{\\cal B(\\boldsymbol \\pi_{t:T}) = \\boldsymbol l_{\\lfloor s/2 \\rfloor:\\mid \\boldsymbol l \\mid}} \\prod_{t'=t}^T y_{\\boldsymbol \\pi_{t'}}^{t'}$<br><br>\n","$\\displaystyle\\alpha_t(s)\\beta_t(s) = \\sum_{\\boldsymbol \\pi \\in {\\cal B}^{-1}(\\boldsymbol l) \\pi_t = l'_s} y_{l'_s}^t \\prod_{t=1}^T y_{\\pi_t}^t$</font>\n","<br><br>\n","- <font color=\"silver\">パス$\\boldsymbol \\pi$の確率の導出</font><br><br>\n","$\\displaystyle\\alpha_t(s)\\beta_t(s) = \\sum_{\\boldsymbol \\pi \\in {\\cal B}^{-1}(\\boldsymbol l) \\pi_t = l'_s} y_{l'_s}^t \\prod_{t=1}^T y_{\\pi_t}^t$\n","<br><br>\n","$\\displaystyle p(\\boldsymbol \\pi \\mid \\boldsymbol {x}) = \\prod_{t=1}^T y_{\\pi_t}^t$<br><br>\n","$\\displaystyle\\frac{\\alpha_t(s)\\beta_t(s)}{y_{l'_s}^t} = \\sum_{\\boldsymbol \\pi \\in {\\cal B}^{-1}(\\boldsymbol l) \\pi_t = l'_s}  p(\\boldsymbol \\pi \\mid \\boldsymbol { x})$<br><br>\n","- <font color=\"silver\">ラベル列$\\boldsymbol l$の確率の導出</font><br><br>\n","$\\displaystyle\\frac{\\alpha_t(s)\\beta_t(s)}{y_{l'_s}^t} = \\sum_{\\boldsymbol \\pi \\in {\\cal B}^{-1}(\\boldsymbol l) \\pi_t = l'_s}  p(\\boldsymbol \\pi \\mid \\boldsymbol { x})$<br><br>\n","$\\displaystyle p(\\boldsymbol{l} \\mid \\boldsymbol{x})=\\sum_{\\boldsymbol{\\pi} \\in \\mathcal{B}^{-1}(\\boldsymbol{l})}p(\\boldsymbol{\\pi} \\mid \\boldsymbol{x}) $<br><br>\n","$\\displaystyle p(\\boldsymbol l \\mid \\boldsymbol {x}) = \\sum_{s=1}^{\\mid \\boldsymbol l' \\mid} \\frac{\\alpha_t(s)\\beta_t(s)}{y_{l'_s}^t}$"],"metadata":{"id":"tIwkWog1C7yV"}},{"cell_type":"markdown","source":["####<font color=\"silver\">CTC</font><br>\n","\n","<font color=\"silver\">拡張ラベル系列を作成して尤度を解く</font><br>\n","-  ブランクの確率は、そのままコピーする<br>\n","-  確率ゼロは無視する<br>\n","-  時刻１は「_」または「a」<br>\n","-  時刻４は「i」または「_」であるが、「_」はゼロなので無視でき、この場合の時刻4は「i」のみ<br>\n","<font color=\"black\">\n","\n","|ラベル|1|2|3|4|\n","| :---: | :---: | :---: | :---: | :---: | \n","|a|1/2|1/2|1/4|0|\n","|i|1/4|0|1/4|1|\n","|_|1/4|1/2|1/2|0|\n","\n","|ラベル|1|2|3|4|\n","| :---: | :---: | :---: | :---: | :---: | \n","|_|1/4|1/2|1/2|0|\n","|a|1/2|1/2|1/4|0|\n","|_|1/4|1/2|1/2|0|\n","|i|1/4|0|1/4|1|\n","|_|1/4|1/2|1/2|0|\n","\n","“ _ _ a i ”$\\qquad$“ _ a _ i ”$\\qquad$“ _ a i i” <br>\n","“ _ a a i ”$\\qquad$“ a a _ i ”$\\qquad$“ a a i i” <br>\n","“ a a a i ”$\\qquad$“ a _ _ i ”$\\qquad$“ a _ i i ”<br><br>\n","\n","<font color=\"silver\">拡張ラベル系列を作成して尤度を解く</font><br>\n","- ブランクの確率は、そのままコピーする<br>\n","- 確率ゼロは無視する<br>\n","- 時刻１は「_」または「a」であるが、「_」はゼロなので無視でき、この場合の時刻1は「a」のみ<br>\n","- 時刻４は「b」または「_」<br>\n","<font color=\"black\">\n","\n","|ラベル|1|2|3|4|\n","| :---: | :---: | :---: | :---: | :---: | \n","|a|2/3|0|0|1/4|\n","|b|1/2|1/3|2/3|1/2|\n","|_|0|2/3|1/3|1/4|\n","\n","|ラベル|1|2|3|4|\n","| :---: | :---: | :---: | :---: | :---: | \n","|_|0|2/3|1/3|1/4|\n","|a|2/3|0|0|1/4|\n","|_|0|2/3|1/3|1/4|\n","|b|1/2|1/3|2/3|1/2|\n","|_|0|2/3|1/3|1/4|\n","\n","abbb, $\\frac{2}{3}$×$\\frac{1}{3}$×$\\frac{2}{3}$×$\\frac{1}{2}$<br><br>\n","abb_, $\\frac{2}{3}$×$\\frac{1}{3}$×$\\frac{2}{3}$×$\\frac{1}{4}$<br><br>\n","ab__, $\\frac{2}{3}$×$\\frac{1}{3}$×$\\frac{1}{3}$×$\\frac{1}{4}$<br><br>\n","a_bb, $\\frac{2}{3}$×$\\frac{2}{3}$×$\\frac{2}{3}$×$\\frac{1}{2}$<br><br>\n","a_b_, $\\frac{2}{3}$×$\\frac{2}{3}$×$\\frac{2}{3}$×$\\frac{1}{4}$<br><br>\n","a__b, $\\frac{2}{3}$×$\\frac{2}{3}$×$\\frac{1}{3}$×$\\frac{1}{2}$<br><br>\n","\n","\n","<font color=\"blue\">Best Path Decoding</font><br>\n","推論時は、近似解として、Best Path Decodingを行って求める。<br>\n","-  各時刻において最も確率の高いラベル(文字)を選んで、\n","それをつなげる<br>\n","-  同じラベルが連続した場合は 1 つにまとめる<br>\n","-  ブランクは削除する<br>\n","<font color=\"black\">\n","\n","|ラベル|1|2|3|4|5|6|7|8 |\n","| :---: | :---: | :---: | :---: | :---: | :---: |  :---: | :---: | :---: |\n","|a|0.1|0.6|0|0.1|0|0.2|0|0.1|\n","|i|0|0.1|0.4|0.7|0|0.3|0.1|0|\n","|u|0|0.3|0.3|0.1|0|0.4|0.1|0.1|\n","|_|0.9|0|0.3|0.1|1.0|0.0|0.8|0.8|\n","||||||||||\n","|パス|_|a|i|i|_|u|_|_|\n","文字列：aiu\n","\n","<br>1/2 ×1/2×1/2×1 ＝1/8<br>\n","\n","|ラベル|1|2|3|4|\n","| :---: | :---: | :---: | :---: | :---: | \n","|a|1/2|1/2|1/4|0|\n","|i|1/4|0|1/4|1|\n","|_|1/4|1/2|1/2|0|\n","\n","<br>2/3 ×2/3×2/3×1/2 ＝8/54＝4/27<br>\n","\n","|ラベル|1|2|3|4|\n","| :---: | :---: | :---: | :---: | :---: | \n","|a|2/3|0|0|1/4|\n","|b|1/2|1/3|2/3|1/2|\n","|_|0|2/3|1/3|1/4|"],"metadata":{"id":"aPXx5FQMB76J"}},{"cell_type":"markdown","source":["-  <font color=\"silver\">Best Path Decoding</font><br><br>\n","   推論時は、近似解として、Best Path Decodingを行って求める。<br>\n","  - 各時刻において最も確率の高いラベル(文字)を選んで、\n","それをつなげる<br>\n","  - 同じラベルが連続した場合は 1 つにまとめる<br>\n","  - ブランクは削除する<br><br>\n","<font color=\"black\">\n","\n","|ラベル|1|2|3|4|5|6|7|8 |\n","| :---: | :---: | :---: | :---: | :---: | :---: |  :---: | :---: | :---: |\n","|a|0.1|0.6|0|0.1|0|0.2|0|0.1|\n","|i|0|0.1|0.4|0.7|0|0.3|0.1|0|\n","|u|0|0.3|0.3|0.1|0|0.4|0.1|0.1|\n","|_|0.9|0|0.3|0.1|1.0|0.0|0.8|0.8|\n","||||||||||\n","|パス|_|a|i|i|_|u|_|_|\n","文字列：aiu\n"],"metadata":{"id":"_Co6ZSdsBkO1"}},{"cell_type":"markdown","source":["####<font color=\"silver\">ベルマン方程式</font><br>\n"," - <font color=\"silver\">収益</font><br><br>\n","$\\displaystyle R_t = r_{t+1} + \\gamma \\, r_{t+2} +  \\gamma^2 \\, r_{t+3} + ... = \\sum_{k=0}^{\\infty} \\gamma^k \\, r_{t+k+1}$<br>\n","$\\displaystyle R_t =\\sum_{k=0}^T γ^k r_{t+k+1}$<br><br>\n","$\\displaystyle R_t=r_{t+1}+γR_{t+1}$<br><br>\n"," - <font color=\"silver\">価値</font><br><br>\n","$V^{\\pi}(s) ={\\mathbb E}^\\pi\\left[G_t|s_t = s\\right]$<br><br>\n","$Q^{\\pi}(s,a) ={\\mathbb E}^\\pi\\left[G_t | s_t = s, a_t = a\\right]$<br><br>\n"," - <font color=\"silver\">状態ベルマン方程式</font><br><br>\n","$ V^{\\pi}(s) = \\displaystyle\\sum_{a\\in A}\\pi(a|s)\\sum_{s'\\in S}P(s'|s, a)[r(s, a, s') + \\gamma V^{\\pi}(s') ]$<br><br>\n","$ V^{*}(s)=\\displaystyle\\max_{a}\\sum_{s^{\\prime}} P(s^{\\prime}|s,a)[r(s,a,s^{\\prime})+\\gamma V^{*}(s^{\\prime})]$<br><br>\n","$V^\\pi(s) = \\displaystyle\\sum_{a\\in\\mathcal{A}}\\pi(a|s)Q^\\pi(s,a)$<br><br>\n"," - <font color=\"silver\">行動ベルマン方程式</font><br><br>\n","$Q^\\pi (s, a) =  \\displaystyle\\sum_{s'\\in S} P(s'|s, a)\\left[r(s, a, s') + \\gamma V^{\\pi}(s') \\right]$<br><br>\n","$ Q^\\pi (s, a) = \\displaystyle\\sum_{s'\\in S} P(s'|s, a)[r(s, a, s') + \\gamma \\displaystyle\\sum_{a'\\in\\mathcal{A}}\\pi(a'|s')Q^\\pi(s',a')]  $<br><br>\n","$ Q^{*}(s,a)=\\displaystyle\\sum_{s^{\\prime}} P(s^{\\prime}|s,a)[r(s,a,s^{\\prime})+\\gamma \\max_{a^{\\prime}}Q^{*}(s^{\\prime},a^{\\prime})]$"],"metadata":{"id":"1xeMoscjpm-T"}},{"cell_type":"markdown","source":["####<font color=\"silver\">価値推定</font><br>\n"," - <font color=\"silver\">方策評価, Policy evaluation<br>\n","状態ベルマン方程式で評価</font><br><br>\n","$\\displaystyle V^{\\pi} (s)  = \\sum_{a \\in \\mathcal{A}(s)} \\pi(s\\mid a) \\, \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [ r(s, a, s') + \\gamma \\, V^{\\pi} (s') ]$<br><br>\n"," - <font color=\"silver\">方策改善, Policy improvement<br>\n","行動ベルマン最適方程式で最大化</font><br><br>\n","$\\displaystyle \\pi(s) \\leftarrow \\text{argmax}_a Q^{\\pi} (s, a) = \\sum_{s' \\in \\mathcal{S}} p(s' | s, a) \\, [r(s, a, s') + \\gamma \\, V^{\\pi}(s') ] $<br><br>\n"," - <font color=\"silver\">価値反復法の価値更新<br>\n","状態ベルマン最適方程式で最大化</font><br><br>\n","<font color=\"black\">$\\displaystyle V_{k+1}(s) = \\max_a \\sum_{s'} p(s' | s,a) [r(s, a, s') + \\gamma \\, V_k(s') ]$</font><br><br>\n"," - <font color=\"silver\">モンテカルロ法<br></font><br>\n","$\\displaystyle  V^π(s)= \\mathbb{E}_π[R_t|s_t=s] ≈  \\frac{1}{M} \\sum_{e=1}^M R_t^{(e)}$<br><br>\n","$V(s_t) = V(s_t) + \\alpha \\, (R_t - V(s_t))$<br><br>\n","$Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\, (R_t - Q(s_t, a_t))$<br><br>\n"," - <font color=\"silver\">SARSA<br></font><br>\n","$Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\, (r_{t+1} + \\gamma \\, Q(s_{t+1}, a_{t+1})  - Q(s_t, a_t))$<br><br>\n","$\\begin{split}\\pi(s_t, a) = \\begin{cases}1 - \\epsilon \\; \\text{if} \\; a = \\text{argmax} \\, Q(s_t, a) \\\\\\frac{\\epsilon}{|\\mathcal{A}(s_t) -1|} \\; \\text{otherwise.} \\\\ \\end{cases}\\end{split}$<br><br>\n"," - <font color=\"silver\">Q学習<br></font><br>\n","$Q(s_t, a_t) = Q(s_t, a_t) + \\alpha \\, (r_{t+1} + \\gamma \\, \\max_a Q(s_{t+1}, a) - Q(s_t, a_t))$<br><br>\n","$\\begin{split}\\pi(s_t, a) = \\begin{cases}1\\; \\text{if} \\; a = \\text{argmax} \\, Q(s_t, a) \\\\ 0 \\; \\text{otherwise.} \\\\ \\end{cases}\\end{split}$<br><br>\n"," - <font color=\"silver\">DQN<br></font><br>\n","$\\displaystyle L(\\theta) = {\\mathbb E}_{s,a,s',r～ D} \\left[\\left(r + \\gamma\\max_{a'} Q(s', a';\\theta^-)-Q(s,a;\\theta) \\right)^2\\right]$"],"metadata":{"id":"u25k1gghu341"}},{"cell_type":"markdown","source":["####<font color=\"silver\">方策勾配</font><br>\n"," - <font color=\"silver\">方策勾配定理の導出<br>\n"," 対数の微分の公式を用いて変形する $(\\log x)' = \\frac{1}{x}$</font><br><br>\n","$\\displaystyle{\\nabla} \\log \\pi(\\theta) \n","= \\frac{\\partial \\log \\pi(\\theta)}{\\partial \\theta} \n","= \\frac{1}{\\pi(\\theta)}\\frac{\\partial \\pi(\\theta)}{\\partial \\theta} \n","= \\frac{{\\nabla}\\pi(\\theta)}{\\pi(\\theta)}$<br><br>\n","$\\begin{aligned}\n","\\nabla J(\\theta) &= \\sum_{s \\in \\mathcal{S}} d(s) \\sum_{a \\in \\mathcal{A}} \\nabla \\pi(a \\vert s; \\theta) Q_\\pi(s, a) \\\\\\\\\n","&= \\sum_{s \\in \\mathcal{S}} d(s) \\sum_{a \\in \\mathcal{A}} \\pi(a \\vert s; \\theta) \\frac{\\nabla \\pi(a \\vert s; \\theta)}{\\pi(a \\vert s; \\theta)} Q_\\pi(s, a) \\\\\\\\\n","& = \\sum_{s \\in \\mathcal{S}} d(s) \\sum_{a \\in \\mathcal{A}} \\pi(a \\vert s; \\theta) \\nabla \\log \\pi(a \\vert s; \\theta) Q_\\pi(s, a) \\\\\\\\\n","& = \\mathbb{E}_{\\pi_\\theta} [\\nabla \\log \\pi(a \\vert s; \\theta) Q_\\pi(s, a)]\n","\\end{aligned}$<br><br>\n"," - <font color=\"silver\">モンテカルロ近似</font><br><br>\n","${\\begin{align}\n","\\nabla J(\\theta) &\\propto \\mathbb{E}_{\\pi_\\theta} [\\nabla \\log \\pi(a \\vert s, \\theta) Q_\\pi(s, a)]\\\\\\\\\n","& \\approx \\frac{1}{N}\\sum_{n=1}^{N}\\frac{1}{T}\\sum_{t=1}^{T}\n","\\nabla_\\theta\\log \\pi_\\theta(a_t^n|s_t^n)Q^\\pi(a_t^n|s_t^n)\n","\\end{align}}$<br>"],"metadata":{"id":"1jY4_L_V0t5N"}},{"cell_type":"markdown","source":["####<font color=\"silver\">Actor-Critic</font>\n"," - <font color=\"silver\">kステップ先読みした収益</font><br><br>\n","$\\displaystyle{ Q^{\\pi}(s_{t}, a_{t}) = \\sum_{i=0}^{k-1}{ \\gamma^iR_{i+1} } + \\gamma^k V^{\\pi}(s_{t+k}) }$<br><br>\n"," - <font color=\"silver\">Actor</font><br><br>\n"," <font color=\"Silver\">Policy Gradient</font><br><br>\n","$\\displaystyle\\nabla \\mathcal{J}  = \\mathbb{E}_{\\pi_\\theta} [\\nabla \\log \\pi(a \\vert s; \\theta) Q_\\pi(s, a)]$<br><br>\n"," <font color=\"Silver\">REINFORCE</font><br><br>\n","$\\displaystyle\\nabla \\mathcal{J}  = \\mathbb{E}_{\\pi_\\theta} [\\nabla \\log \\pi(a \\vert s; \\theta) R_t]$<br><br>\n"," <font color=\"Silver\">REINFORCE with baseline</font><br><br>\n","$\\displaystyle\\nabla \\mathcal{J}  = \\mathbb{E}_{\\pi_\\theta} [\\nabla \\log \\pi(a \\vert s; \\theta) R_t-b]$<br><br>\n"," <font color=\"Silver\">advantage actor-critic</font><br><br>\n","${\\begin{align}\n","\\nabla \\mathcal{J} \n","& = \\mathbb{E}_{\\pi_\\theta} [\\nabla \\log \\pi(a \\vert s, \\theta) A_\\pi(s,a)]\\\\\\\\\n","& = \\mathbb{E}_{\\pi_\\theta} [\\nabla \\log \\pi(a \\vert s, \\theta) Q_\\pi(s, a)- V_\\pi(s)]\\\\\\\\\n","& = \\mathbb{E}_{\\pi_\\theta} [\\nabla \\log \\pi(a \\vert s, \\theta) \\sum_{i=0}^{k-1}{ R^i_{t+i} } + \\gamma^k V_{\\pi}(s_{t+k})- V_\\pi(s)]\n","\\end{align}}$<br>\n"," - <font color=\"silver\">Critic</font><br><br>\n","  <font color=\"Silver\">Monte-Carlo critic</font><br><br>\n","$\\mathcal{L}(\\varphi) =  \\mathbb{E}_{s \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[(R(s, a) - Q_\\varphi(s, a))^2]$<br><br>\n"," <font color=\"Silver\">SARSA critic</font><br><br>\n","$\\mathcal{L}(\\varphi) =  \\mathbb{E}_{s, s' \\sim \\rho_\\theta, a, a' \\sim \\pi_\\theta}[(r + \\gamma \\, Q_\\varphi(s', a') - Q_\\varphi(s, a))^2]$<br><br>\n"," <font color=\"Silver\">Q-learning critic</font><br><br>\n","$\\mathcal{L}(\\varphi) =  \\mathbb{E}_{s, s' \\sim \\rho_\\theta, a \\sim \\pi_\\theta}[(r + \\gamma \\, \\max_{a'} Q_\\varphi(s', a') - Q_\\varphi(s, a))^2]$<br>"],"metadata":{"id":"NzKwiMSj17W8"}},{"cell_type":"markdown","source":["####<font color=\"silver\">距離学習</font>\n","   - <font color=\"silver\">Contrastive Loss</font><br><br>\n","$L=\\cfrac{1}{2} \\left(Yd^2+(1-Y)\\max(m-d, 0)^2 \\right)$<br>\n","<br>\n","$Yd^2$：<font color=\"silver\">類似ペアの距離を小さくする</font><br>\n","$(1-Y)\\max(m-d, 0)^2$：<font color=\"silver\">非類似ペアの距離をマージンに近づける</font><br>\n","$d$：<font color=\"silver\">二つの埋め込みベクトルの距離</font><br>\n","$Y$：<font color=\"silver\">ベクトルの関係を表すラベル（類似=1、非類似=0）</font><br>\n","$m$：<font color=\"silver\">マージン（類似ペアとして扱われる距離の最大値）</font><br><br>\n","   - <font color=\"silver\">Triplet Loss</font><br><br>\n","$L = [d_p- d_n + \\alpha]_+$<br><br>\n","${\\rm where}[z]_+ = \\max(z, 0)$<br><br>\n","$\\alpha$<font color=\"silver\">：マージンの大きさを決める正の値であり、ハイパーパラメータ</font><br>"],"metadata":{"id":"KnMlbhRTLUM5"}},{"cell_type":"markdown","source":["####<font color=\"silver\">MAML</font>\n","-  Few-shot learning<br><br>\n","-  <font color=\"silver\">MAMLの目的関数</font><br><br>\n","$\\begin{align}\n","\\theta &= {\\rm arg}\\min_\\theta \\sum_{\\mathcal T_i \\sim p(\\mathcal T)} \\mathcal L_{\\mathcal T_i}(f_{\\theta_i'}) \\\\\\\\\n","&= {\\rm arg}\\min_\\theta \\sum_{\\mathcal T_i \\sim p(\\mathcal T)} \\mathcal L_{\\mathcal T_i}(f_{\\theta - \\alpha \\nabla_{\\theta}\\mathcal{L}_{\\mathcal T_i}(f_\\theta)})\n","\\end{align}$<br><br>\n","- <font color=\"silver\">MAMLのパラメータ更新</font><br><br>\n","$\\,\\displaystyle\\theta \\leftarrow \\theta - \\beta \\nabla_\\theta \\sum_{\\mathcal T_i \\sim p(\\mathcal T)} \\mathcal L_{\\mathcal T_i} (f_{\\theta_i'})\\,$<br>"],"metadata":{"id":"CyZsUZSKKqSj"}},{"cell_type":"markdown","source":["####<font color=\"silver\">LIME</font>\n","-  <font color=\"silver\">LIMEの損失関数<br>\n","$f$と$g$との距離を類似度$\\,π_x\\,$で重み付けしたもの</font><br><br>\n","$\\displaystyle L(f,g,\\pi_x )=\\sum_{z,z^{\\prime } \\in Z} \\pi_x (z)(f(z)-g(z^{\\prime } ))^2$<br><br>\n","$\\displaystyle \\pi_x (z) = \\exp\\Bigl(\\frac{-D(x,z)^2}{\\sigma^2}\\Bigr)$<br><br>\n","$z' \\in \\{0,1\\}^d$<font color=\"Silver\">：非ゼロ要素を一部だけ含むサンプリングにより生成された2値のスパースな点</font><br>\n","$z \\in R^d$<font color=\"Silver\">：$\\,z′\\,$を用いて復元された元のサンプルの特徴表現</font><br>\n","$D(x,z)$<font color=\"Silver\">：距離関数</font><br>\n","$\\sigma$<font color=\"Silver\">：カーネル指数</font><br><br>\n","- <font color=\"silver\">LIMEの目的関数</font><br><br>\n","$\\xi (x)=\\mathop{\\rm argmin}\\limits_{g\\in G} L(f,g,\\pi_x )+\\Omega (g)$\n","<br><br>\n","$\\Omega (g)$<font color=\"Silver\">：モデル$\\,g\\,$の複雑さ</font><br>\n","$x$<font color=\"Silver\">：説明したいインスタンス</font><br>\n","$f$<font color=\"Silver\">：説明したいブラックボックスなモデル</font><br>\n","$G$<font color=\"Silver\">：解釈可能なモデルの集合</font><br>\n","$g$<font color=\"Silver\">：解釈可能なモデルの集合$\\,G\\,$のうちの一つのモデル</font><br>"],"metadata":{"id":"j-9NX4AcIK7a"}},{"cell_type":"markdown","source":["####<font color=\"silver\">SHAP</font>\n","- <font color=\"silver\">Local Accuracy, 局所的正確性</font><br><br>\n","$\\displaystyle f(x)=g(x^{'})=\\phi_{0}+\\sum_{i=1}^{M}\\phi_{i}x_{i} ^{'}$<br><br>\n","- <font color=\"silver\">Missingness, 欠落性</font><br><br>\n","$x_{i}^{'}=0\\Rightarrow \\phi_{i}=0$<br><br>\n","- <font color=\"silver\">Consistency, 一貫性</font><br><br>\n","$ f_\\boldsymbol{x}'(\\boldsymbol{z}') -   f_\\boldsymbol{x}'(\\boldsymbol{z}'\\backslash i)\n","  \\geq f_\\boldsymbol{x}(\\boldsymbol{z}') -   f_\\boldsymbol{x}(\\boldsymbol{z}'\\backslash i) \\ \\ \\mathrm{for \\ all \\ inputs } \\ \\boldsymbol{z}' \\in \\{0,1\\}^M$\n","のとき、<br><br>\n","$\\phi_i(f', \\boldsymbol{x}) \\geq \\phi_i(f, \\boldsymbol{x})$<br><br>\n","- <font color=\"silver\">限界貢献度</font><br><br>\n","$f_{x}(S\\cup \\{i\\})-f_{x}(S)$<br><br>\n","$f_{x}(S)=E[f(x)|x_{S}]$<br><br>\n","$S$：<font color=\"Silver\">モデルで使用されている特徴量の部分集合</font><br><br>\n","- <font color=\"silver\">シャープレイ値</font><br><br>\n","$\\displaystyle \\phi_{i}=\\sum_{S\\subseteq N\\setminus \\{i\\}}\\frac{|S|!(n-|S|-1)!}{n!}(f_{x}(S\\cup \\{i\\})-f_{x}(S))$<br><br>\n","$n$：<font color=\"Silver\">特徴量の数</font><br>"],"metadata":{"id":"l8dljR1KFhpp"}},{"cell_type":"markdown","source":["####<font color=\"silver\">CAM</font>\n","- <font color=\"silver\">GAP を行った結果 $F_{k}$</font><br><br>\n","   $\\displaystyle F_{k} = \\sum_{x, y}f_{k}(x,y)$<br><br>\n","- <font color=\"silver\">全結合層で計算する、CAM の計算部分は$\\sum_{k}w_{k}^{c}f_{k}(x,y)$</font><br><br>\n","   $\\displaystyle S_{c} =\\sum_{k}w_{k}^{c}F_{k}=  \\sum_{k}w_{k}^{c}\\sum_{x, y}f_{k}(x,y) = \\sum_{x, y}\\sum_{k}w_{k}^{c}f_{k}(x,y) $<br><br>\n","- <font color=\"silver\">カテゴリ c の出力値$S_{c}$</font><br><br>\n","   $\\displaystyle S_{c} = \\sum_{x, y} M_{c}(x,y)$<br><br>\n","- <font color=\"silver\">クラス$c$に対するAttention map$M_c$</font><br><br>\n","   $\\displaystyle M_c(x,y)=\\sum_{k}w^{c}_{k}f_{k}(x,y)$<br><br>\n","- <font color=\"silver\">定義</font><br><br>\n","$w^{c}_{k}$：<font color=\"Silver\">クラス$c$の$k$番目の特徴マップに対する結合重み</font><br>\n","$f_{k}(x,y)$：<font color=\"Silver\">k 番目の特徴量マップ</font><br>"],"metadata":{"id":"gCbbd2_X_1AQ"}},{"cell_type":"markdown","source":["####<font color=\"silver\">Grad-CAM<br>\n","<font color=\"silver\">勾配を計算</font><br><br>\n","$\\displaystyle\\frac{\\delta y^c}{\\delta A_{ij}^k}=\\displaystyle \\frac{\\partial S_{c}}{\\partial f_{k}(x, y)}$<br><br>\n","<font color=\"silver\">GAPする</font><br><br>\n","$\\displaystyle \\alpha^c_k = \\frac{1}{Z} \\sum_i \\sum_j \\frac{\\partial y^c}{\\partial A^k_{ij}}$<br><br>\n","<font color=\"silver\">加重平均する</font><br><br>\n","$\\displaystyle L_{\\text {Grad-CAM}}^{c}=\\operatorname{ReLU}\\left(\\sum_{k} \\alpha_{k}^{c} A^{k}\\right)$<br><br>\n","<font color=\"silver\">定義</font><br><br>\n","$\\displaystyle L_{\\text {Grad-CAM}}^{c}$：<font color=\"Silver\">クラス$c$に対するAttention map</font><br>\n","$\\alpha^{c}_{k}$：<font color=\"Silver\">逆伝播時のクラス$c$に対する$k$番目の勾配に対する重み</font><br>\n","$Z$：<font color=\"Silver\">勾配の空間方向に対する値の数</font><br>\n","$y^c$：<font color=\"Silver\">クラス$c$に対するスコア</font><br>\n","$A^{k}_{i,j}$：<font color=\"Silver\">$k$番目の特徴マップ（$i$, $j$）の値</font><br><br>\n","$f_{k}(x,y)$：<font color=\"Silver\">k 番目の特徴量マップ</font><br><br>\n","$\\displaystyle \\alpha_k^c = \\overbrace{\\frac{1}{Z}\\sum_{i}\\sum_{j}}^{\\text{global average pooling}} \\underbrace{\\frac{\\delta y^c}{\\delta A_{ij}^k}}_{\\text{gradients via backprop}}$<br><br>"],"metadata":{"id":"4Rf-LY-s_1ve"}},{"cell_type":"markdown","source":["<img src=\"https://axa.biopapyrus.jp/media/cnn-cam-fig1.png\"  width=\"720\"><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://evaluelog.com/post-129/)</font><br>\n","<img src=\"https://evaluelog.com/blog/wp-content/uploads/2021/02/%E5%9B%B35.png\"  width=\"480\"><br><br>\n","model = vgg16(pretrained=True)<br>\n","model.eval()<br>\n","<font color=\"Silver\"># VGGにおいてTabby cat(281)なのにTiger cat(282)と分類した場合<br></font>\n","target_layer = model.features[-1]<br>\n","target_category = 282<br>\n","gradcam = GradCAM(model=model, target_layer=target_layer)<br>\n","grayscale_cam = gradcam(input_tensor=input_tensor, target_category=target_category)<br>\n","grayscale_cam = grayscale_cam[0, :]<br>\n","cam_image = show_cam_on_image(rgb_img, grayscale_cam)<br>\n","cam_image = cv2.cvtColor(cam_image, cv2.COLOR_BGR2RGB)<br>\n","plt.imshow(cam_image, cmap='jet')<br>\n","plt.colorbar()<br>\n","plt.savefig('result.png')<br>\n","<br>\n","- FasterRCNN: model.backbone<br>\n","- Resnet18 and 50: model.layer4[-1]<br>\n","- VGG and densenet161: model.features[-1]<br>"],"metadata":{"id":"e9kAZp2j1Y0d"}}]}