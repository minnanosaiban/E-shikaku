{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/group-nai-shomu/00/blob/main/15%E8%AB%96%E6%96%87.ipynb","timestamp":1682401833742}],"collapsed_sections":["ogtjZDMMHYTa","xXPyDeYrHykq","sRQeK_gzH3gT","cd1YDHM5mRIa","o8iceq4lzir4","8GhWXj9e-8Pe","zoCeCcSh9iui"],"authorship_tag":"ABX9TyMBdaugRo73QqGOXHzCwH1Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"0N5d6UIKNDHV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <font color=\"Blue\">■</font><font color=\"silver\">GoogLeNet [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4855&cid=b0f01606242a6ed3&CT=1666714515497&OR=ItemsView)</font><br>\n","https://arxiv.org/pdf/1409.4842.pdf<br>"],"metadata":{"id":"oztwAt_m-NHR"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GoogLeNet  1 Introduction 背景"],"metadata":{"id":"8h1r-NmqCh6X"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GoogLeNet  2 関連研究"],"metadata":{"id":"gUH0i6fQCqPJ"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GoogLeNet  3 Motivation and High Level Considerations"],"metadata":{"id":"X1u_VquHCqk-"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GoogLeNet  4 Architectural Detailse\n","# Inception Module<br>\n","> ※ 異なるサイズの畳み込みを独立して行っているため、非零のパラメータ数が大きく減る<br>\n","> ※ 小さな畳込みフィルタを並列に並べて近似することで、 表現力とパラメータ数のトレードオフを改善<br>\n","> ※ 1×1の畳み込み層を挿入することで、次元削減を行い、さらにパラメータ数を削減している<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/jun40vn/items/5ac97a6f1d8f82a49194)</font></font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F73d58375-38ca-b8f3-49dd-a5dc2b7ea2cd.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=14759e901deefd4e04e04fca3d6832aa\" width=\"320\">\n","<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://www.researchgate.net/figure/Basic-Inception-v3-structure_fig3_332428603)</font></font>\n","<br>\n","<img src=\"https://www.researchgate.net/publication/332428603/figure/fig3/AS:748134426755072@1555380555179/Basic-Inception-v3-structure.png\" width=\"640\"><br>\n","<br>\n","<img src=\"https://axa.biopapyrus.jp/media/objectclassification_ref_googlenet_02.png\" width=\"640\"><br>\n","\n"],"metadata":{"id":"NN7-CQufCq1H"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GoogLeNet  5 GoogLeNet\n","# Auxiliary Classifier<br>\n","> ※ 学習時は、出力層以外に2つの補助の分類器の出力を重み付き平均をとり、損失を計算する。<br>\n","> ※ ネットワークの中間層に直接誤差を伝搬させることで、勾配消失防止と正則化を実現している。<br>\n","> ※ アンサンブル学習と同様の効果が得られるため、汎化性能の向上が期待できる。<br>\n","> ※ AuxililaryLossを導入しない場合でもBatchNormalizationを加えることにより、同様に学習がうまく進むことがある。<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/jun40vn/items/5ac97a6f1d8f82a49194)</font></font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F8d251aa2-54e4-a886-390b-b5aa87519e8c.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=cd3c041fb8b150ca4daa86514cdfbffb\" width=\"640\"><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F67f98213-a8ff-de52-2d7e-18a3ef37cdd2.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=b114488e13f5781fbb2ebc72856b3a3b\" width=\"640\">\n"],"metadata":{"id":"4xTpBfaTCraP"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GoogLeNet  6 Training Methodology"],"metadata":{"id":"J7sOM60jDQVq"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GoogLeNet  7 ILSVRC 2014 Classification Challenge Setup and Results"],"metadata":{"id":"gLceM0KwDRFI"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GoogLeNet  8 ILSVRC 2014 Detection Challenge Setup and Results"],"metadata":{"id":"GHDPL4qADgsB"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GoogLeNet  9 Conclusions"],"metadata":{"id":"YsU8d7XoDhCP"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GoogLeNet  10 Acknowledgement"],"metadata":{"id":"YWZVFUORDqig"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Inception v3</font><br>\n","<img src=\"https://cloud.google.com/static/tpu/docs/images/inceptionv3onc--oview.png\" width=\"640\">"],"metadata":{"id":"uTYPHAJGKN77"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Inception v3（3×3 Conv）</font><br>\n","# 3×3 Convolution<br>\n","> ※ 3×3 の畳み込み層を複数使用することで、受容野を同じにしつつ、計算量の削減を図る。<br>\n","<img src=\"https://miro.medium.com/max/1100/1*gyc_dcvBf51JLIu986LteQ.png\" width=\"240\">\n","<img src=\"https://pystyle.info/wp/wp-content/uploads/2021/11/pytorch-inceptionv3_01.jpg\" Height=\"160\">\n","<img src=\"https://miro.medium.com/max/1100/1*UvfZWRbPS8d1RGtBuKhIgw.png\" width=\"240\">"],"metadata":{"id":"E34XVFhoKYT6"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Inception v3（1×n Conv, n×1 Conv）\n","# 1×n Convolution, n×1 Convolution<br>\n","> ※ nx1 の畳み込み層と 1xn の畳み込み層を使用することで、受容野を同じにしつつ、計算量の削減を図る<br>\n","<img src=\"https://miro.medium.com/max/1100/1*UvfZWRbPS8d1RGtBuKhIgw.png\" width=\"240\">\n","<img src=\"https://pystyle.info/wp/wp-content/uploads/2021/11/pytorch-inceptionv3_03.jpg\" Height=\"160\">\n","<img src=\"https://miro.medium.com/max/1100/1*QPKnhTEjA4GELiGnLnvi8g.png\" width=\"240\">"],"metadata":{"id":"oP7PjeBkKYnx"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Inception v3（Batch Norm）"],"metadata":{"id":"YNAqTgFTLYwg"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Inception v3（Label Smoothing）"],"metadata":{"id":"sCsoRGZELdj6"}},{"cell_type":"markdown","source":["# <font color=\"Blue\">■</font><font color=\"silver\">ResNet [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4853&cid=b0f01606242a6ed3&CT=1666715412254&OR=ItemsView)</font><br>\n","https://arxiv.org/pdf/1512.03385.pdf<br>\n","[*…*</font>](https://ai-kenkyujo.com/artificial-intelligence/ai-architecture-02/)\n","\n"],"metadata":{"id":"IurcoaAvpe9P"}},{"cell_type":"markdown","source":["# <font color=\"silver\">ResNet  1 Introduction 背景\n","><img src=\"https://raw.githubusercontent.com/rohan-varma/resnet-implementation/master/images/verydeep_network.png\" width=\"640\">"],"metadata":{"id":"fknIUpehlVWN"}},{"cell_type":"markdown","source":["# <font color=\"silver\">ResNet  2 関連研究"],"metadata":{"id":"79qKCyHqlV3p"}},{"cell_type":"markdown","source":["# <font color=\"silver\">ResNet  3 Deep Residual\n","# Residual Learning<br>Identity Mapping by Shortcuts\n","> ※  勾配消失を抑え、「層を深くすると学習できない」という劣化問題を解決した。<br>\n","> ※  バイパスすることによって入力層に近い層にも誤差が伝わるため、勾配消失を防ぐ。<br>\n","> ※  色々なバイパスの組み合わせが存在することになりアンサンブル効果がある。<br>\n","> ※  入力をショートカットして足すだけなので、計算コストはほとんど増えずに、実装も容易。<br>\n","> ※  𝐹(𝑥) は、入力𝑥と出力 H(𝑥) の残差 H𝑥−𝑥 を学習している<br>\n","> ※  ブロックへの入力にこれ以上の変換が必要ない場合は重みが0となり、小さな変換が求められる場合は対応する小さな変動をより見つけやすくなる。<br> <br>\n","<font color=\"black\">$H(x)=F(x)+x$\n","<br>\n","<img src=\"https://raw.githubusercontent.com/rohan-varma/resnet-implementation/master/images/residual_learning_block.png\" width=\"480\"><br><br>\n","# Network Architectures\n","><img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20200424011138/ResNet.PNG\" width=\"640\">\n","# <font color=\"silver\">ResNet  3.4. Implementation"],"metadata":{"id":"93jQa6oNlV__"}},{"cell_type":"markdown","source":["# <font color=\"silver\">ResNet  4 実験\n","# Bottleneck Block\n","> ※ 1×1 Convで次元削減、3×3 Conv、1×1 Convで次元を復元するという形<br>\n","> ※ 同等の計算量を保ちながら、より深いモデルを構築することが出来る。<br>\n","> ※ 左図のパラメーター数：3×3×64×64、3×3×64×64≒70k<br>\n","> ※ 右図のパラメーター数：256×64、64×3×3×64、64×256≒70k（ボトルネック）<br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/s/shikiponn/20190603/20190603173844.png\" width=\"480\"><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F38555%2F1557d5bd-50bc-a65e-cf9a-dad10649a6c0.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=21774d16a1f02b2759fa9fb941212d13\" width=\"640\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/be48afbdd9da19f1e43e)<br></font>\n","<img src=\"https://camo.qiitausercontent.com/310591eea55adba318520a682e19baac8ab64d19/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3331353036332f36303336393736332d643734382d386334382d396262322d3734616564623262663263392e706e67\" width=\"480\">"],"metadata":{"id":"lQSswzq8lsMa"}},{"cell_type":"markdown","source":["# <font color=\"Blue\">■</font><font color=\"silver\">WideResNet [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4863&cid=b0f01606242a6ed3&CT=1666720554388&OR=ItemsView)\n","※ ResNetが残差接続によって層を深くしたのに対し、 WideResNet は残差ブロック内の畳み込み層の出力チャンネル数を増やし、層を浅くしたモデル。<br></font>\n","<br>\n","<font color=\"Black\">depth：<font color=\"silver\">全体のdepth（畳み込みの深さ、畳み込みの数）<br>\n","<font color=\"Black\">l：<font color=\"silver\">ブロック内のdepth（畳み込みの深さ、畳み込みの数）<br>\n","<font color=\"Black\">k：<font color=\"silver\">ブロック内のwidth（畳み込みの広さ）特徴平面の数（チャネル数）を k 倍することを意味する。<br>\n","<font color=\"Black\">B(M)：<font color=\"silver\"> Residial Block（残差ブロック）<br>\n","<font color=\"Black\">M：<font color=\"silver\">ブロック内の畳み込み層のカーネルサイズのリスト<br>\n","<font color=\"Black\">WRN-n-k：<font color=\"silver\"> n層の畳み込みをもち、幅k をもつ Wide Residual Networks<br>"],"metadata":{"id":"FqQ40TgEGGAh"}},{"cell_type":"markdown","source":["# <font color=\"silver\">WideResNet  1 Introduction 背景 <br>\n","# <font color=\"silver\"> 図1<br>\n","<img src=\"https://norman3.github.io/papers/images/wrn/f01.png\" width=\"640\"><br>"],"metadata":{"id":"R-hfUIbhHek9"}},{"cell_type":"markdown","source":["# <font color=\"silver\">WideResNet  2 Wide residual networks<br>\n","# <font color=\"silver\">式１<br>\n","${\\bf x}_{l+1} = {\\bf x}_{l} + F({\\bf x}_{l}, {W_l})$<br>\n","# <font color=\"silver\">表1 Baseline Model<br>\n","<img src=\"https://norman3.github.io/papers/images/wrn/f02.png\" width=\"640\"><br>\n","# <font color=\"silver\">（参考）\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F38555%2F1557d5bd-50bc-a65e-cf9a-dad10649a6c0.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=21774d16a1f02b2759fa9fb941212d13\" width=\"640\"><br>\n"],"metadata":{"id":"_apKKPVgHeve"}},{"cell_type":"markdown","source":["# <font color=\"silver\">WideResNet  2.1 Type of convolutions in residual block<br>\n","# <font color=\"silver\">表2 Block Type\n","Residual Block内の畳み込み層の組み合わせを検証<br>\n","チャンネル数K＝2倍のときの性能比較では、B(3, 3)の性能が最も良い。<br>\n","<br>\n","<img src=\"https://norman3.github.io/papers/images/wrn/f03.png\" width=\"400\"><br>\n","<font color=\"black\">\n","B(1, 3, 1): 1×1 -> 3×3 -> 1×1。オリジナルの Bottleneck Block<br>\n","B(3, 1): 3×3 -> 1×1<br>\n","B(1, 3): 1×1 -> 3×3<br>\n","B(3, 1, 1): 3×3 -> 1×1 -> 1×1。Network In Network と同じ構造。<br>\n","B(3, 3): 3×3 -> 3×3。オリジナルの Building Block<br>\n","B(3, 1, 3): 3×3 -> 1×1 -> 3×3<br>"],"metadata":{"id":"37gPeOxeHe4r"}},{"cell_type":"markdown","source":["# <font color=\"silver\">WideResNet  2.2 Number of convolutional layers per residual block<br>\n","# <font color=\"silver\">表3 Layers per Block\n","ネットワーク全体の層数を40として、1 ブロック内の畳み込み層の数$\\,l\\,$を変化させて検証<br>\n","層の数$\\,l=1\\,$では表現力が足りず、$\\,l=3\\,$以上のときは残差接続が減少したため最適化が難しくなった結果といえる。<br><br>\n","<img src=\"https://norman3.github.io/papers/images/wrn/f04.png\" width=\"200\"><br>\n","l=1はB(3)<br>\n","l=2はB(3, 3)<br>\n","l=3はB(3, 3, 3)<br>\n","l=4はB(3, 3, 3, 3)<br>"],"metadata":{"id":"Eaqd3QeyHfEJ"}},{"cell_type":"markdown","source":["# <font color=\"silver\">WideResNet  2.3 Width of residual blocks<br>\n","\n","\n"],"metadata":{"id":"KNkAbuGaHuhQ"}},{"cell_type":"markdown","source":["# <font color=\"silver\">WideResNet  2.4 Dropout in residual blocks<br>"],"metadata":{"id":"vbMEMecdHx7u"}},{"cell_type":"markdown","source":["# <font color=\"silver\">WideResNet  3 Experimental results<br>\n","# <font color=\"silver\">表４\n","depth（全体の層の深さ）と k（幅の広さ）の組み合わせの結果<br>\n","<br>\n","<img src=\"https://norman3.github.io/papers/images/wrn/f05.png\" width=\"640\"><br>\n","# <font color=\"silver\">表５<br>\n","他のモデルとの性能比較の結果<br><br>\n","<img src=\"https://norman3.github.io/papers/images/wrn/f06.png\" width=\"640\"><br>\n","# <font color=\"silver\">図２<br>\n","ResNetとWideResNetとの比較<br><br>\n","<img src=\"https://norman3.github.io/papers/images/wrn/f07.png\" width=\"640\"><br>\n","# <font color=\"silver\">表6 Dropout\n","ドロップアウト有と無の性能比較<br><br>\n","<img src=\"https://norman3.github.io/papers/images/wrn/f08.png\" width=\"640\"><br>\n","# <font color=\"silver\">図3 Dropout\n","ResNet と WideResNet の比較、WideResNet のドロップアウト有と無の比較<br>\n","Dropout なしの場合は、学習率を下げると過剰適合により急激に訓練損失が下がる。<br><br>\n","<img src=\"https://norman3.github.io/papers/images/wrn/f09.png\" width=\"640\"><br>\n","# <font color=\"silver\">表7\n","WideResNet の幅や深さを変更したときの性能比較<br><br>\n","<img src=\"https://norman3.github.io/papers/images/wrn/f10.png\" width=\"640\"><br>\n","# <font color=\"silver\">表8\n","ResNetとWideResNetとの比較<br><br>\n","<img src=\"https://norman3.github.io/papers/images/wrn/f11.png\" width=\"640\"><br>\n","# <font color=\"silver\">表9\n","各データセットに対する WideResNet の結果<br><br>\n","<img src=\"https://norman3.github.io/papers/images/wrn/f12.png\" width=\"640\"><br>\n","# <font color=\"silver\">図4\n","オリジナルのResNetとの精度と計算時間の比較<br>\n","<img src=\"https://norman3.github.io/papers/images/wrn/f13.png\" width=\"240\"><br>"],"metadata":{"id":"_QJvx_Q-HyJ1"}},{"cell_type":"markdown","source":["# <font color=\"silver\">WideResNet  4 Conclusions<br>"],"metadata":{"id":"eiHN90dKHfND"}},{"cell_type":"markdown","source":["# <font color=\"silver\">WideResNet  5 Acknowledgements <br>"],"metadata":{"id":"cptGEm8SH37F"}},{"cell_type":"markdown","source":["# <font color=\"Blue\">■</font><font color=\"silver\">DenseNet [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4861&cid=b0f01606242a6ed3&CT=1666715758082&OR=ItemsView)</font><br>\n","※ Denseなスキップ接続群により，ResNetよりも勾配消失問題を緩和した。<br>\n","※ Denseブロック内で層間が全てスキップ接続されているため、特徴マップの伝搬を強化している。<br>\n","※ 特徴量を再利用する結合になっているため、パラメータ数を少なくすることができる。<br>\n","※ 実際には、次元削減のためBottleneckを使用。<br>\n","※ Grow Rateとは、Dense Blockの中で増やすフィルターの数で、k=32に設定されることが多い\n","<br><br>\n","<font color=\"black\">$xl = Hl(x0, x1, …, xl-1)$\n","<br><br>\n","$xl$：<font color=\"silver\">第l層の出力</font><br>\n","$Hl$：<font color=\"silver\">Batch normalization、ReLU、3×3 Convolution</font>\n","<br><br>\n","<img src=\"https://ai-kenkyujo.com/wp-content/uploads/2021/12/ai-img2-20.png.webp\" width=\"640\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://techplay.jp/event/846663)</font><br>\n","<img src=\"https://i.gyazo.com/8cae5dd02904890ca66f11470603cfd3.png\" width=\"640\">\n","<br><br>\n","※ Dense Blockで大きくなったチャンネル数を圧縮するダウンサンプリングの役割を持つレイヤー<br>\n","※ 1×1 Convolution、2×2 Average Poolingで構成されている<br>\n","※ ダウンサンプリングを行う層は重要な要素になるため、Dense Block間にPooling層を導入した。<br>\n","<img src=\"https://ai-kenkyujo.com/wp-content/uploads/2021/12/ai-img2-21.png.webp\" width=\"640\">"],"metadata":{"id":"ogtjZDMMHYTa"}},{"cell_type":"markdown","source":["# <font color=\"silver\">DenseNet  図1 Dense Block\n","<font color=\"black\">$xl = Hl(x0, x1, …, xl-1)$\n","<br><br>\n","$xl$：<font color=\"silver\">第l層の出力</font><br>\n","$Hl$：<font color=\"silver\">Batch normalization、ReLU、3×3 Convolution</font>\n","<br><br>\n","<img src=\"https://ai-kenkyujo.com/wp-content/uploads/2021/12/ai-img2-20.png.webp\" width=\"640\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://techplay.jp/event/846663)</font><br>\n","<img src=\"https://i.gyazo.com/8cae5dd02904890ca66f11470603cfd3.png\" width=\"640\"><br>\n","<img src=\"https://velog.velcdn.com/images/skhim520/post/abd9ef85-e295-43d6-8e3e-929308129b88/KakaoTalk_20210506_175258356.jpg\" width=\"640\"><br>\n"],"metadata":{"id":"xXPyDeYrHykq"}},{"cell_type":"markdown","source":["# <font color=\"silver\">DenseNet  図2 Transition Layer\n","※ Dense Blockで大きくなったチャンネル数を圧縮するダウンサンプリングの役割を持つレイヤー<br>\n","※ 1×1 Convolution、2×2 Average Poolingで構成されている<br>\n","※ ダウンサンプリングを行う層は重要な要素になるため、Dense Block間にPooling層を導入した。<br>\n","<img src=\"https://ai-kenkyujo.com/wp-content/uploads/2021/12/ai-img2-21.png.webp\" width=\"640\">\n","<img src=\"https://images.velog.io/images/skhim520/post/9e72346d-ef5e-4a42-95f3-1c0e2feb235d/image.png\" width=\"640\"><br>\n","\n"],"metadata":{"id":"sRQeK_gzH3gT"}},{"cell_type":"markdown","source":["# <font color=\"silver\">DenseNet  表1 Architecture\n","<img src=\"https://arthurdouillard.com/figures/densenet_archi.png\" width=\"640\"><br>"],"metadata":{"id":"cd1YDHM5mRIa"}},{"cell_type":"markdown","source":["# <font color=\"Blue\">■</font><font color=\"silver\">MobileNet v1 [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4869&cid=b0f01606242a6ed3&CT=1666816954089&OR=ItemsView)</font>\n"],"metadata":{"id":"wftD_3BVdEoj"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v1  1. Introduction\n","# <font color=\"silver\">図１\n","<img src=\"https://greeksharifa.github.io/public/img/2022-02-01-MobileNetV1/fig01.png\" width=\"640\"><br>"],"metadata":{"id":"0hjfe9kgMCU6"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v1  2. Prior Work"],"metadata":{"id":"O3C9S_A6MTvL"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v1  3. MobileNet Architecture"],"metadata":{"id":"gX5_-BMtMURD"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v1  3.1. Depthwise Separable Convolution\n","<img src=\"https://greeksharifa.github.io/public/img/2022-02-01-MobileNetV1/fig02.png\" width=\"640\">\n","# <font color=\"silver\">図２\n","<br><br>\n","<font color=\"black\">${\\cfrac{D_{K} · D_{K} · M · D_{F} · D_{F} ＋M · N · D_{F} · D_{F} }{D_{K} · D_{K} · M · N · D_{F} · D_{F} } = \\cfrac{1}{N} + \\cfrac{1}{D_{K}^2}}$\n","<br><br>\n","<font color=\"black\">$M$：<font color=\"silver\">入力チャネル数</font><br>\n","<font color=\"black\">$N$：<font color=\"silver\">出力チャネル数</font><br>\n","<font color=\"black\">$D_{K}×D_{K}$：<font color=\"silver\">カーネルサイズ</font><br>\n","<font color=\"black\">$D_{F}×D_{F}$：<font color=\"silver\">特徴マップサイズ</font>\n","<br><br>\n","${\\rm Depthwise} :c_{in}･k^2$\n","<br>\n","${\\rm Pointwise} :c_{in}･c_{out}$\n","<br>\n","$c_{in}･k^2 + c_{in}･c_{out} = c_{in}(c_{out}+k^2)$\n","<br><br>\n","$\\cfrac{c_{in}(c_{out}+k^2)}{c_{in}･c_{out}･k^2} ＝\\cfrac{c_{out}＋k^2}{c_{out}･k^2} ＝\\cfrac{1}{k^2} ＋\\cfrac{1}{c_{out}}$\n","<br><br>\n","\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://muscle-programmer.hatenablog.com/entry/2018/06/07/190221)</font></font><br>\n","<img src=\"https://qiita-image-store.s3.amazonaws.com/0/108729/5e4bb20f-127e-4d9e-10fb-110ba4694360.png\" width=\"480\">\n","<br><br>\n","<img src=\"https://qiita-image-store.s3.amazonaws.com/0/108729/72ca6fe6-f6a0-7dd3-3b24-7aa3aa185ab6.png\" width=\"480\">\n","<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://techplay.jp/event/846663)</font></font><br>\n","<img src=\"https://i.gyazo.com/fa9826d7d41fee8f3910fe0916c1f595.png\" width=\"640\">\n","<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://www.mdpi.com/2072-4292/13/16/3211/htm)</font></font><br>\n","<img src=\"https://www.mdpi.com/remotesensing/remotesensing-13-03211/article_deploy/html/images/remotesensing-13-03211-g002-550.jpg\" width=\"640\">"],"metadata":{"id":"gFZbB1wdMUt7"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v1  3.2. Network Structure and Training\n","# <font color=\"silver\"> 図３\n","<img src=\"https://greeksharifa.github.io/public/img/2022-02-01-MobileNetV1/fig03.png\" width=\"320\"><br>\n","# <font color=\"silver\"> 表１\n","<img src=\"https://greeksharifa.github.io/public/img/2022-02-01-MobileNetV1/tab01.png\" width=\"480\"><br>\n","# <font color=\"silver\"> 表２\n","<img src=\"https://greeksharifa.github.io/public/img/2022-02-01-MobileNetV1/tab02.png\" width=\"480\"><br>"],"metadata":{"id":"72wOiHlvMVId"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v1  3.3. Width Multiplier: Thinner Models\n","※ ネットワーク内のチャネル数を調整するハイパーパラメータ<br>\n","※ 各レイヤでネットワークを一様に薄くする役割を持つ<br>\n","※ 計算コストとパラメータ数を約$α$の二乗の二次関数的に削減する<br><br>\n","<font color=\"black\">$D_{K} · D_{K} · αM · \\rho D_{F} · \\rho D_{F} ＋αM · αN · \\rho D_{F} · \\rho D_{F} $\n","<br><br>\n","<font color=\"black\">$M$：<font color=\"silver\">入力チャネル数</font><br>\n","<font color=\"black\">$N$：<font color=\"silver\">出力チャネル数</font><br>\n","<font color=\"black\">$D_{K}×D_{K}$：<font color=\"silver\">カーネルサイズ</font><br>\n","<font color=\"black\">$D_{F}×D_{F}$：<font color=\"silver\">特徴マップサイズ</font>\n","<br><br>\n","<img src=\"https://greeksharifa.github.io/public/img/2022-02-01-MobileNetV1/tab06.png\" width=\"480\"><br>"],"metadata":{"id":"FO43yCq-MVi8"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v1  3.4. Resolution Multiplier: Reduced Representation\n","※ 力画像やネットワーク内での中間表現の解像度を調整するハイパーパラメータ<br>\n","※ 入力画像に適用すると、各層の内部表現が同じ乗数で削減される。<br>\n","※ 各層のチャネル数 / 解像度を小さくし、精度は下がってしまうものの、計算コストを削減<br>\n","<br>\n","<font color=\"black\">$D_{K} · D_{K} · αM · \\rho D_{F} · \\rho D_{F} ＋αM · αN · \\rho D_{F} · \\rho D_{F} $\n","<br><br>\n","<font color=\"black\">$M$：<font color=\"silver\">入力チャネル数</font><br>\n","<font color=\"black\">$N$：<font color=\"silver\">出力チャネル数</font><br>\n","<font color=\"black\">$D_{K}×D_{K}$：<font color=\"silver\">カーネルサイズ</font><br>\n","<font color=\"black\">$D_{F}×D_{F}$：<font color=\"silver\">特徴マップサイズ</font>\n","<br><br>\n","<img src=\"https://greeksharifa.github.io/public/img/2022-02-01-MobileNetV1/tab06.png\" width=\"480\"><br>"],"metadata":{"id":"eo1sp-QtMXGC"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v1  4. Experiments\n","<img src=\"https://greeksharifa.github.io/public/img/2022-02-01-MobileNetV1/tab03.png\" width=\"480\"><br>\n","<img src=\"https://greeksharifa.github.io/public/img/2022-02-01-MobileNetV1/tab04.png\" width=\"480\"><br>\n","<img src=\"https://greeksharifa.github.io/public/img/2022-02-01-MobileNetV1/tab04.png\" width=\"480\"><br>\n","<img src=\"https://greeksharifa.github.io/public/img/2022-02-01-MobileNetV1/tab06.png\" width=\"480\"><br>\n","<img src=\"https://greeksharifa.github.io/public/img/2022-02-01-MobileNetV1/fig04.png\" width=\"320\"><br>\n","<img src=\"https://greeksharifa.github.io/public/img/2022-02-01-MobileNetV1/fig05.png\" width=\"320\"><br>\n","<img src=\"https://greeksharifa.github.io/public/img/2022-02-01-MobileNetV1/tab08.png\" width=\"480\"><br>\n","<img src=\"https://greeksharifa.github.io/public/img/2022-02-01-MobileNetV1/tab08.png\" width=\"480\"><br>\n","<img src=\"https://greeksharifa.github.io/public/img/2022-02-01-MobileNetV1/tab12.png\" width=\"480\"><br>\n","<img src=\"https://greeksharifa.github.io/public/img/2022-02-01-MobileNetV1/tab13.png\" width=\"480\"><br>\n","<img src=\"https://greeksharifa.github.io/public/img/2022-02-01-MobileNetV1/tab14.png\" width=\"480\"><br>"],"metadata":{"id":"BGiOUokeMoXh"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v1  5. Conclusion"],"metadata":{"id":"FfCpwq00MqCA"}},{"cell_type":"markdown","source":["# <font color=\"Blue\">■</font><font color=\"silver\">MobileNet v2 [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4871&cid=b0f01606242a6ed3&CT=1666818057888&OR=ItemsView)</font><br>\n","<img src=\"https://miro.medium.com/max/1100/1*bqE59FvgpvoAQUMQ0WEoUA.png\" width=\"480\">"],"metadata":{"id":"o8iceq4lzir4"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v2  1. Introduction"],"metadata":{"id":"WiSHfISQTZdo"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v2  2. Related Work"],"metadata":{"id":"b-OBzKwbTeJQ"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v2  3. Preliminaries, discussion and intuition"],"metadata":{"id":"-cWJZGunTeta"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v2  3.1. Depthwise Separable Convolutions\n","<img src=\"https://camo.githubusercontent.com/f456646d6aec8b4c677d568c7d00844bf9c93cbf86f7872f0a8f69915968404f/68747470733a2f2f692e696d6775722e636f6d2f4a62386a3373682e706e67\" width=\"480\"><br>"],"metadata":{"id":"jnFHJ9cQTfLp"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v2  3.2. Linear Bottlenecks"],"metadata":{"id":"XlV9FU-bTfl6"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v2  3.3. Inverted residuals\n","$\\rm{ReLU}6(x) = \\min(\\max(x,0),6)$\n","<br><br>\n","<img src=\"https://camo.githubusercontent.com/2c0209ee2e2da079a17428c6d96e945a6c362116f1883ca6af5ea63ea5125c7d/68747470733a2f2f692e696d6775722e636f6d2f6b554a753635522e706e67\" width=\"480\">\n","<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://www.researchgate.net/figure/Residual-block-12-36-and-inverted-residual-block_fig2_358518820)</font></font><br>\n","<img src=\"https://www.researchgate.net/publication/358518820/figure/fig2/AS:1157628617072649@1653011578181/Residual-block-12-36-and-inverted-residual-block.png\" width=\"400\">"],"metadata":{"id":"cs4Xwp_LTf_a"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v2  3.4. Information flow interpretation"],"metadata":{"id":"MdqHf0rTTg0k"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v2  4. Model Architecture"],"metadata":{"id":"CDbapiaBThOy"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v2  5. Implementation Notes"],"metadata":{"id":"WJUW0LjQTz-m"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v2  5.1. Memory efficient inference"],"metadata":{"id":"sYaBH-NNT0bd"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v2  6. Experiments"],"metadata":{"id":"OBHT8fIpT02E"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v2  7. Conclusions and future work"],"metadata":{"id":"2bucsEBrT9vM"}},{"cell_type":"markdown","source":["# <font color=\"Blue\">■</font><font color=\"silver\">MobileNet v3 [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4871&cid=b0f01606242a6ed3&CT=1666818057888&OR=ItemsView)</font><br>\n","\n","NetAdapt</font><br>\n","<img src=\"https://miro.medium.com/max/1100/1*pPnym46xkAH5b3VaipEp7Q.png\" width=\"640\">"],"metadata":{"id":"8GhWXj9e-8Pe"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v3  1. Introduction"],"metadata":{"id":"4Vnh9cnmV8vM"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v3  2. Related Work"],"metadata":{"id":"u1LbtTyIWJ_A"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v3  3. Efficient Mobile Building Blocks\n","<img src=\"https://miro.medium.com/max/786/1*UzDT6jEDJYKGkHB2nAejUw.png\" width=\"480\">"],"metadata":{"id":"oN4_n79sWKaD"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v3  4. Network Search"],"metadata":{"id":"bGD0U2QDWLWa"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v3  4.1. Platform-Aware NAS for Block-wise Search"],"metadata":{"id":"eeYiPabwWLwk"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v3  4.2. NetAdapt for Layer-wise Search"],"metadata":{"id":"dGiwazIYWMPT"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v3  5. Network Improvements"],"metadata":{"id":"EU2lOgp9WMnl"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v3  5.1. Redesigning Expensive Layers\n","<img src=\"https://miro.medium.com/max/828/1*sV45zudDSKlqnlftIXc09w.png\" width=\"640\">"],"metadata":{"id":"pcQtAxNZWNAF"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v3  5.2. Nonlinearities\n","$h-{\\rm swish}[x] = x\\cfrac{{\\rm ReLU}6(x+3)}{6}$\n","<br>\n","<img src=\"https://miro.medium.com/max/828/1*fL0LcL4ZC3i7mbfpuaAQiw.png\" width=\"480\">"],"metadata":{"id":"5T99RFBsWNZM"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v3  5.3. Large squeeze-and-excite\n","\n"],"metadata":{"id":"5OWvnoi9WN3U"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v3  5.4. MobileNetV3 Definitions"],"metadata":{"id":"ovBOak4GWvY_"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v3  6. Experiments\n","<img src=\"https://combustion.readthedocs.io/en/v0.1.0rc1/_images/raspp.png\" width=\"800\">"],"metadata":{"id":"ldr7hrA5Wvzu"}},{"cell_type":"markdown","source":["# <font color=\"silver\">MobileNet v3  7. Conclusions and future work"],"metadata":{"id":"AE0OEmDWWwUh"}},{"cell_type":"markdown","source":["# <font color=\"Blue\">■</font><font color=\"silver\">EfficientNet [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4875&cid=b0f01606242a6ed3&CT=1666816283914&OR=ItemsView)"],"metadata":{"id":"DNW1gt0ijGuT"}},{"cell_type":"markdown","source":["# <font color=\"silver\">EfficientNet  1. Introduction\n","# <font color=\"silver\"> 図1\n","モデルの種類・パラメータ数・正解率のグラフ<br>\n","<font color=\"black\">F1. Model Size vs. ImageNet Accuracy.<br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/nuka137/20200908/20200908141057.png\" width=\"480\"><br>\n","# <font color=\"silver\"> 図2\n","<font color=\"black\">F2. Model Scaling.<br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/nuka137/20200904/20200904095539.png\" width=\"800\"><br>"],"metadata":{"id":"YvK_YtQ91uGB"}},{"cell_type":"markdown","source":["# <font color=\"silver\">EfficientNet  2. Related Work"],"metadata":{"id":"9-SSPbay1vPq"}},{"cell_type":"markdown","source":["# <font color=\"silver\">EfficientNet  3. Compound Model Scaling"],"metadata":{"id":"wTX5caG01v5T"}},{"cell_type":"markdown","source":["# <font color=\"silver\">EfficientNet  3.1. Problem Formulation\n","# <font color=\"silver\">式1\n","$\\displaystyle \\mathcal{N}=\\bigodot_{i=1,...,s}\\mathcal{F}_i^{L_i}\\left(X_{<H_i, W_i, C_i>}\\right)$\n","# <font color=\"silver\">式2\n","ネットワークの正解率を最大にする$d,w,r$をバランスをとってスケーリングする<br><br>\n","<font color=\"black\">$\\displaystyle \\max_{d, w, r} \\quad Accuracy\\left(\\mathcal{N}(d, w, r)\\right)$<br>\n","$\\displaystyle \\mathcal{N}(d, w, r) = \\bigodot_{i=1,...s}\\hat{\\mathcal{F}}_i^{d \\cdot \\hat{L}_i}\\left(X_{<r \\cdot \\hat{H}_i, r \\cdot \\hat{W}_i, w \\cdot \\hat{C}_i>}\\right)$<br>\n","$\\displaystyle \\rm Memory(\\mathcal{N}) \\le target\\_memory$<br>\n","$\\displaystyle \\rm FLOPS(\\mathcal{N}) \\le target\\_flops$<br><br>\n","<font color=\"black\">$\\mathcal{N}$：<font color=\"silver\">Baseline Modelのネットワーク<br>\n","<font color=\"black\">$α, β ,γ$：<font color=\"silver\">Baseline Modelにおける層数、幅、解像度のスケーリング係数<br>\n","<font color=\"black\">$\\displaystyle\\bigodot_{i=1...s}$：<font color=\"silver\">モデル$\\mathcal{N}$が$s$個のステージ(ブロック)で成り立っている<br>\n","<font color=\"black\">$\\rm FLOPS$：<font color=\"silver\">1秒あたりにできる浮動小数点演算の回数（FLoating-point Operations Per Second）<br><br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/nuka137/20200908/20200908105522.png\" width=\"400\"><br>"],"metadata":{"id":"E1ggqpzX1wZK"}},{"cell_type":"markdown","source":["# <font color=\"silver\">EfficientNet  3.2. Scaling Dimensions\n","# <font color=\"silver\">図3\n","パラメータ増加に伴って精度向上が鈍化する<br>\n","<font color=\"black\">F3. Scaling Up a Baseline Model with Different Network Width (w), Depth (d), and Resolution (r) Coefficients.<br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/nuka137/20200904/20200904134009.png\" width=\"800\"><br>\n","<font color=\"black\">層数 d<br>\n","<font color=\"black\">層を増やすことで、より複雑な特徴量を捉えることができて汎化性能の向上につながる。一方で、勾配消失問題が発生しやすくなる。勾配消失問題を解消するため、ResNetやBatch Normlizationなどの手法がある。<br>\n","<font color=\"black\">幅 w<br>\n","<font color=\"black\">ネットワークの幅を広げることで、より細かい特徴を捉えることができて、学習がうまくいくようになる。一方で、ネットワークの幅が広くてネットワークの層数が少ない場合は、複雑な特徴を捉えきれないという問題がある。<br>\n","<font color=\"black\">解像度 r<br>\n","<font color=\"black\">入力画像の解像度を高めることで、より細かい特徴を捉えることができる。<br>"],"metadata":{"id":"hEpKRX501w1J"}},{"cell_type":"markdown","source":["# <font color=\"silver\">EfficientNet  3.3. Compound Scaling\n","# <font color=\"silver\">図4\n","パラメータを1つだけスケールアップするよりも、バランスよくパラメータを調整してスケールアップしたほうが精度がよい。<br>\n","<font color=\"black\">F4. Scaling Network Width for Different Baseline Networks.<br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/nuka137/20200904/20200904135046.png\" width=\"400\"><br>\n","# <font color=\"silver\">式3\n","パラメータを程よい具合にスケールアップさせるための式を提案した。 これを、Compound Model Scalingと呼ぶ。<br><br>\n","<font color=\"black\">${\\rm depth}: d=\\alpha^{\\phi}$<br>\n","<font color=\"black\">${\\rm width}: w=\\beta^{\\phi}$<br>\n","<font color=\"black\">${\\rm resolution}: r=\\gamma^{\\phi}$<br>\n","$\\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\simeq 2 \\qquad (\\alpha \\ge 1, \\beta \\ge 1, \\gamma \\ge 1)$<br>\n","<br>\n","<font color=\"black\">$\\phi$：<font color=\"silver\">利用できる計算資源の上限に基づいて決定されるべきハイパーパラメータ<br>\n","<font color=\"black\">$α, β ,γ$：<font color=\"silver\">Baseline Modelにおける層数、幅、解像度のスケーリング係数<br>\n","<font color=\"black\">$\\mathcal{F}, \\hat{L_i}, \\hat{H_i}, \\hat{W_i}, \\hat{C_i}$：<font color=\"silver\">H1を参考<br>\n","<br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/nuka137/20200908/20200908105552.png\" width=\"400\">\n"],"metadata":{"id":"m5YXY3OO1xRR"}},{"cell_type":"markdown","source":["# <font color=\"silver\">EfficientNet  4. EfficientNet Architecture\n","# <font color=\"silver\">表1\n","<font color=\"black\">H1. EfficientNet-B0 baseline network<br>\n","<img src=\"https://camo.qiitausercontent.com/8002ee099889ab5f0378ff78e8cd7113c0c6d4a5/68747470733a2f2f696d6775722e636f6d2f4c38524231314a2e706e67\" width=\"400\"><br>"],"metadata":{"id":"RhaKpR0S1xyC"}},{"cell_type":"markdown","source":["# <font color=\"silver\">EfficientNet  5. Experiment\n","# <font color=\"silver\">表2 B0‐B7\n","<font color=\"black\">H2. EfficientNet Performance Results on ImageNet (Russakovsky et al., 2015).<br>\n","<img src=\"https://norman3.github.io/papers/images/efficient_net/t02.png\" width=\"800\"><br>\n","# <font color=\"silver\">表3\n","<font color=\"black\">H3. Scaling Up MobileNets and ResNet.<br>\n","<img src=\"https://norman3.github.io/papers/images/efficient_net/t03.png\" width=\"400\"><br>\n","# <font color=\"silver\">図5\n","<font color=\"black\">F5. FLOPS vs. ImageNet Accuracy<br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/nuka137/20200908/20200908141014.png\" width=\"400\"><br>\n","# <font color=\"silver\">表4\n","<font color=\"black\">H4. Inference Latency Comparison<br>\n","<img src=\"https://norman3.github.io/papers/images/efficient_net/t04.png\" width=\"400\"><br>\n","# <font color=\"silver\">表5\n","<font color=\"black\">H5. EfficientNet Performance Results on Transfer Learning Datasets.<br>\n","<img src=\"https://norman3.github.io/papers/images/efficient_net/t05.png\" width=\"800\"><br>\n","# <font color=\"silver\">図6\n","<font color=\"black\">F6. Model Parameters vs. Transfer Learning Accuracy<br>\n","<img src=\"https://norman3.github.io/papers/images/efficient_net/f06.png\" width=\"800\"><br>\n","# <font color=\"silver\">図7\n","<font color=\"black\">F7. Class Activation Map (CAM) (Zhou et al., 2016) for Models with different scaling methods<br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/nuka137/20200908/20200908142419.png\" width=\"800\"><br>\n","# <font color=\"silver\">表6\n","<font color=\"black\">H6. Transfer Learning Datasets.<br>\n","<img src=\"https://norman3.github.io/papers/images/efficient_net/t06.png\" width=\"400\"><br>\n","# <font color=\"silver\">図8\n","<font color=\"black\">F8. Scaling Up EfficientNet-B0 with Different Methods.<br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/nuka137/20200908/20200908143507.png\" width=\"400\"><br>\n","# <font color=\"silver\">表7\n","<font color=\"black\">H7. Scaled Models Used in F7.<br>\n","<img src=\"https://norman3.github.io/papers/images/efficient_net/t07.png\" width=\"400\"><br>\n"],"metadata":{"id":"XP8lBMwt1yOJ"}},{"cell_type":"markdown","source":["# <font color=\"silver\">EfficientNet  6. Discussion"],"metadata":{"id":"eY9wRodp2Ufy"}},{"cell_type":"markdown","source":["# <font color=\"silver\">EfficientNet  7. Conclusion"],"metadata":{"id":"NtRFCeyA2foD"}},{"cell_type":"markdown","source":["# <font color=\"Blue\">■</font><font color=\"silver\">R-CNN family [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4894&cid=b0f01606242a6ed3&CT=1666717278455&OR=ItemsView)</font><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/)</font></font><br>\n","<img src=\"https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/rcnn-family-summary.png\" width=\"640\"><br>\n"],"metadata":{"id":"5A2sKu46NVnD"}},{"cell_type":"markdown","source":["# <font color=\"silver\">R-CNN（Region Proposals）\n","<font color=\"silver\">Region Proposals, 領域候補<br>"],"metadata":{"id":"sQ1CzRbwOAgC"}},{"cell_type":"markdown","source":["# <font color=\"silver\">R-CNN（Selective Search）"],"metadata":{"id":"qIUHh9NsONwj"}},{"cell_type":"markdown","source":["# <font color=\"silver\">R-CNN（Wrap）"],"metadata":{"id":"goWxivVqOXI6"}},{"cell_type":"markdown","source":["# <font color=\"silver\">R-CNN（SVM）"],"metadata":{"id":"buy3ITuWOiwq"}},{"cell_type":"markdown","source":["# <font color=\"silver\">SPP</font>\n","<font color=\"silver\">SPP, Spatial Pyramid Pooling, 空間ピラミッドプーリング</font>\n","<br><br>\n","<img src=\"https://axa.biopapyrus.jp/media/sppnet-desc03.png\" width=\"320\">\n","<br><br>\n","<img src=\"https://axa.biopapyrus.jp/media/sppnet-desc01.png\" width=\"320\">"],"metadata":{"id":"YElgnCTBSxrb"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Fast R-CNN（Region of Interest）\n","<font color=\"silver\">Region of Interest,  関心領域"],"metadata":{"id":"gl0GYpqhO6Vb"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Fast R-CNN（Selective Search）"],"metadata":{"id":"NHLkTy1FPLIy"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Fast R-CNN（RoI Pooling）"],"metadata":{"id":"yNqGLK12PLaf"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Fast R-CNN（Multi-Task Loss）"],"metadata":{"id":"UXup0bTLPLsJ"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Faster R-CNN（Region of Interest）\n","<font color=\"silver\">Region of Interest,  関心領域"],"metadata":{"id":"EHcJ6HAxPmyD"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Faster R-CNN（RPN）\n","<font color=\"silver\">RPN, Region Proposal Network, 領域提案ネットワーク</font><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://cvml-expertguide.net/terms/dl/object-detection/)</font></font><br>\n","<img src=\"https://i0.wp.com/cvml-expertguide.net/wp-content/uploads/2021/09/RPN-1.png?resize=1024%2C662&ssl=1\" width=\"480\">\n","<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/shtmr/items/4283c851bc3d9721ed96)</font></font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F62555%2F267eaff6-4462-e413-e9d7-3239f728b4a7.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=84b488754133bd3053ab85763d2d1988\" width=\"480\">\n"],"metadata":{"id":"mCfjN5RIP6dA"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Faster R-CNN（RoI Pooling）"],"metadata":{"id":"dJ0TMLrqQBIB"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Faster R-CNN（Multi-Task Loss）"],"metadata":{"id":"LGxc8RwRQFvy"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Mask R-CNN（Region of Interest）\n","<font color=\"silver\">Region of Interest,  関心領域"],"metadata":{"id":"vpSBka7fQMcC"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Mask R-CNN（RPN）"],"metadata":{"id":"Jp0sOnQTQbOK"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Mask R-CNN（RoI Align）"],"metadata":{"id":"-C52a5sSQfYH"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Mask R-CNN（Bilinear Interpolation）\n","<font color=\"silver\">Bilinear Interpolation, 双線形補間"],"metadata":{"id":"-0xa9VBEQit4"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Multi-Task Loss（Multi-Task Loss）"],"metadata":{"id":"ghQGf3AMQ_eg"}},{"cell_type":"markdown","source":["# <font color=\"Blue\">■</font><font color=\"silver\">YOLOv1 [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4900&cid=b0f01606242a6ed3&CT=1666717393959&OR=ItemsView) </font><br>\n","<font color=\"silver\">YOLO, You Only Look Once: Unified, Real-Time Object Detection</font><br><br>\n","※ 分割されたグリッドのサイズは固定で、グリッド内で識別クラスは1つまで、検出対象の物体は2つまで。そのため、グリッド内の物体が大量の場合は、精度が下がる<br>\n","※ サイズの小さな物体の検出を苦手としており、バウンディング・ボックスを個別に分析できるFaster R-CNNに比べて、識別の精度が低い。<br>\n","※ 領域探索とクラス分類を同時に実行し、物体検出を分類問題ではなく回帰問題推定を行う<br>\n","※ 検出と識別を同時に行うことで処理速度を早めた<br>\n","※ 画像全体に対して特徴マップを生成していくため、汎化制度が高い<br>\n","※ 小さい画像の検出は不得意な部分があり、密接した対象の識別には向いていない<br>\n","<br>"],"metadata":{"id":"AlwrOHr2I2G9"}},{"cell_type":"markdown","source":["# <font color=\"silver\">YOLOv1   1 Introduction 背景\n","※ スライディングwindowやregion proposalといった領域スキャンのアプローチを使わずに、畳み込みニューラルネットワークで画像全体から直接物体らしさと位置を算出する。<br>\n","※まず入力画像を正方形(論文の例では448×448)にリサイズし、それを畳み込みニューラルネットワークの入力とする。<br>\n","<br>\n","<img src=\"https://tech-swim-bike.info/wp-content/uploads/2022/02/YoLo-2.png\" width=\"480\"><br>\n"],"metadata":{"id":"3HC75u4PJ0Rw"}},{"cell_type":"markdown","source":["# <font color=\"silver\">YOLOv1   2. Unified Detection\n","※ S × S の各 grid cell に対して、B個の Bounding Box を推定する。Sはユーザーが決める。論文では７×７。<br>\n","※ 1つのBounding Boxにつき、Bounding Boxの座標値(x, y, w, h)と、そのBounding Boxが物体である信頼度(confidence)スコアの計5つの値を出力する。<br>\n","※ 座標値のx, yはgrid cellの境界を基準にしたBounding Boxの中心座標、幅wと高さhは画像全体のサイズに対する相対値。信頼度スコアはそのBounding Boxが物体か背景かの確率を表す。(物体なら1, 背景なら0)<br>\n","<font color=\"black\"><br>\n","$\\rm Confidence = Pr(Object) * IoU$ <font color=\"silver\">物体が含まれている時の予測検出位置の正確性\n","<br><br><font color=\"black\">\n","$\\rm C = Pr(Class|Object)$\n","<br><br>\n","$\\rm Confidence Score = C * Pr(Object) * IoU$\n","<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://blog.negativemind.com/2019/02/21/general-object-recognition-yolo/)</font></font><br>\n","<img src=\"https://blog.negativemind.com/wp-content/uploads/2019/02/YOLO_detection.jpg\" width=\"640\"><br>\n","<img src=\"https://tech-swim-bike.info/wp-content/uploads/2022/02/YoLo-1.png\" width=\"480\"><br>"],"metadata":{"id":"kRfr8O9BJ0ub"}},{"cell_type":"markdown","source":["# <font color=\"silver\">YOLOv1   2.1. Network Design, Darknet\n","※ 24層のCNNと４層のPooling層から特徴量を抽出し、２層の全結合層で物体のバウンディングボックスと物体の種類の確率を推定する。<br>\n","※ 畳み込み層の最終出力サイズ7×7はgrid cellの分割数と一致する。<br>\n","※ YOLOの出力は、1つのgrid cellにつきB × 5 + C個の出力となり、全体の出力はS × S × (B × 5 + C)個<br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F296108%2F53abd9cc-4633-44b6-7592-9854437bb64b.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=e6a4fcfaee4209aa78e21ae225b06921\" width=\"640\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/#yolo-you-only-look-once)</font></font><br>\n","<img src=\"https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolo-network-architecture.png\" width=\"640\">\n","<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://human-blog.com/yolocommentary3/)</font></font><br>\n","<img src=\"https://human-blog.com/wp-content/uploads/2022/05/Screen-Shot-2022-05-17-at-19.29.43-1024x457.png\" width=\"640\">\n","<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://human-blog.com/yolocommentary3/)</font></font><br>\n","<img src=\"https://human-blog.com/wp-content/uploads/2022/05/CE471F42-AA15-41CF-95F7-E5D31BF5FD84.png\" width=\"640\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://wikidocs.net/167502)</font></font><br>\n","<img src=\"https://wikidocs.net/images/page/163644/Fig_8.png\" width=\"640\"><br>\n","<img src=\"https://wikidocs.net/images/page/163644/Fig_9.png\" width=\"640\"><br><br>"],"metadata":{"id":"TohMbbLOJ1By"}},{"cell_type":"markdown","source":["# <font color=\"silver\">YOLOv1   2.2. Training\n","※ 最終層のReLUを除いて活性化層はLeaky ReLUを利用する。<br>\n","※ 損失関数は、Localization Loss and Classification Loss<br>\n","※ 「(1)バウンディンボックスの損失」＋「(2)信頼度の損失」＋「(3)クラス識別損失（交差エントロピー）」の３つを重みづけした合成損失<br><br>\n","<font color=\"black\">${ \\phi(x) = \\begin{cases} x, \\quad if \\quad x>0 \\\\ 0.1x, \\quad {\\rm otherwise} \\end{cases}}$\n","<br><br>\n","$\\mathcal{L}_{YOLOv1}= \\mathcal{L}_{BB} + \\lambda_1 \\mathcal{L}_{conf} +\\lambda_2  \\mathcal{L}_{CE}$\n","<br><br>\n","$\\mathcal{L} = \\mathcal{L}_\\text{loc} + \\mathcal{L}_\\text{cls}$\n","<br><br>\n"," $\\begin{aligned}\n","\\mathcal{L}_\\text{loc} = \\lambda_\\text{coord} \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{ij}^\\text{obj} [(x_i - \\hat{x}_i)^2 + (y_i - \\hat{y}_i)^2 + (\\sqrt{w_i} - \\sqrt{\\hat{w}_i})^2 + (\\sqrt{h_i} - \\sqrt{\\hat{h}_i})^2 ]\n","\\end{aligned}$\n","<br><br>\n"," $\\begin{aligned}\n","\\mathcal{L}_\\text{cls}  = \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\big( \\mathbb{1}_{ij}^\\text{obj} + \\lambda_\\text{noobj} (1 - \\mathbb{1}_{ij}^\\text{obj})\\big) (C_{ij} - \\hat{C}_{ij})^2 + \\sum_{i=0}^{S^2} \\sum_{c \\in \\mathcal{C}} \\mathbb{1}_i^\\text{obj} (p_i(c) - \\hat{p}_i(c))^2\n","\\end{aligned}$\n","<br><br>\n","<img src=\"https://camo.qiitausercontent.com/4669cb72cfdf272a36e9958db40523743387a1a9/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3337323138322f37393332653335622d353331362d373436352d373032612d3937343031613462633966652e706e67\" width=\"480\">"],"metadata":{"id":"rUnmaPi1J1UL"}},{"cell_type":"markdown","source":["# <font color=\"silver\">YOLOv1   2.3. Inference\n","<font color=\"black\">NMS, Non-Maximum Suppression\n","<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://human-blog.com/yolocommentary3/)</font></font><br>\n","<img src=\"https://human-blog.com/wp-content/uploads/2022/05/Screen-Shot-2022-05-22-at-11.17.03-1536x715.png\" width=\"320\">"],"metadata":{"id":"ZM_1ssauJ1pd"}},{"cell_type":"markdown","source":["# <font color=\"silver\">YOLOv1   3. Comparison to Other Detection Systems"],"metadata":{"id":"XpelwRoHJ2La"}},{"cell_type":"markdown","source":["# <font color=\"silver\">YOLOv1   4. Experiments"],"metadata":{"id":"NFVoX3GwJ2e6"}},{"cell_type":"markdown","source":["# <font color=\"silver\">YOLOv1   4.1. Comparison to Other Real-Time Systems"],"metadata":{"id":"gBWK3gAsJ2yq"}},{"cell_type":"markdown","source":["# <font color=\"silver\">YOLOv1   4.2. VOC 2007 Error Analysis"],"metadata":{"id":"4SmCe8jGJ3FD"}},{"cell_type":"markdown","source":["# <font color=\"silver\">YOLOv1   4.3. Combining Fast R-CNN and YOLO"],"metadata":{"id":"cmtU_m5hKJ-l"}},{"cell_type":"markdown","source":["# <font color=\"silver\">YOLOv1   4.4. VOC 2012 Results"],"metadata":{"id":"GdKZwrh7KKV6"}},{"cell_type":"markdown","source":["# <font color=\"silver\">YOLOv1   4.5. Generalizability: Person Detection in Artwork"],"metadata":{"id":"_bQwdf5lKVTk"}},{"cell_type":"markdown","source":["# <font color=\"silver\">YOLOv1   5. Real-Time Detection In The Wild"],"metadata":{"id":"2INj5XvdKVn5"}},{"cell_type":"markdown","source":["# <font color=\"silver\">YOLOv1   6. Conclusion"],"metadata":{"id":"sNju5Ww-Ka-M"}},{"cell_type":"markdown","source":["# <font color=\"Blue\">■</font><font color=\"silver\">SSD [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4904&cid=b0f01606242a6ed3&CT=1666717577682&OR=ItemsView) </font><br>\n","<font color=\"silver\">SSD, Single Shot MultiBox Detector</font><br>"],"metadata":{"id":"oexG4l_lDBDj"}},{"cell_type":"markdown","source":["# <font color=\"silver\">SSD  1 Introduction 背景\n"],"metadata":{"id":"T6pEV0toEio0"}},{"cell_type":"markdown","source":["# <font color=\"silver\">SSD  2 The Single Shot Detector (SSD)\n","<img src=\"https://camo.qiitausercontent.com/1f6ef47cc3e6e3298d4ddf48b55e600a3d0e821c/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3231313731322f65616530383430362d363530352d626165662d666139322d3537613632643364303561632e706e67\" width=\"640\"><br>"],"metadata":{"id":"FthsMEbjEi9J"}},{"cell_type":"markdown","source":["# <font color=\"silver\">SSD  2.1 Model\n","<img src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/455946/0e56b7b4-9e62-f28d-3655-336d429f4e83.png\" width=\"640\"><br>"],"metadata":{"id":"99bUELa8EjO5"}},{"cell_type":"markdown","source":["# <font color=\"silver\">SSD  2.2 Training\n","※ 信頼度(confidence)損失(3.2.1節)と領域検出(localization)損失 (3.2.2節)の，2タスクの損失を合成した，以下のMultibox損失関数</font><br><br>\n","$L(x, c, l, g) = \\cfrac{1}{N}(L_{CLS}(x, c) + \\alpha L_{LOC}(x, l, g))$<br><br>\n","${\\begin{aligned}\n","L_{\\mathrm{loc}}(x,l,g)\n","&= \\sum_{k=1}^p\\sum_{i\\in \\mathrm{Default Box}}\\sum_{j\\in \\mathrm{GT Box}} \\sum_{m\\in (cx, cy, w, h)} x_{ij}^k \\mathrm{smooth}_{\\mathrm{L1}}(l_i^m- \\hat g_j^m) \\\\\n","\\hat g_j^{\\mathrm{cx}} &= \\frac{g_j^{\\mathrm{cx}} - d_i^{\\mathrm{cx}}}{d_i^{\\mathrm y}} \\\\\n","\\hat g_j^{\\mathrm{cy}} &= \\frac{g_j^{\\mathrm{cy}} - d_i^{\\mathrm{cy}}}{d_i^{\\mathrm h}} \\\\\n","\\hat g_j^{\\mathrm w} &= \\log \\frac{g_j^{\\mathrm w} }{d_i^{\\mathrm w} } \\\\\n","\\hat g_j^{\\mathrm h} &= \\log \\frac{g_j^{\\mathrm h} }{d_i^{\\mathrm h} }\n","\\end{aligned}\n","}$<br><br>\n","${\\begin{aligned}\n","L_{\\mathrm{conf}}(x,c) &=\n","-\\sum_{k=1}^p\\sum_{i\\in \\mathrm{Default Box}}\\sum_{j\\in \\mathrm{GT Box}}  x_{ij}^k \\log(\\hat c_i^k) - \\sum_{i\\in \\mathrm{Default Box}}\\log(\\hat c_i^0)\\prod_{k=1}^p\\prod_{j\\in \\mathrm{GT Box}}(1-x_{ij}^k ) \\\\\n","\\hat c_i^k &= \\frac{\\exp(c_i^k)}{\\sum_k \\exp(c_i^k)}\n","\\end{aligned}\n","}$\n","<br><br>\n","$\\displaystyle {s_k = s_{\\mathrm{min}} + \\frac{s_{\\mathrm{max}} - s_{\\mathrm{min}}\n","}{m-1}(k-1), \\ k \\in [1,m]\n","}$"],"metadata":{"id":"RtOo_10FEjfZ"}},{"cell_type":"markdown","source":["# <font color=\"silver\">SSD  3 Experimental Results"],"metadata":{"id":"pGlTeCSQEjwx"}},{"cell_type":"markdown","source":["# <font color=\"silver\">SSD  3.1 PASCAL VOC2007"],"metadata":{"id":"aI14w0B0EkBI"}},{"cell_type":"markdown","source":["# <font color=\"silver\">SSD  3.2 Model analysis"],"metadata":{"id":"1b1xV7FpEkSp"}},{"cell_type":"markdown","source":["# <font color=\"silver\">SSD  3.3 PASCAL VOC2012"],"metadata":{"id":"LHV5muyXEkkq"}},{"cell_type":"markdown","source":["# <font color=\"silver\">SSD  3.4 COCO"],"metadata":{"id":"ua3V53o4EvVF"}},{"cell_type":"markdown","source":["# <font color=\"silver\">SSD  3.5 Preliminary ILSVRC results"],"metadata":{"id":"gQihDdC7Evoq"}},{"cell_type":"markdown","source":["# <font color=\"silver\">SSD  3.6 Data Augmentation for Small Object Accuracy"],"metadata":{"id":"hTvpj9itEv5o"}},{"cell_type":"markdown","source":["# <font color=\"silver\">SSD  3.7 Inference time"],"metadata":{"id":"FPGNPdukFAfE"}},{"cell_type":"markdown","source":["# <font color=\"silver\">SSD  4 Related Work"],"metadata":{"id":"DDZS7o7_FAv4"}},{"cell_type":"markdown","source":["# <font color=\"silver\">SSD  5 Conclusions"],"metadata":{"id":"nLmOPYXCFBDo"}},{"cell_type":"markdown","source":["# <font color=\"silver\">SSD  6 Acknowledgment"],"metadata":{"id":"X2cis9wJFXKi"}},{"cell_type":"markdown","source":["# <font color=\"Blue\">■</font><font color=\"silver\">FCOS  [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4906&cid=b0f01606242a6ed3&CT=1666771227100&OR=ItemsView) </font><br>\n","<font color=\"silver\">FCOS, Fully Convolutional One-Stage Object Detection</font><br><br>\n","> ※ FPNを用いて複数のスケールで抽出した特徴マップに「head」を追加して、クラス分類、BB回帰とセンターネスを学習するモデル。<br>\n","> ※ 複数のクラスに当てはまる点への対処<br>\n","> ※ Feature Pyramid を用いて、異なるレベルの特徴マップを複数出力し、それぞれの特徴マップで、異なるサイズの物体を検出する<br>\n","> ※ ポジティブサンプル数とネガティブサンプル数(検出物と背景)の不均衡を改善している。<br>\n","<br>\n","<img src=\"https://user-images.githubusercontent.com/24524018/88918758-f8459280-d2a4-11ea-9119-175abfa056b8.png\" width=\"480\">"],"metadata":{"id":"zLX0biKD3RqN"}},{"cell_type":"markdown","source":["# <font color=\"silver\">FCOS  1. 背景★\n","\n"],"metadata":{"id":"_lXqfF_X3R8m"}},{"cell_type":"markdown","source":["# <font color=\"silver\">FCOS  2. 関連研究★\n","\n"," アンカーボックスの欠点<br>\n","> ※ アンカーボックスのアスペクト比や数が、予測精度に影響する<br>\n","> ※ フィーチャーマップごとにアスペクト比を設定する必要がある<br>\n","> ※ アンカーボックスのほとんどは、予測時に採用されない<br>\n","> ※ 全てのアンカーボックスの IoU を計算する必要あるため計算負荷が大きい<br>\n","> ※ ポジティブサンプル数とネガティブサンプル数が不均衡な状態<br>\n"],"metadata":{"id":"kb3LvyCp3SPS"}},{"cell_type":"markdown","source":["# <font color=\"silver\">FCOS  Anchor-free Detectors"],"metadata":{"id":"t-ET4vwA3SiB"}},{"cell_type":"markdown","source":["# <font color=\"silver\">FCOS  3. Our Approach"],"metadata":{"id":"SFwnpMP43UF0"}},{"cell_type":"markdown","source":["# <font color=\"silver\">FCOS  3.1. Fully Convolutional One-Stage Object Detector★\n",">$\\begin{cases}l^{\\ast} = x - x^{(i)}_0, t^\\ast = y - y^{(i)}_0 \\\\ r^\\ast = x^{(i)}_1 - x, b^\\ast = y^{(i)}_1 - y  \\end{cases} $\n"],"metadata":{"id":"KVRjFXX23dSS"}},{"cell_type":"markdown","source":["# <font color=\"silver\">FCOS  Network Outputs."],"metadata":{"id":"3BVXvHaY3dsr"}},{"cell_type":"markdown","source":["# <font color=\"silver\">FCOS  Loss Function.★</font>\n","> ※ 各特徴量マップの位置ijにおいて、正しいBounding boxとの上下左右の距離を回帰させる手法を提案した。</font><br>\n","> ※ クラス分類に関する損失 + バウンディングボックスの座標に関する損失<br>\n","> ※ これによりアンカーが不要になる。<br>\n","<br>\n","<img src=\"https://user-images.githubusercontent.com/24524018/88918650-c7655d80-d2a4-11ea-8a04-32ade7894a88.png\" width=\"480\">\n","<br><font color=\"Black\">\n","<br>\n","第 1 項：<font color=\"silver\">クラス分類に関する損失<br></font>\n","第 2 項：<font color=\"silver\">バウンディングボックスの座標に関する損失<br></font>\n","<br>\n","$p_{x,y}$：<font color=\"silver\">クラス分類の予測確率<br></font>\n","$c_{x,y}^*$：<font color=\"silver\">正解のクラスラベル<br></font>\n","$t_{x,y}$：<font color=\"silver\">(l, r, t, b)の予測結果<br></font>\n","$t_{x,y}^*$：<font color=\"silver\">正解の$t_{x,y}$<br></font>\n","$1_{c_{x,y}^*>0}$：<font color=\"silver\">物体が存在するときは1<br></font>\n","$N_{pos}$：<font color=\"silver\">バウンディングボックスの個数<br></font>\n","<br>\n","$\\displaystyle L(\\{\\vec p_{x, y}\\}, \\{\\vec t_{x, y}\\}) = \\frac{1}{N_{pos}} \\sum_{x,y} L_{cls}(\\vec p_{x, y}, c^{\\ast}_{x, y}) + \\frac{\\lambda}{N_{pos}} \\sum_{x,y} I_{c^\\ast_{x,y} > 0} L_{reg} (\\vec t_{x, y}, \\vec t^\\ast_{x,y}) $"],"metadata":{"id":"P36EdE0j5m1V"}},{"cell_type":"markdown","source":["# <font color=\"silver\">FCOS  Inference"],"metadata":{"id":"lXBmZNfj3eCC"}},{"cell_type":"markdown","source":["# <font color=\"silver\">FCOS  3.2. Multi-level Prediction with FPN for FCOS★\n","> ※ FPNを用いて複数のスケールで特徴量マップの抽出を行い、レベルごと(P3〜P7)に予測することで、さまざまなスケールの特徴を捉える。<br>\n","> ※ FCN と同様に、全結合層を持たないアーキテクチャ。<br>\n","> ※ 出力テンソルの要素は、特徴マップ上の各点に関する値。<br>\n","> （上）クラスを出力。<br>\n","> （中）バウンディングボックスの中心に対するズレを出力。<br>\n","> （下）バウンディングボックスの各辺までの距離を出力。<br><br>\n","<img src=\"https://user-images.githubusercontent.com/24524018/88918756-f67bcf00-d2a4-11ea-9549-cb7a28a3c4b3.png\" width=\"800\">\n","<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://www.researchgate.net/figure/Network-structure-of-Tiny-FCOS_fig1_355699312)</font><br>\n","<img src=\"https://www.researchgate.net/publication/355699312/figure/fig1/AS:1123578456088597@1644893387688/Network-structure-of-Tiny-FCOS.png\" width=\"640\">"],"metadata":{"id":"aGUgAWYl3eYS"}},{"cell_type":"markdown","source":["# <font color=\"silver\">FCOS  3.3. Center-ness for FCOS★\n","<font color=\"silver\">Center-ness for FCOS\n","> ※ フィーチャーマップ上の点と ground truth の中心距離を数値化したインデックス。<br>\n","> ※ 特徴マップ上のある点が正解ボックスの中心からどれくらい離れているかを表す指標。<br>\n","> ※ 学習において損失は BCELoss:Binary Cross Entropyによって計算される。<br><br>\n","> ※ FCOS ではフィーチャーマップ上の ground truth 内に入る全ての点はポジティブサンプル(物体が存在する領域)として扱う。そのため、物体の中心から離れた点を中心としたバウンディングボックスが予測されることがある。その様な状況を防ぐため、 FCOS では Center-ness というインデックスを学習に加え、低品質なバウンディングボックスが作成されることを抑制する。<br>\n","> ※ （ $l, t, r, b$ ）で計算する 0〜1の値で、値が大きいほど、正解ボックスの中心に位置する。<br>\n","> ※ バウンディングボックス予測の際、センターネスの低い点はNMSによりフィルタリングする。<br>\n","> ※ 正解バウンディングボックスから離れた位置に予測バウンディングボックスが出現することを抑制<br>\n","> ※ 学習時に、正解バウンディングボックスに対応する予測バウンディングボックス(正のサンプル)が多くなり、学習効率が上がる<br>\n","<br><font color=\"black\">\n","$\\displaystyle {\\rm centerness}^\\ast = \\sqrt{\\frac{\\min(l^\\ast, r^\\ast)}{\\max(l^\\ast, r^\\ast)} \\times \\frac{\\min(t^\\ast, b^\\ast)}{\\max(t^\\ast, b^\\ast)}} $\n","<br><br>\n","（ $l^*, t^*, r^*, b^*$ ）：<font color=\"silver\">ある点からバウンディングボックスの各辺までの距離<br><br>\n","<img src=\"https://user-images.githubusercontent.com/24524018/88918747-f1b71b00-d2a4-11ea-8faa-1892f21c064b.png\" width=\"320\">"],"metadata":{"id":"t5O-D7zd4Xom"}},{"cell_type":"markdown","source":["# <font color=\"silver\">FCOS  4. Experiments"],"metadata":{"id":"SRjacsun4YFj"}},{"cell_type":"markdown","source":["# <font color=\"silver\">FCOS  5. Extensions on Region Proposal Networks"],"metadata":{"id":"sM54Evyw5Dzd"}},{"cell_type":"markdown","source":["# <font color=\"silver\">FCOS  6. Conclusion"],"metadata":{"id":"mFHGkLkF5IeM"}},{"cell_type":"markdown","source":["#<font color=\"Blue\">**■**</font> <font color=\"silver\">Transformer [<font color=\"silver\">…](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4673&cid=b0f01606242a6ed3&CT=1666551601096&OR=ItemsView)</font><br>\n","https://arxiv.org/pdf/1706.03762.pdf"],"metadata":{"id":"o8ZgIoBnlcZd"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Transformer 1 Introduction 導入 <br>"],"metadata":{"id":"yE5a1Kgfmwqd"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Transformer 2 Background 背景 <br>"],"metadata":{"id":"duGjAOO4my8v"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Transformer 3 Model Architecture<br>\n","><font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://lilianweng.github.io/posts/2018-06-24-attention/)</font><br>\n","<img src=\"https://lilianweng.github.io/posts/2018-06-24-attention/transformer.png\" width=\"640\"><br>\n","<br>\n","<font color=\"Blue\">**Layer Normalization**</font><br>\n","><font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)</font><br>\n","<img src=\"https://lena-voita.github.io/resources/lectures/seq2seq/transformer/layer_norm-min.png\" width=\"320\">\n"],"metadata":{"id":"W7y6Csl9m3yL"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Transformer 3.1 Encoder and Decoder Stacks<br>\n","><font color=\"Blue\">**Encoder and Decoder**</font><br>\n","><font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://nlpillustration.tech/?p=2171)</font><br>\n","<img src=\"https://nlpillustration.tech/wp-content/uploads/2022/08/Transformer6-770x770.jpg\" width=\"480\">"],"metadata":{"id":"akIo8vEGm5bD"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Transformer 3.2 Attention<br></font>"],"metadata":{"id":"PFIEt2iUm7UU"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Transformer 3.2.1 Scaled Dot-Product Attention<br></font>\n","><font color=\"Blue\">**Scaled Dot-Product Attention**</font><br>\n","> ※ $\\sqrt{d_k}$で割る理由<br>\n","> ・ $\\sqrt{d_k}$はquery, keyの単語分散表現の次元数で、論文では512<br>\n","> ・ 確率が低い部分の勾配情報を保持するため</font><br>\n","> ・ $\\sqrt{d_k}$ が大きくなると逆伝播時のソフトマックス関数の勾配が小さくなるため、 学習が円滑に進まなくなる</font><br>\n","<br>\n","$\\rm{ScaledDotProductAttention}(\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V}) = \\rm{softmax}\\left(\\cfrac{\\boldsymbol{QK}^T}{\\sqrt{d_k}}\\right)\\boldsymbol{V}$<br>\n","<br>\n","<img src=\"https://production-media.paperswithcode.com/methods/SCALDE.png\" width=\"200\"><br>"],"metadata":{"id":"K66PsnrIm9C1"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Transformer 3.2.2 Multi-Head Attention <br>\n","><font color=\"Blue\">**Multi-Head Attention**</font><br>\n","<br>\n",">$\\begin{eqnarray} \\rm{MultiHead Attention}(\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V}) &=& \\rm{Concat}(head_1, head_2, \\cdots, head_h)\\boldsymbol{W}_o\\\\ \\rm{where}\\ head_i &=& \\rm{ScaledDotProductAttention}(\\boldsymbol{QW}^Q_i, \\boldsymbol{KW}^K_i, \\boldsymbol{VW}^V_i) \\end{eqnarray}$<br>\n","<br>\n","ただし、$W_i^Q, W_i^K \\in \\mathbb{R}^{d_{model} \\times d_k}, W_i^V \\in \\mathbb{R}^{d_{model} \\times d_v}, W^O \\in \\mathbb{R}^{hd_v \\times d_{model}}$\n","<br><br>\n","<img src=\"https://production-media.paperswithcode.com/methods/multi-head-attention_l1A3G7a.png\" width=\"200\">\n","<br>"],"metadata":{"id":"9p3OFL3mnAc6"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Transformer 3.2.3 Applications of Attention in our Model <br></font>\n","># <font color=\"silver\">エンコーダー ー デコーダーのAttention\n","> ※ Queryはデコーダ内前層の出力、KeyとValueはエンコーダの最終出力。すなわち、系列変換モデルでのAttentionと同様のもの（<font color=\"Blue\">**Source Target Attention**</font>）。\n","># <font color=\"silver\">エンコーダーのAttention\n","> ※ Query, Key, Valueの全てがエンコーダ内の前層出力（<font color=\"Blue\">**Self-Attention**</font>）。\n","># <font color=\"silver\">デコーダーのAttention\n","> ※ Scaled Dot-Product Attentionのsoftmaxの前の入力に対応する部分を$-\\infty$に置き換えてマスクする（<font color=\"Blue\">**Masked Mult-Head Attention**</font>）。<br>\n","> ※ ${\\rm softmax}(x_i)=\\frac{\\exp(x_i)}{\\sum_j \\exp (x_j)}$ なので、$x_i \\to -\\infty$なら、maskされる。<br>\n","><font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://lilianweng.github.io/posts/2018-06-24-attention/)</font><br>\n","<img src=\"https://lilianweng.github.io/posts/2018-06-24-attention/transformer.png\" width=\"640\">\n"],"metadata":{"id":"ogF-XZGdnB20"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Transformer 3.3 Position-wise Feed-Forward Networks <br></font>\n","><font color=\"Blue\">**Position-wise Feed-Forward Networks <br></font>**</font><br>\n","> ※ Position-wiseというのはただ単に、各単語ごとに独立してニューラルネットワークがあるということ(ただし、重みは共有)。 ニューラルネットワーク内では他単語との干渉はない。2層のニューラルネットワークになっている。<br>\n","<br>\n",">$\\rm{FFN}(x) = \\max(0, xW_1+b_1)W_2+b_2$"],"metadata":{"id":"RTBUAbnwnDXd"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Transformer 3.4 Embeddings and Softmax <br>"],"metadata":{"id":"QwQXdt5vnEvC"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Transformer 3.5 Positional Encoding<br></font>\n","><font color=\"Blue\">**Positional Encoding**</font><br>\n","> ※ 時系列を考慮するために、入力の埋め込み表現に「位置情報」を埋め込む。<br>\n","<br>\n",">$\\rm{PE}_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})$\n","<br><br>\n","$\\rm{PE}_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})$\n","<br><br>\n","$pos$：<font color=\"silver\">位置</font><br>\n","$i$：<font color=\"silver\">次元</font><br>"],"metadata":{"id":"Cbp3Px4FnGO7"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Transformer 4 Why Self-Attention<br>\n",">※ 計算量が小さい。(再帰や畳み込み(Separable Convolution)よりも。)<br>\n","※ 並列計算可能<br>\n","※ 広範囲の依存関係を学習可能<br>\n","※ 高い解釈可能性<br>"],"metadata":{"id":"aQHReotVnH07"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Transformer 5 Training<br>"],"metadata":{"id":"dXntoyfmnJWj"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Transformer 6 Results<br></font>\n","\n",">  ※ シーケンスの長さ $n$ は，モデルの次元 $d$ よりも小さいことが多いので、$n<d$ で，Self-Attentionの計算量コストが最も小さくなる。\n","<br><br>\n","><img src=\"https://camo.qiitausercontent.com/493038d5bbfdd7e5a27858e84f288bf99c398d85/68747470733a2f2f696d6775722e636f6d2f667478666255682e706e67\" width=\"640\">\n","<br><br>\n","Complexity per layer：1層あたりの計算量<br>\n","Sequential Operations：逐次処理を最小限にする並列処理可能な計算量。Recurrent層はシーケンスの長さだけコストがかかる。<br>\n","Maximum Path Length：ネットワーク内の長距離依存関係間の経路長。 Self-Attentionは定数のコストで，入出力間の任意の組み合わせの経路を繋げることができる。<br>\n","<br>\n","> ※ Transformerは高い翻訳精度を出しつつ、かつ計算コストを削減できている。<br>\n","<br>\n","><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fimgur.com%2FNzfVUU1.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=09d6a8ddcccf9bfb54845cea29493f2c\" width=\"640\"><br>"],"metadata":{"id":"O224v9pNnK50"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Transformer 7 Conclusion <br>"],"metadata":{"id":"dsPaGkk2nMgo"}},{"cell_type":"markdown","source":["# <font color=\"Blue\">■</font><font color=\"silver\">GNMT [<font color=\"silver\">…](https://1drv.ms/w/s!AtNuKiQGFvCwpDqHErlaj63TplyF?e=NNelkC)</font><br>\n","<font color=\"silver\">GNMT, Google's Neural Machine Translation</font><br>\n","https://arxiv.org/pdf/1609.08144.pdf"],"metadata":{"id":"YcBxi6eYwbX2"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GNMT  1 Introduction 背景"],"metadata":{"id":"QjphAxfTJti3"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GNMT  2 Related Work 関連事項"],"metadata":{"id":"ILrkvnHgJtq5"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GNMT  3 Model Architecture</font>\n","># <font color=\"silver\">Language Model, 言語モデル</font>\n",">$\\boldsymbol{x_1}, \\, \\boldsymbol{x_2}, \\, \\ldots, \\, \\boldsymbol{x_M} = \\mathrm{EncorderRNN}(x_1, \\, x_2, \\, \\ldots, \\, x_M)$\n","<br><br>\n","$\\displaystyle\\begin{align}\n","P(Y|X) &= P(Y|\\boldsymbol{x_1}, \\, \\boldsymbol{x_2}, \\, \\ldots, \\, \\boldsymbol{x_M})\\\\\n","&= \\displaystyle\\prod_{i=1}^{N} P(y_i|y_0,y_1,\\ldots,y_{i-1};\\boldsymbol{x_1}, \\, \\boldsymbol{x_2}, \\, \\ldots, \\, \\boldsymbol{x_M})\n"," \\end{align}$\n","<br>\n"," $P(y_i | y_0, y_1, y_2, ..., y_{i-1};{\\boldsymbol x}_1, {\\boldsymbol x}_2, ..., {\\boldsymbol x_M}) $\n","># <font color=\"silver\">Encoder-Decoder</font>\n","> ※ 8層のエンコーダおよび8層のデコーダの <font color=\"Blue\">**Encoder-Decoder構造**</font> であり、1層ごとに計16個の <font color=\"Blue\">**GPU**</font> を割り当てる。<br><br>\n","> ※ エンコーダの1層目（1段目と2段目を合わせた部分）が <font color=\"Blue\">**双方向LSTM**</font> である。 もし、全層を双方向LSTMにした場合、それぞれの層は前の層の順方向と逆方向 の計算が終わるのを待たなければならず、 2GPU (順方向に1、 逆方向に1) し か並列に動かすことができない。<br><br>\n","> ※ デコーダ第1層の出力とエンコーダ第8層の出力とで <font color=\"Blue\">**Attention**</font> を計算し、デコーダの全層に送る。デコーダの第1層とエンコーダの第8層で計算するのは、並列化の効率を高めるため。 デコーダの第8層とエンコーダの第8層とでアテンションを計算すると待ち時間が発生する。<br><br>\n","<img src=\"https://norman3.github.io/papers/images/gnmt/f01.png\" width=\"640\"><br><br>\n",">$s_t = \\mathrm{AttentionFunction}(\\boldsymbol{y_{i-1}}, \\boldsymbol{x_t})  \\quad ∀t,\\quad 1 ≤ t ≤ M$\n","<br><br>\n","> $p_t = \\exp(s_t)/\\displaystyle\\sum_{t=1}^{M}\\exp(s_t) \\quad ∀t,\\quad 1 ≤ t ≤ M$\n","<br><br>\n","> $\\boldsymbol{a_i} = \\displaystyle\\sum_{t=1}^{M}p_t \\boldsymbol{x_t}$\n"],"metadata":{"id":"6njRUetvJ8PZ"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GNMT 3.1 Residual Connections</font>\n","> ※ <font color=\"Blue\">**Residual Connections, 残差接続**</font><br>\n","><img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/R/Ryobot/20171218/20171218171123.png\" width=\"640\">\n","<br><br>\n","$\\begin{align}{\\bf c}_{t}^{i}, {\\bf m}_{t}^{i} &= LSTM_i({\\bf c}_{t-1}^{i}, {\\bf m}_{t-1}^{i}, {\\bf x}_{t}^{i-1};{\\bf W}^{i}) \\\\\n","{\\bf x}_{t}^{i} &= {\\bf m}_{t}^{i} \\\\\n","{\\bf c}_{t}^{i+1}, {\\bf m}_{t}^{i+1} &= LSTM_{i+1}({\\bf c}_{t-1}^{i+1}, {\\bf m}_{t-1}^{i+1}, {\\bf x}_{t}^{i};{\\bf W}^{i+1}) \\end{align}$\n","<br><br>\n","$\\begin{align}{\\bf c}_{t}^{i}, {\\bf m}_{t}^{i} &= LSTM_i({\\bf c}_{t-1}^{i}, {\\bf m}_{t-1}^{i}, {\\bf x}_{t}^{i-1};{\\bf W}^{i}) \\\\\n","{\\bf x}_{t}^{i} &= {\\bf m}_{t}^{i} + {\\bf x}_{t}^{i-1} \\\\\n","{\\bf c}_{t}^{i+1}, {\\bf m}_{t}^{i+1} &= LSTM_{i+1}({\\bf c}_{t-1}^{i+1}, {\\bf m}_{t-1}^{i+1}, {\\bf x}_{t}^{i};{\\bf W}^{i+1}) \\end{align}$"],"metadata":{"id":"zoCeCcSh9iui"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GNMT  3.2 Bi-directional Encoder for First Layer</font><br>\n","><img src=\"https://norman3.github.io/papers/images/gnmt/f04.png\" width=\"480\">"],"metadata":{"id":"1PAzNTvo-lz4"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GNMT 3.3 Model Parallelism\n","> ※ <font color=\"Blue\">**モデル並列**</font> 及び <font color=\"Blue\">**データ並列**</font> を採用しており、パラメータの更新は <font color=\"Blue\">**非同期型**</font> で行う。<br>"],"metadata":{"id":"MUOgJYjOOqHk"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GNMT 4 Segmentation Approaches"],"metadata":{"id":"nNSa-qspPIGL"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GNMT 4.1 Wordpiece Model\n","> ※ <font color=\"Blue\">**Wordpiece**</font> と  <font color=\"Blue\">**Sentencepiece**</font><br>"],"metadata":{"id":"wKJNd42-PSpX"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GNMT 4.2 Mixed Word/Character Model"],"metadata":{"id":"0Dah_KPIPVpI"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GNMT 5 Training Criteria\n","> ※ <font color=\"Blue\">**Reward Objective, 強化学習による再学習**</font><br><br>\n","><font color=\"black\">$\\displaystyle\\mathcal{O}_{mixed}(\\theta) = \\alpha \\times \\mathcal{O}_{ML}(\\theta) + \\mathcal{O}_{RL}(\\theta)$\n","<br><br>\n","$\\displaystyle\\mathcal{O}_{ML}({\\bf \\theta}) = \\sum_{i=1}^{N}\\log P_{\\theta}({\\bf Y}^{*(i)}|{\\bf X}^{(i)}) $\n","<br><br>\n","$\\displaystyle\\mathcal{O}_{RL}({\\bf \\theta}) = \\sum_{i=1}^{N}\\sum_{Y \\in \\mathcal{Y}} P_{\\theta}({\\bf Y}|{\\bf X}^{(i)}) r({\\bf Y}, {\\bf Y}^{*(i)}) $"],"metadata":{"id":"1d42zmUQPab-"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GNMT 6 Quantizable Model and Quantized Inference\n","> ※ <font color=\"Blue\">**Quantization, 量子化による高速化**</font><br><br>\n",">$\\begin{align}{\\bf c}_{t}^{'i}, {\\bf m}_{t}^{i} &= LSTM_i({\\bf c}_{t-1}^{i}, {\\bf m}_{t-1}^{i}, {\\bf x}_{t}^{i-1};{\\bf W}^{i}) \\\\\n","{\\bf c}_{t}^{i} &= \\max(-\\delta, \\min(\\delta, {\\bf c}_{t}^{'i})) \\\\\n","{\\bf x}_{t}^{'i} &= {\\bf m}_{t}^{i} + {\\bf x}_{t}^{i-1} \\\\\n","{\\bf x}_{t}^{'i} &= \\max(-\\delta, min(\\delta, {\\bf x}_{t}^{'i}) \\\\\n","{\\bf c}_{t}^{i+1}, {\\bf m}_{t}^{i+1} &= LSTM_{i+1}({\\bf c}_{t-1}^{i+1}, {\\bf m}_{t-1}^{i+1}, {\\bf x}_{t}^{i};{\\bf W}^{i+1}) \\\\\n","{\\bf c}_{t}^{'i+1} &= \\max(\\delta, min(\\delta, {\\bf c}_{t}^{'i+1})) \\end{align}$\n","<br><br>\n","$\\begin{align}{\\bf W} &= [{\\bf W}_1, {\\bf W}_2, {\\bf W}_3, {\\bf W}_4, {\\bf W}_5, {\\bf W}_6, {\\bf W}_7, {\\bf W}_8 ] \\\\\n","{\\bf i}_t &= sigmoid({\\bf W}_1{\\bf x}_t + {\\bf W}_2{\\bf m}_t) \\\\\n","{\\bf i'}_t &=tanh({\\bf W}_3{\\bf x}_t + {\\bf W}_4{\\bf m}_t) \\\\\n","{\\bf f}_t &= sigmoid({\\bf W}_5{\\bf x}_t + {\\bf W}_6{\\bf m}_t) \\\\\n","{\\bf o}_t &= sigmoid({\\bf W}_7{\\bf x}_t + {\\bf W}_8{\\bf m}_t) \\\\\n","{\\bf c}_t &= {\\bf c}_{t-1} \\odot {\\bf f}_{t} + {\\bf i}_{t}^{'} \\odot {\\bf i}_{t} \\\\\n","{\\bf m}_t &= {\\bf c}_{t} \\odot {\\bf o}_{t} \\end{align}$\n","<br><br>\n","$\\begin{align}s_i &= \\max(abs({\\bf W}[i,:])) \\\\\n","{\\bf WQ}[i,j] &= round({\\bf W}[i,j] / {\\bf s}_i \\times 127.0) \\end{align}$\n","<br><br>\n","$\\begin{align}{\\bf v}_t &= {\\bf W}_s \\times {\\bf y}_t \\\\\n","{\\bf v}_t^{'} &= \\max(-\\gamma, min(\\gamma, {\\bf v}_t)) \\\\\n","{\\bf p}_t &= softmax({\\bf v}_t^{'})\\end{align}$\n","<br><br>\n","><img src=\"https://norman3.github.io/papers/images/gnmt/f05.png\" width=\"480\">\n","<br><br>\n","><img src=\"https://norman3.github.io/papers/images/gnmt/f06.png\" width=\"480\">\n","\n"],"metadata":{"id":"Lniu3De4QPnp"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GNMT 7 Decoder\n","\n",">$\\begin{align}s({\\bf Y}, {\\bf X}) &= \\log(P({\\bf Y}|{\\bf X}))/lp({\\bf Y}) + cp({\\bf X};{\\bf Y}) \\\\\n","lp({\\bf Y}) &= \\frac{(5+|{\\bf Y}|)^{\\alpha}}{(5+1)^{\\alpha}} \\\\\n","cp({\\bf X};{\\bf Y}) &= \\beta * \\sum_{i=1}^{|{\\bf X}|} \\log(\\min(\\sum_{j=1}^{|{\\bf Y}|}p_{i,j}, 1.0)), \\end{align}$\n","<br><br>\n","><img src=\"https://norman3.github.io/papers/images/gnmt/f07.png\" width=\"480\">\n","<br><br>\n","><img src=\"https://norman3.github.io/papers/images/gnmt/f08.png\" width=\"480\">"],"metadata":{"id":"OuKasYSShUD5"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GNMT 8 Experiments and Results 実験\n","\n","><img src=\"https://norman3.github.io/papers/images/gnmt/f09.png\" width=\"480\">"],"metadata":{"id":"ptL3llHSRik4"}},{"cell_type":"markdown","source":["# <font color=\"Blue\">■</font><font color=\"silver\">BERT [<font color=\"silver\">…](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4675&cid=b0f01606242a6ed3&CT=1668010639719&OR=ItemsView)</font>\n","https://arxiv.org/pdf/1810.04805.pdf\n"],"metadata":{"id":"pK5t0MwcVt0l"}},{"cell_type":"markdown","source":["# <font color=\"silver\">BERT  1 Introduction 導入\n","<img src=\"https://data-analytics.fun/wp-content/uploads/2020/04/bert3.png\" width=\"640\"><br>"],"metadata":{"id":"CZkaclhbekfd"}},{"cell_type":"markdown","source":["# <font color=\"silver\">BERT  2  Related Work 関連事項"],"metadata":{"id":"MICpVnPbhad4"}},{"cell_type":"markdown","source":["# <font color=\"silver\">BERT  3 BERT\n","> ※ 初めての深い双方向型教師なし言語モデル<br>\n","> ※ 教師なし事前学習により言語分散表現を獲得し、教師ありファインチューニングによって特定のタスクへ適応させる<br>\n","> ※ 最後に出力層を追加するだけで利用できる汎用性が高いモデル<br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fimgur.com%2F1ol4NHO.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=e51ac8d5c804ad835a5d5c50a13eb5dc\" width=\"640\"><br>\n","># <font color=\"silver\">モデルアーキテクチャ\n","> ※ 深い双方向型のTransformerのエンコーダーのみで構成された構造<br>\n","> ※ TransformerやGPTは前向き、また、ELMoは前向きと後ろ向きを別で計算してアウトプットを結合するため、浅い双方向LSTM<br>\n","># <font color=\"silver\">BERT  入力／出力表現\n","> ※ sentenceの先頭に[CLS]トークンを持たせる。<br>\n","> ※ 間に[SEP]トークンを入れ 1文目か2文目かを表す埋め込み表現を加算 する。<br>\n","> ※ トークン＋セグメント＋ポジション。<br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fimgur.com%2F65BPFqu.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=1b61f96cff68acdd08e93cb7f0ca2d77\" width=\"800\"><br>"],"metadata":{"id":"lkKQLdDZhwch"}},{"cell_type":"markdown","source":["# <font color=\"silver\">BERT  3.1 Pre-training BERT\n","># <font color=\"silver\">Task1: MLM,  Masked Language Modeling, 単語マスク問題, 局所的な特徴学習\n","> ※ 入力系列のうち、隠された単語がなにかを予測<br>\n","> ※ 入力の15%のトークンを[Mask]トークンでマスクし、元のトークンを当てるタスク<br>\n","># <font color=\"silver\">Task2: NSP, Next Sentence Prediction,  隣接文問題, 大域的な特徴学習\n","> ※ 2つの入力文が隣り合うものかどうかを判別<br>\n","> ※ 2文選んでそれらが連続した文かどうかを当てるタスク<br>"],"metadata":{"id":"YzERtA2Dhxqk"}},{"cell_type":"markdown","source":["# <font color=\"silver\">BERT  3.2 Fine-tuning BERT\n","> <img src=\"https://camo.qiitausercontent.com/1e8f78d544a2e314fc94afeb7490a6b720ae3e73/68747470733a2f2f696d6775722e636f6d2f4a4f69735065762e706e67\" width=\"640\"><br>"],"metadata":{"id":"tJRP8V9jicA8"}},{"cell_type":"markdown","source":["# <font color=\"silver\">BERT  4 Experiments 実験\n","> ※ GLUEベンチマークについて<br>\n","<img src=\"https://deepsquare.jp/wp-content/uploads/2020/09/GLUE.jpg\" width=\"800\"><br>\n","<img src=\"https://deepsquare.jp/wp-content/uploads/2020/09/SQuAD.png\" width=\"480\"><br>\n","<img src=\"https://deepsquare.jp/wp-content/uploads/2020/09/SWAG.png\" width=\"480\"><br>\n"],"metadata":{"id":"8kfhzPekilic"}},{"cell_type":"markdown","source":["# <font color=\"silver\">BERT  5. Ablation Studies\n","><img src=\"https://deepsquare.jp/wp-content/uploads/2020/09/compare_traing.png\" width=\"480\"><br>\n","<img src=\"https://deepsquare.jp/wp-content/uploads/2020/09/model_size.png\" width=\"480\"><br>\n","<img src=\"https://deepsquare.jp/wp-content/uploads/2020/09/feature_approch.png\" width=\"480\"><br>"],"metadata":{"id":"ePahR0lFjI1c"}},{"cell_type":"markdown","source":["# <font color=\"silver\">BERT  6 Conclusion 結論"],"metadata":{"id":"0LzXTRlikL2X"}},{"cell_type":"markdown","source":["# <font color=\"Blue\">■</font><font color=\"silver\">GPT [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4677&cid=b0f01606242a6ed3&CT=1666551710079&OR=ItemsView)</font><br>"],"metadata":{"id":"BnfMqUeMUgme"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   1 Introduction 背景"],"metadata":{"id":"1TAix0KpmO6Z"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   2 Related Work 関連事項"],"metadata":{"id":"bk8ZIDzNmPO-"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   Semi-supervised learning for NLP  "],"metadata":{"id":"Ec-SGIsomPgY"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   Unsupervised pre-training\n"],"metadata":{"id":"YvJh3j3lmPxw"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   Auxiliary training objectives"],"metadata":{"id":"56vfn6GzmQHo"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   3 Framework\n","<img src=\"https://data-analytics.fun/wp-content/uploads/2020/04/open-gpt.png\" width=\"160\"><br>\n","<img src=\"https://data-analytics.fun/wp-content/uploads/2020/04/open-gpt_input.png\" width=\"640\"><br>"],"metadata":{"id":"ui_D-kXBmQY_"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   3.1 Unsupervised pre-training\n","$ \\displaystyle L_1\\left(\\mathcal{U}\\right)=\\sum_i \\log P\\left(u_i| u_{i-k}, \\cdots, u_{i-1}; \\Theta\\right)$\n","<br><br>\n","$\\begin{align} h_0 = UW_e + W_p \\end{align}$\n","<br><br>\n","$h_l=\\text{transformer_block}\\left(h_{l-1}\\right)$\n","<br><br>\n","$P(u)=\\text{softmax}\\left(h_nW_e^T\\right)$\n","<br><br>\n","$W_e$：<font color=\"silver\">word embedding matix<br></font>\n","$W_p$：<font color=\"silver\">position embedding matrix、Transformerの論文のようにsin・cosを使った方法ではなく、データから学習する。"],"metadata":{"id":"YD1FGqCFmQpn"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   3.2 Supervised fine-tuning\n","$P\\left(y|x^1, \\cdots, x^m\\right)=\\text{softmax}\\left(h_l^m W_y\\right)$\n","<br><br>\n","$\\displaystyle L_2\\left(\\mathcal{C}\\right)=\\sum_{(x, y)}\\log P\\left(y|x^1, \\cdots, x^m\\right)$\n","<br><br>\n","$L_3(\\mathcal{C})=L_2(\\mathcal{C})+\\lambda*L_1(\\mathcal{C})$"],"metadata":{"id":"wvjJe-kbmQ5-"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   3.3 Task-specific input transformation"],"metadata":{"id":"zpA8700mmg7o"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   Textual entailment"],"metadata":{"id":"sPaf_68jmhO2"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   Similarity"],"metadata":{"id":"usCzAjVQmhgG"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   Question Answering and Commonsense Reasoning  "],"metadata":{"id":"_XB5KJrImRKW"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   4 Experiments 実験\n"],"metadata":{"id":"2qth9pJlms6C"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   4.1 Setup"],"metadata":{"id":"LbUe5dCYmtP3"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   Unsupervised pre-training"],"metadata":{"id":"0c85svJKm1X3"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   Model specifications\n","GELU, Gaussian Error Linear Unit<br>\n","<br>\n","$\\begin{align} \\text{GELU}(x)&=x\\Phi(x)\\\\ &\\sim0.5x\\left(1+\\tanh\\left[\\sqrt{2/\\pi}\\left(x+0.044715x^3\\right)\\right]\\right) \\end{align}$<br>\n","<br>\n","<img src=\"https://data-analytics.fun/wp-content/uploads/2020/04/GELU.png\" width=\"320\"><br>\n"],"metadata":{"id":"bQyF18Wfm13X"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   Fine-tuning details"],"metadata":{"id":"X-iYemojm74o"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   4.2 Supervised fine-tuning"],"metadata":{"id":"de_amNOCm8JP"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   Natural Language Inference\n","<img src=\"https://data-analytics.fun/wp-content/uploads/2020/04/open-gpt_result1-1024x444.png\" width=\"640\"><br>"],"metadata":{"id":"riUdybsLm_-o"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   Question answering and commonsense reasoning\n","<img src=\"https://data-analytics.fun/wp-content/uploads/2020/04/open-gpt_result2-1024x376.png\" width=\"640\"><br>"],"metadata":{"id":"whtwCqztnAP-"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   Semantic Similarity\n","<img src=\"https://data-analytics.fun/wp-content/uploads/2020/04/open-gpt_result3-1024x481.png\" width=\"640\"><br>"],"metadata":{"id":"PlypquaSnAiZ"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   Classification"],"metadata":{"id":"AiYbHaBgqLhg"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   5 Analysis"],"metadata":{"id":"tFpggFo6qL1N"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   Impact of number of layers transferred"],"metadata":{"id":"PRsVw7JOqMJ1"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   Zero-shot Behaviors"],"metadata":{"id":"XeRPMguQqSWw"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   Ablation studies"],"metadata":{"id":"X3PvWNZfqSrF"}},{"cell_type":"markdown","source":["# <font color=\"silver\">GPT   6 Conclusion 結論"],"metadata":{"id":"e1NqLmCrqYRg"}},{"cell_type":"markdown","source":["# <font color=\"Blue\">■</font><font color=\"silver\">WaveNet [<font color=\"silver\">…](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4718&cid=b0f01606242a6ed3&CT=1666690533933&OR=ItemsView)<br>\n","https://arxiv.org/pdf/1609.03499.pdf\n"],"metadata":{"id":"AiYokRajnRBM"}},{"cell_type":"markdown","source":["# <font color=\"silver\">WaveNet  1 Introduction 背景"],"metadata":{"id":"oLopu_7aoWQI"}},{"cell_type":"markdown","source":["# <font color=\"silver\">WaveNet  2 WaveNet</font>\n","> ※ PixelCNNをベースにした音声波形を生成するためのディープニューラルネットワーク</font><br>\n","${p(\\mathbf{x}) = \\displaystyle \\prod_{t=1}^{T}p(x_{t}\\ |\\ x_{1}, \\cdots, x_{t-1})\n","}$"],"metadata":{"id":"X_ruWZ1hoaY2"}},{"cell_type":"markdown","source":["# <font color=\"silver\">WaveNet  2.1 Dilated Causal Convolutions\n","># <font color=\"silver\">Causal Convolution</font>\n","> ※ 予測分布 $p(x_{t+1} | x_{1}, \\cdots, x_{t})$は将来の時間ステップ $x_{t+1}, x_{t+2}, \\cdots, x_{T}$には依存しない<br>\n","> ※ 将来のデータにマスクしているというmasked convolutionと等価<br>\n","> ※ RNNより学習が速い<br>\n","> ※ 受容体を広くするには、多くの層又は大きなフィルタが必要<br><br>\n","<img src=\"https://storage.googleapis.com/zenn-user-upload/44262da54bfa-20220804.png\" width=\"480\"><br>\n","># <font color=\"silver\"> Dilated Causal Convolution</font>\n","> ※ 層が深くなるにつれて, 畳み込むノードを離す(Dilation)<br>\n","> ※ 1,2,4,8,⋯,512と指数的に大きくしている.  InputからOutputにかけて, Dilation=1,2,4,8Dilation=1,2,4,8となっている<br><br>\n","<img src=\"https://storage.googleapis.com/zenn-user-upload/d30785693173-20220804.png\" width=\"480\"><br>\n","<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://subscription.packtpub.com/book/data/9781789136364/4/ch04lvl1sec59/dilated-and-causal-convolution)</font></font><br>\n","<img src=\"https://static.packt-cdn.com/products/9781789136364/graphics/B10354_04_16.jpg\" width=\"240\">\n","<img src=\"https://static.packt-cdn.com/products/9781789136364/graphics/B10354_04_17.jpg\" width=\"240\">\n","<img src=\"https://static.packt-cdn.com/products/9781789136364/graphics/B10354_04_18.jpg\" width=\"240\">"],"metadata":{"id":"KVxOxI4tobm6"}},{"cell_type":"markdown","source":["# <font color=\"silver\">WaveNet  2.2 Softmax Distribution</font>\n","> ※ $μ$-law companding transformation</font><br>\n","> ※ 音声データは16-bit/tであるため、65,536個の確率の計算が必要で計算量が多い。計算量削減のため、Softmax Layerでは、μ-lowアルゴリズムを用いて256通りに量子化し、生成する音声がどのクラスに属するかという分類問題として生成音声の予測を行う。\n","<br>\n","$f(x_{t}) = \\rm{sign}(x_{t}) \\displaystyle \\frac{\\log \\{1 + \\mu |x_{t}|\\}}{\\log \\{1 + \\mu \\}}$\n","<br><br>\n","$-1 < x_{t} < 1, \\mu = 255$\n"],"metadata":{"id":"e06wODeHob_7"}},{"cell_type":"markdown","source":["# <font color=\"silver\">WaveNet  2.3 Gated Activation Units\n","> ※ ゲートと出力候補には別の重みを用いる<br>\n","<font color=\"black\">$\\mathbf{z} = \\tanh (W_{f, k} * \\mathbf{x}) \\odot \\sigma (W_{g, k} * \\mathbf{x})$<br><br>\n","$*$：<font color=\"silver\">畳み込みの演算</font><br>\n","$k$：<font color=\"silver\">layer index</font><br>\n","$f$：<font color=\"silver\">filter</font><br>\n","$g$：<font color=\"silver\">gate</font><br>\n","$σ$：<font color=\"silver\">sigmoid関数</font>\n","<br><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F218720%2F081f61bb-6bc2-f891-36b0-26f3b4df6e16.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=b093519e6fa1320dca152e8beaff1ad7\" width=\"480\"><br>\n"],"metadata":{"id":"uWslJqWwocVT"}},{"cell_type":"markdown","source":["# <font color=\"silver\">WaveNet  2.4 Residual and skip connections\n","> ※ Residual and skip connectionss<br>\n","> ※ 1x1 Convolutionsでチャンネル数を調節する\n"],"metadata":{"id":"JxgGhqrXocrk"}},{"cell_type":"markdown","source":["# <font color=\"silver\">WaveNet  2.5 Conditional WaveNet\n","> ※ 自己回帰モデルに生成された音声の特徴を特定することを目的とするインプット$\\mathbf{h}$を加える。<br>\n","> ※ 例えば, 複数の話し手の音声が含まれる音声データセットについてインプット$\\mathbf{h}$として話し手の特徴をモデルに加えることによって、その複数の話し手の中から特定の話し手を選択することができる<br>\n","<font color=\"black\">$p(\\mathbf{x}\\ |\\ \\mathbf{h}) = \\displaystyle \\prod_{t=1}^{T}p(x_{t}\\ | \\ x_{1}, \\cdots, x_{t-1}, \\mathbf{h})$\n","># <font color=\"silver\">Global conditioning\n",">$\\mathbf{z} = \\tanh (W_{f, k} * \\mathbf{x} + V_{f, k}^{T}\\mathbf{h}) \\odot \\sigma (W_{g, k} * \\mathbf{x} + V_{g, k}^{T}\\mathbf{h})$\n","># <font color=\"silver\">Local conditioning\n",">$\\mathbf{z} = \\tanh (W_{f, k} * \\mathbf{x} + V_{f, k}^{T} * f(\\mathbf{h})) \\odot \\sigma (W_{g, k} * \\mathbf{x} + V_{g, k}^{T} * f(\\mathbf{h}))$"],"metadata":{"id":"ZuiKpVpfodBU"}},{"cell_type":"markdown","source":["# <font color=\"silver\">WaveNet  2.6 Context Stack"],"metadata":{"id":"sHH_FvoGozbP"}},{"cell_type":"markdown","source":["# <font color=\"silver\">WaveNet  3 Experiments 実験"],"metadata":{"id":"Kc0kFp2xpvJu"}},{"cell_type":"markdown","source":["# <font color=\"silver\">WaveNet  4 Conclusion 結論"],"metadata":{"id":"dM1jI4mkp27m"}}]}