{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/group-nai-shomu/00/blob/main/13DL.ipynb","timestamp":1682401920102}],"collapsed_sections":["bQDQj7XOQggp"],"toc_visible":true,"authorship_tag":"ABX9TyNsA+Lqdp7kbcuG9MJhRHj0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"SodkKgcYM5gm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <font color=\"silver\">実装│TwoLayerNet</font>\n","<font color=\"silver\"># ❶ 重みの初期値をdictで定義</font><br>\n","<font color=\"silver\"># ❷ 隠れ層をdictで定義</font><br>\n","<font color=\"silver\"># ❸ 出力層を定義</font><br>\n","<font color=\"silver\"># ❹ 推論値算出（batch size, DataLoader）</font><br>\n","<font color=\"silver\"># ❺ 損失値算出（epoch, network size）</font><br>\n","<font color=\"silver\"># ❻ 勾配値算出</font><br>\n","<font color=\"silver\"># ❼ 勾配値をdictでまとめる</font><br><br>\n","class TwoLayerNet:<br>\n","$\\qquad$def __ init __ (<br>\n","$\\qquad$$\\qquad$self, <br>\n","$\\qquad$$\\qquad$input_size, <br>\n","$\\qquad$$\\qquad$hidden_size, <br>\n","$\\qquad$$\\qquad$output_size, <br>\n","$\\qquad$$\\qquad$weight_init_std = 0.01<br>\n","$\\qquad$$\\qquad$):<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❶ 重みの初期値をdictで定義</font><br>\n","$\\qquad$$\\qquad$self.params = {}<br>\n","$\\qquad$$\\qquad$self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)<br>\n","$\\qquad$$\\qquad$self.params['b1'] = np.zeros(hidden_size)<br>\n","$\\qquad$$\\qquad$self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) <br>\n","$\\qquad$$\\qquad$self.params['b2'] = np.zeros(output_size)<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❷ 隠れ層をdictで定義</font><br>\n","$\\qquad$$\\qquad$self.layers = OrderedDict()<br>\n","$\\qquad$$\\qquad$self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])<br>\n","$\\qquad$$\\qquad$self.layers['Relu1'] = Relu()<br>\n","$\\qquad$$\\qquad$self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❸ 出力層を定義</font><br>\n","$\\qquad$$\\qquad$self.lastLayer = SoftmaxWithLoss()<br>\n","$\\qquad$<font color=\"silver\"># ❹ 推論値算出（batch size, DataLoader）</font><br>\n","$\\qquad$def predict(self, x):<br>\n","$\\qquad$$\\qquad$for layer in self.layers.values():<br>\n","$\\qquad$$\\qquad$$\\qquad$x = layer.forward(x)<br>\n","$\\qquad$$\\qquad$return x<br>\n","$\\qquad$<font color=\"silver\"># ❺ 損失値算出（epoch, network size）</font><br>\n","$\\qquad$def loss(self, x, t):<br>\n","$\\qquad$$\\qquad$y = self.predict(x)<br>\n","$\\qquad$$\\qquad$return self.lastLayer.forward(y, t)<br>\n","$\\qquad$<font color=\"silver\"># 正解率算出 </font><br>\n","$\\qquad$def accuracy(self, x, t):<br>\n","$\\qquad$$\\qquad$y = self.predict(x)<br>\n","$\\qquad$$\\qquad$y = np.argmax(y, axis=1)<br>\n","$\\qquad$$\\qquad$if t.ndim != 1 : t = np.argmax(t, axis=1)<br>\n","$\\qquad$$\\qquad$accuracy = np.sum(y == t) / float(x.shape[0])<br>\n","$\\qquad$$\\qquad$return accuracy<br>\n","$\\qquad$<font color=\"silver\"># 数値微分の勾配算出</font><br>\n","$\\qquad$def numerical_gradient(self, x, t):<br>\n","$\\qquad$$\\qquad$loss_W = lambda W: self.loss(x, t)<br>\n","$\\qquad$$\\qquad$grads = {}<br>\n","$\\qquad$$\\qquad$grads['W1'] = numerical_gradient(loss_W, self.params['W1'])<br>\n","$\\qquad$$\\qquad$grads['b1'] = numerical_gradient(loss_W, self.params['b1'])<br>\n","$\\qquad$$\\qquad$grads['W2'] = numerical_gradient(loss_W, self.params['W2'])<br>\n","$\\qquad$$\\qquad$grads['b2'] = numerical_gradient(loss_W, self.params['b2'])<br>\n","$\\qquad$$\\qquad$return grads<br>\n","$\\qquad$<font color=\"silver\"># ❻ 勾配値算出</font><br>\n","$\\qquad$def gradient(self, x, t):<br>\n","$\\qquad$$\\qquad$self.loss(x, t)<br>\n","$\\qquad$$\\qquad$dout = 1<br>\n","$\\qquad$$\\qquad$dout = self.lastLayer.backward(dout)<br>\n","$\\qquad$$\\qquad$layers = list(self.layers.values())<br>\n","$\\qquad$$\\qquad$layers.reverse()<br>\n","$\\qquad$$\\qquad$for layer in layers:<br>\n","$\\qquad$$\\qquad$$\\qquad$dout = layer.backward(dout)<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❼ 勾配値をdictでまとめる</font><br>\n","$\\qquad$$\\qquad$grads = {}<br>\n","$\\qquad$$\\qquad$grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db<br>\n","$\\qquad$$\\qquad$grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db<br>\n","$\\qquad$$\\qquad$return grads<br>"],"metadata":{"id":"zeltQoK5-YLa"}},{"cell_type":"markdown","source":["# <font color=\"silver\">実装│TwoLayerNet with WeightDecay</font>\n","<font color=\"silver\"># ❶ 重みの初期値をdictで定義</font><br>\n","<font color=\"silver\"># ❷ 隠れ層をdictで定義</font><br>\n","<font color=\"silver\"># ❸ 出力層を定義</font><br>\n","<font color=\"silver\"># ❹ 推論値算出（batch size, DataLoader）</font><br>\n","<font color=\"silver\"># ❺ 損失値算出（epoch, network size）</font><br>\n","<font color=\"silver\"># ❻ 勾配値算出</font><br>\n","<font color=\"silver\"># ❼ 勾配値をdictでまとめる</font><br><br>\n","class TwoLayerNet:<br>\n","$\\qquad$def __ init __ (self, input_size, hidden_size, output_size, <br>\n","$\\qquad$$\\qquad$$\\qquad$weight_init_std = 0.01, weight_decay_lambda=0.01):<br>\n","$\\qquad$$\\qquad$self.hidden_layer_num = 1<br>\n","$\\qquad$$\\qquad$self.weight_decay_lambda = weight_decay_lambda<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❶ 重みの初期値をdictで定義</font><br>\n","$\\qquad$$\\qquad$self.params = {}<br>\n","$\\qquad$$\\qquad$np.random.seed(1111)<br>\n","$\\qquad$$\\qquad$self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)<br>\n","$\\qquad$$\\qquad$self.params['b1'] = np.zeros(hidden_size)<br>\n","$\\qquad$$\\qquad$self.params['W2'] = weight_init_std * np.random.randn(hidden_size,output_size) <br>\n","$\\qquad$$\\qquad$self.params['b2'] = np.zeros(output_size)<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❷ 隠れ層をdictで定義</font><br>\n","$\\qquad$$\\qquad$self.layers = OrderedDict() <br>\n","$\\qquad$$\\qquad$self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])<br>\n","$\\qquad$$\\qquad$self.layers['Relu1'] =  ReLU()<br>\n","$\\qquad$$\\qquad$self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❸ 出力層を定義</font><br>\n","$\\qquad$$\\qquad$self.lastLayer = MeanSquaredLoss()<br>\n","$\\qquad$<font color=\"silver\"># ❹ 推論値算出（batch size, DataLoader）</font><br>\n","$\\qquad$def predict(self, x):<br>\n","$\\qquad$$\\qquad$for layer in self.layers.values():<br>\n","$\\qquad$$\\qquad$$\\qquad$x = layer.forward(x)<br>\n","$\\qquad$$\\qquad$return x<br>\n","$\\qquad$<font color=\"silver\"># ❺ 損失値算出（epoch, network size）</font><br>\n","$\\qquad$def loss(self, x, t):<br>\n","$\\qquad$$\\qquad$y = self.predict(x)<br>\n","$\\qquad$$\\qquad$lmd = self.weight_decay_lambda<br>\n","$\\qquad$$\\qquad$weight_decay = 0<br>\n","$\\qquad$$\\qquad$for idx in range(1, self.hidden_layer_num + 2):<br>\n","$\\qquad$$\\qquad$$\\qquad$W = self.params['W' + str(idx)]<br>\n","$\\qquad$$\\qquad$$\\qquad$weight_decay += 0.5 * lmd * np.sum(W**2) <font color=\"silver\"> # 全ての行列Wについて積算していく</font><br>\n","$\\qquad$$\\qquad$return self.lastLayer.forward(y, t) + weight_decay <br>\n","$\\qquad$<font color=\"silver\"># ❻ 勾配値算出</font><br>\n","$\\qquad$def gradient(self, x, t):<br>\n","$\\qquad$$\\qquad$self.loss(x, t)<br>\n","$\\qquad$$\\qquad$dout = self.lastLayer.backward(dout=1) <br>\n","$\\qquad$$\\qquad$layers = list(self.layers.values())<br>\n","$\\qquad$$\\qquad$layers.reverse()<br>\n","$\\qquad$$\\qquad$for layer in layers:<br>\n","$\\qquad$$\\qquad$$\\qquad$dout = layer.backward(dout)<br>\n","$\\qquad$$\\qquad$lmd = self.weight_decay_lambda<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❼ 勾配値をdictでまとめる</font><br>\n","$\\qquad$$\\qquad$grads = {}<br>\n","$\\qquad$$\\qquad$for idx in range(1, self.hidden_layer_num+2):<br>\n","$\\qquad$$\\qquad$$\\qquad$grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + lmd * self.layers['Affine' + str(idx)].W<br>\n","$\\qquad$$\\qquad$$\\qquad$grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db<br>\n","$\\qquad$$\\qquad$return grads<br>"],"metadata":{"id":"TvoJDmed-j-Y"}},{"cell_type":"markdown","source":["# <font color=\"silver\">実装│MultiLayerNet</font><br>\n","[<font color=\"Blue\">$\\tiny{\\rm Link}$…</font>](https://drive.google.com/drive/folders/1T6fLWYPPiN47bhUMUNmK3F8avjNnqJCY)</font></font><br>\n","input_size=784 <font color=\"silver\"> # 入力は28 × 28 =784の画像データ</font><br>\n","output_size=10 <font color=\"silver\"> # 出力は０～９のどれであるかの予測確率</font><br>\n","hidden_size_list=[100, 100, 100, 100] <font color=\"silver\"> # 隠れ層が４層あり、ニューロンの数が各１００個</font><br>\n","batch_size =128<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/slow_learner/items/28a8025c9b881491e3be)</font></font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F686010%2F132cfd43-e3a5-b406-b181-d10f6c73668b.jpeg?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=e1b0958b579595ada283bac098c6869b\" width=\"640\"><br><br>\n","<font color=\"silver\"># ❶ 重みの初期値をdictで定義</font><br>\n","<font color=\"silver\"># ❷ 隠れ層をdictで定義</font><br>\n","<font color=\"silver\"># ❸ 出力層を定義</font><br>\n","<font color=\"silver\"># ❹ 推論値算出（batch size, DataLoader）</font><br>\n","<font color=\"silver\"># ❺ 損失値算出（epoch, network size）</font><br>\n","<font color=\"silver\"># ❻ 勾配値算出</font><br>\n","<font color=\"silver\"># ❼ 勾配値をdictでまとめる</font><br><br>\n","class MultiLayerNet:<br>\n","$\\qquad$def __ init __ (<br>\n","$\\qquad$$\\qquad$self, <br>\n","$\\qquad$$\\qquad$input_size, <br>\n","$\\qquad$$\\qquad$hidden_size_list, <br>\n","$\\qquad$$\\qquad$output_size,<br>\n","$\\qquad$$\\qquad$activation='relu', <br>\n","$\\qquad$$\\qquad$weight_init_std='relu', <br>\n","$\\qquad$$\\qquad$weight_decay_lambda=0<br>\n","$\\qquad$$\\qquad$):<br>\n","$\\qquad$$\\qquad$self.input_size = input_size<br>\n","$\\qquad$$\\qquad$self.output_size = output_size<br>\n","$\\qquad$$\\qquad$self.hidden_size_list = hidden_size_list<br>\n","$\\qquad$$\\qquad$self.hidden_layer_num = len(hidden_size_list)<br>\n","$\\qquad$$\\qquad$self.weight_decay_lambda = weight_decay_lambda<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❶ 重みの初期値をdictで定義</font><br>\n","$\\qquad$$\\qquad$self.params = {}<br>\n","$\\qquad$$\\qquad$self.__init_weight(weight_init_std)<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❷ 隠れ層をdictで定義</font><br>\n","$\\qquad$$\\qquad$activation_layer = {'sigmoid': Sigmoid, 'relu': Relu}<br>\n","$\\qquad$$\\qquad$self.layers = OrderedDict()<br>\n","$\\qquad$$\\qquad$for idx in range(1, self.hidden_layer_num+1):<br>\n","$\\qquad$$\\qquad$$\\qquad$self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])<br>\n","$\\qquad$$\\qquad$$\\qquad$self.layers['Activation_function' + str(idx)] = activation_layer[activation] ()<br>\n","$\\qquad$$\\qquad$idx = self.hidden_layer_num + 1<br>\n","$\\qquad$$\\qquad$self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❸ 出力層を定義</font><br>\n","$\\qquad$$\\qquad$self.last_layer = SoftmaxWithLoss()<br>\n","$\\qquad$<font color=\"silver\"># パラメータ初期化する関数</font><br>\n","$\\qquad$def __ init_weight(self, weight_init_std):<br>\n","$\\qquad$$\\qquad$all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]<br>\n","$\\qquad$$\\qquad$for idx in range(1, len(all_size_list)):<br>\n","$\\qquad$$\\qquad$$\\qquad$scale = weight_init_std<br>\n","$\\qquad$$\\qquad$$\\qquad$if str(weight_init_std).lower() in ('relu', 'he'):<br>\n","$\\qquad$$\\qquad$$\\qquad$$\\qquad$scale = np.sqrt(2.0 / all_size_list[idx - 1])<br>\n","$\\qquad$$\\qquad$$\\qquad$elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):<br>\n","$\\qquad$$\\qquad$$\\qquad$$\\qquad$scale = np.sqrt(1.0 / all_size_list[idx - 1])<br>\n","$\\qquad$$\\qquad$$\\qquad$self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])<br>\n","$\\qquad$$\\qquad$$\\qquad$self.params['b' + str(idx)] = np.zeros(all_size_list[idx])<br>\n","$\\qquad$<font color=\"silver\"># ❹ 推論値算出（batch size, DataLoader）</font><br>\n","$\\qquad$def predict(self, x):<br>\n","$\\qquad$$\\qquad$for layer in self.layers.values():<br>\n","$\\qquad$$\\qquad$$\\qquad$x = layer.forward(x)<br>\n","$\\qquad$$\\qquad$return x<br>\n","$\\qquad$<font color=\"silver\"># ❺ 損失値算出（epoch, network size）</font><br>\n","$\\qquad$def loss(self, x, t):<br>\n","$\\qquad$$\\qquad$y = self.predict(x)<br>\n","$\\qquad$$\\qquad$weight_decay = 0<br>\n","$\\qquad$$\\qquad$for idx in range(1, self.hidden_layer_num + 2):<br>\n","$\\qquad$$\\qquad$$\\qquad$W = self.params['W' + str(idx)]<br>\n","$\\qquad$$\\qquad$$\\qquad$weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W ** 2)<br>\n","$\\qquad$$\\qquad$return self.last_layer.forward(y, t) + weight_decay<br>\n","$\\qquad$<font color=\"silver\"># ❻ 勾配値算出</font><br>\n","$\\qquad$def gradient(self, x, t):<br>\n","$\\qquad$$\\qquad$self.loss(x, t)<br>\n","$\\qquad$$\\qquad$dout = 1<br>\n","$\\qquad$$\\qquad$dout = self.last_layer.backward(dout)<br>\n","$\\qquad$$\\qquad$layers = list(self.layers.values())<br>\n","$\\qquad$$\\qquad$layers.reverse()<br>\n","$\\qquad$$\\qquad$for layer in layers:<br>\n","$\\qquad$$\\qquad$$\\qquad$dout = layer.backward(dout)<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❼ 勾配値をdictでまとめる</font><br>\n","$\\qquad$$\\qquad$grads = {}<br>\n","$\\qquad$$\\qquad$for idx in range(1, self.hidden_layer_num+2):<br>\n","$\\qquad$$\\qquad$$\\qquad$grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.layers['Affine' + str(idx)].W<br>\n","$\\qquad$$\\qquad$$\\qquad$grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db<br>\n","$\\qquad$$\\qquad$return grads<br>"],"metadata":{"id":"40sTq_AmMwQN"}},{"cell_type":"markdown","source":["# <font color=\"silver\">実装│SimpleConv</font><br>\n","<font color=\"silver\"># ❶ 重みの初期値をdictで定義</font><br>\n","<font color=\"silver\"># ❷ 隠れ層をdictで定義</font><br>\n","<font color=\"silver\"># ❸ 出力層を定義</font><br>\n","<font color=\"silver\"># ❹ 推論値算出（batch size, DataLoader）</font><br>\n","<font color=\"silver\"># ❺ 損失値算出（epoch, network size）</font><br>\n","<font color=\"silver\"># ❻ 勾配値算出</font><br>\n","<font color=\"silver\"># ❼ 勾配値をdictでまとめる</font><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/slow_learner/items/28a8025c9b881491e3be)</font></font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F686010%2F175c93a8-0e5d-6318-7762-68fe3497ac60.jpeg?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=1fecb08b6886622a0e2f13ca5abf6218\" width=\"480\"><br><br>\n","class SimpleConvNet:<br>\n","$\\qquad$def __ init __(self, <br>\n","$\\qquad$$\\qquad$$\\qquad$input_dim=(1, 28, 28), <br>\n","$\\qquad$$\\qquad$$\\qquad$conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},<br>\n","$\\qquad$$\\qquad$$\\qquad$pool_param={'pool_size':2, 'pad':0, 'stride':2},<br>\n","$\\qquad$$\\qquad$$\\qquad$hidden_size=100,<br>\n","$\\qquad$$\\qquad$$\\qquad$output_size=10,<br>\n","$\\qquad$$\\qquad$$\\qquad$weight_init_std=0.01,<br>\n","$\\qquad$$\\qquad$$\\qquad$weight_decay_lambda=0.01<br>\n","$\\qquad$$\\qquad$$\\qquad$):<br>\n","$\\qquad$$\\qquad$filter_num = conv_param['filter_num']<br>\n","$\\qquad$$\\qquad$filter_size = conv_param['filter_size']<br>\n","$\\qquad$$\\qquad$filter_pad = conv_param['pad']<br>\n","$\\qquad$$\\qquad$filter_stride = conv_param['stride']<br>\n","$\\qquad$$\\qquad$pool_size = pool_param['pool_size']<br>\n","$\\qquad$$\\qquad$pool_pad = pool_param['pad']<br>\n","$\\qquad$$\\qquad$pool_stride = pool_param['stride']<br>\n","$\\qquad$$\\qquad$input_size = input_dim[1]<br>\n","$\\qquad$$\\qquad$conv_output_size = (input_size + 2 * filter_pad - filter_size) // filter_stride + 1 <font color=\"silver\"> # ConvのOH, OW<br></font>\n","$\\qquad$$\\qquad$pool_output_size = (conv_output_size + 2 * pool_pad - pool_size) // pool_stride + 1 <font color=\"silver\"> # PoolのOH, OW<br></font>\n","$\\qquad$$\\qquad$pool_output_pixel = filter_num * pool_output_size * pool_output_size <font color=\"silver\"> # Poolのフラットのサイズ<br></font>\n","$\\qquad$$\\qquad$self.weight_decay_lambda = weight_decay_lambda<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❶ 重みの初期値をdictで定義</font><br>\n","$\\qquad$$\\qquad$self.params = {}<br>\n","$\\qquad$$\\qquad$std = weight_init_std<br>\n","$\\qquad$$\\qquad$self.params['W1'] = std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)<br>\n","$\\qquad$$\\qquad$self.params['b1'] = np.zeros(filter_num)<br>\n","$\\qquad$$\\qquad$self.params['W2'] = std *  np.random.randn(pool_output_pixel, hidden_size)<br>\n","$\\qquad$$\\qquad$self.params['b2'] = np.zeros(hidden_size)<br>\n","$\\qquad$$\\qquad$self.params['W3'] = std *  np.random.randn(hidden_size, output_size)<br>\n","$\\qquad$$\\qquad$self.params['b3'] = np.zeros(output_size)<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❷ 隠れ層をdictで定義</font><br>\n","$\\qquad$$\\qquad$self.layers = OrderedDict()<br>\n","$\\qquad$$\\qquad$self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], conv_param['stride'], conv_param['pad'])<br>\n","$\\qquad$$\\qquad$self.layers['ReLU1'] = ReLU()<br>\n","$\\qquad$$\\qquad$self.layers['Pool1'] = MaxPooling(pool_h=pool_size, pool_w=pool_size, stride=pool_stride, pad=pool_pad)<br>\n","$\\qquad$$\\qquad$self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])<br>\n","$\\qquad$$\\qquad$self.layers['ReLU2'] = ReLU()<br>\n","$\\qquad$$\\qquad$self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❸ 出力層を定義</font><br>\n","$\\qquad$$\\qquad$self.last_layer = SoftmaxWithLoss()<br>\n","$\\qquad$<font color=\"silver\"># ❹ 推論値算出（batch size, DataLoader）</font><br>\n","$\\qquad$def predict(self, x):<br>\n","$\\qquad$$\\qquad$for layer in self.layers.values():<br>\n","$\\qquad$$\\qquad$$\\qquad$x = layer.forward(x)<br>\n","$\\qquad$$\\qquad$return x<br>\n","$\\qquad$<font color=\"silver\"># ❺ 損失値算出（epoch, network size）</font><br>\n","$\\qquad$def loss(self, x, t):<br>\n","$\\qquad$$\\qquad$y = self.predict(x)<br>\n","$\\qquad$$\\qquad$weight_decay = 0<br>\n","$\\qquad$$\\qquad$for idx in range(1, self.hidden_layer_num + 2):<br>\n","$\\qquad$$\\qquad$$\\qquad$W = self.params['W' + str(idx)]<br>\n","$\\qquad$$\\qquad$$\\qquad$weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W ** 2)<br>\n","$\\qquad$$\\qquad$return self.last_layer.forward(y, t) + weight_decay<br>\n","$\\qquad$def accuracy(self, x, t, batch_size=100):<br>\n","$\\qquad$$\\qquad$if t.ndim != 1 : t = np.argmax(t, axis=1)<br>\n","$\\qquad$$\\qquad$acc = 0.0<br>\n","$\\qquad$$\\qquad$for i in range(int(x.shape[0] / batch_size)):<br>\n","$\\qquad$$\\qquad$$\\qquad$tx = x[i*batch_size:(i+1)*batch_size]<br>\n","$\\qquad$$\\qquad$$\\qquad$tt = t[i*batch_size:(i+1)*batch_size]<br>\n","$\\qquad$$\\qquad$$\\qquad$y = self.predict(tx)<br>\n","$\\qquad$$\\qquad$$\\qquad$y = np.argmax(y, axis=1)<br>\n","$\\qquad$$\\qquad$$\\qquad$acc += np.sum(y == tt) <br>\n","$\\qquad$$\\qquad$return acc / x.shape[0]<br>\n","$\\qquad$<font color=\"silver\"># ❻ 勾配値算出</font><br>\n","$\\qquad$def gradient(self, x, t):<br>\n","$\\qquad$$\\qquad$self.loss(x, t)<br>\n","$\\qquad$$\\qquad$dout = 1<br>\n","$\\qquad$$\\qquad$dout = self.last_layer.backward(dout)<br>\n","$\\qquad$$\\qquad$layers = list(self.layers.values())<br>\n","$\\qquad$$\\qquad$layers.reverse()<br>\n","$\\qquad$$\\qquad$for layer in layers:<br>\n","$\\qquad$$\\qquad$$\\qquad$dout = layer.backward(dout)<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❼ 勾配値をdictでまとめる</font><br>\n","$\\qquad$$\\qquad$grads = {}<br>\n","$\\qquad$$\\qquad$grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db<br>\n","$\\qquad$$\\qquad$grads['W2'], grads['b2'] = self.layers['Affine1'].dW + self.weight_decay_lambda * self.layers['Affine1'].W, self.layers['Affine1'].db<br>\n","$\\qquad$$\\qquad$grads['W3'], grads['b3'] = self.layers['Affine2'].dW + self.weight_decay_lambda * self.layers['Affine2'].W, self.layers['Affine2'].db<br>\n","$\\qquad$$\\qquad$return grads<br>\n"],"metadata":{"id":"Qct1lYN3NxVH"}},{"cell_type":"markdown","source":["# <font color=\"silver\">実装│PytorchCNN</font><br>\n","torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)<br>\n","torch.nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True, device=None, dtype=None)<br>\n","torch.nn.InstanceNorm2d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False, device=None, dtype=None)<br>\n","torch.nn.GroupNorm(num_groups, num_channels, eps=1e-05, affine=True, device=None, dtype=None)<br><br>\n","torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, …)<br>\n","torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1,…)<br>\n","torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)<br>\n","torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://rightcode.co.jp/blog/information-technology/pytorch-cifar-10-cnn-learning)</font></font><br>\n","<img src=\"https://rightcode.co.jp/wp-content/uploads/2020/02/pytorch_lenet.png\" width=\"640\"><br><br>\n","class LeNet(torch.nn.Module):<br>\n","$\\qquad$def __ init __ (self):<br>\n","$\\qquad$$\\qquad$super(MyCNN, self). __ init __ ()<br>\n","$\\qquad$$\\qquad$self.conv1 = torch.nn.Conv2d(3, 6, 5) <font color=\"silver\"> # (in_channels, out_channels, kernel, stride=1)</font><br>\n","$\\qquad$$\\qquad$self.conv2 = torch.nn.Conv2d(6, 16, 5)<br>\n","$\\qquad$$\\qquad$self.pool = torch.nn.MaxPool2d(2, 2) <font color=\"silver\"> # (kernel,  stride)</font><br>\n","$\\qquad$$\\qquad$self.fc1 = torch.nn.Linear (16 * 5 * 5, 120) <font color=\"silver\"> # (in_features, out_features)</font><br>\n","$\\qquad$$\\qquad$self.fc2 = torch.nn.Linear(120, 84)<br>\n","$\\qquad$$\\qquad$self.fc3 = torch.nn.Linear(84, 10) <br>\n","$\\qquad$def forward(self, x):<br>\n","$\\qquad$$\\qquad$x = f.relu(self.conv1(x))<br>\n","$\\qquad$$\\qquad$x = self.pool(x)<br>\n","$\\qquad$$\\qquad$x = f.relu(self.conv2(x))<br>\n","$\\qquad$$\\qquad$x = self.pool(x)<br>\n","$\\qquad$$\\qquad$x = x.view(-1, 16 * 5 * 5) <br>\n","$\\qquad$$\\qquad$x = f.relu(self.fc1(x))<br>\n","$\\qquad$$\\qquad$x = f.relu(self.fc2(x))<br>\n","$\\qquad$$\\qquad$x = self.fc3(x) <br>\n","$\\qquad$$\\qquad$return x<br>\n","class CNN(nn.Module):<br>\n","$\\qquad$def __ init __ (self):<br>\n","$\\qquad$$\\qquad$super(Net, self).__ init __ ()<br>\n","$\\qquad$$\\qquad$self.relu = nn.ReLU()<br>\n","$\\qquad$$\\qquad$self.pool = nn.MaxPool2d(2, stride=2)<br>\n","$\\qquad$$\\qquad$self.conv1 = nn.Conv2d(1,16,3) <font color=\"silver\">  # パラメータ数, 1x16x3x3+16=144+16</font><br>\n","$\\qquad$$\\qquad$self.conv2 = nn.Conv2d(16,32,3) <font color=\"silver\">  # 32x16x3x3+32x16=4608+512</font><br>\n","$\\qquad$$\\qquad$self.fc1 = nn.Linear(32 * 5 * 5, 120) <font color=\"silver\">  # 32x5x5x120</font><br>\n","$\\qquad$def forward(self, x):<br>\n","$\\qquad$$\\qquad$x = self.conv1(x)<br>\n","$\\qquad$$\\qquad$x = self.relu(x)<br>\n","$\\qquad$$\\qquad$x = self.pool(x)<br>\n","$\\qquad$$\\qquad$x = self.conv2(x)<br>\n","$\\qquad$$\\qquad$x = self.relu(x)<br>\n","$\\qquad$$\\qquad$x = self.pool(x)<br>\n","$\\qquad$$\\qquad$x = x.view(x.size()[0], -1)<br>\n","$\\qquad$$\\qquad$x = self.fc1(x)<br>\n","$\\qquad$$\\qquad$x = self.relu(x)<br>\n","$\\qquad$$\\qquad$x = self.fc2(x)<br>\n","$\\qquad$$\\qquad$return x<br>"],"metadata":{"id":"4l1y98aspQN9"}},{"cell_type":"markdown","source":["# <font color=\"silver\">実装│Pytorch画像認識</font><br>\n","<font color=\"silver\"># ❶ データセットの準備</font><br>\n","<font color=\"silver\"># ❷ ネットワークの準備</font><br>\n","<font color=\"silver\"># ❸ モデルの準備（Model, Criterion, Optimizer）</font><br>\n","<font color=\"silver\"># ❹ 学習の準備（batch size, DataLoader）</font><br>\n","<font color=\"silver\"># ❺ 学習（epoch, network size）</font><br>\n","<font color=\"silver\"># ❻ 検証</font><br><br>\n","import numpy as np<br>\n","import matplotlib.pyplot as plt<br>\n","import torch<br>\n","import torch.nn as nn<br>\n","import torchvision<br>\n","import torchvision.transforms as transforms<br>\n","from torchvision import models<br>\n","use_cuda = torch.cuda.is_available()<br>\n","<font color=\"silver\"># ❶ データセットの準備</font><br>\n","transform_train = transforms.Compose([<br>\n","$\\qquad$transforms.RandomCrop(32, padding=1),<br>\n","$\\qquad$transforms.RandomHorizontalFlip(),<br>\n","$\\qquad$transforms.ToTensor()<br>\n","$\\qquad$])<br>\n","transform_test = transforms.Compose([transforms.ToTensor()])<br>\n","train_data = torchvision.datasets.CIFAR10(<br>\n","$\\qquad$root=\"./\", <br>\n","$\\qquad$train=True, <br>\n","$\\qquad$transform=transform_train, <br>\n","$\\qquad$download=True<br>\n","$\\qquad$)<br>\n","test_data = torchvision.datasets.CIFAR10(<br>\n","$\\qquad$root=\"./\", <br>\n","$\\qquad$train=False, <br>\n","$\\qquad$transform=transform_test, <br>\n","$\\qquad$download=True<br>\n","$\\qquad$)<br>\n","<font color=\"silver\"># ❷ ネットワークの準備</font><br>\n","class CNN(nn.Module):<br>\n","$\\qquad$def  __ init __ (self):<br>\n","$\\qquad$$\\qquad$super(). __ init __ ()<br>\n","$\\qquad$$\\qquad$self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)<br>\n","$\\qquad$$\\qquad$self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)<br>\n","$\\qquad$$\\qquad$self.l1 = nn.Linear(8 * 8 * 32, 1024)<br>\n","$\\qquad$$\\qquad$self.l2 = nn.Linear(1024, 1024)<br>\n","$\\qquad$$\\qquad$self.l3 = nn.Linear(1024, 10)<br>\n","$\\qquad$$\\qquad$self.act = nn.ReLU()<br>\n","$\\qquad$$\\qquad$self.pool = nn.MaxPool2d(2, 2)$\\qquad$<br>\n","$\\qquad$def forward(self, x):<br>\n","$\\qquad$$\\qquad$h = self.pool(self.act(self.conv1(x)))<br>\n","$\\qquad$$\\qquad$h = self.pool(self.act(self.conv2(h)))<br>\n","$\\qquad$$\\qquad$h = h.view(h.size()[0], -1)<br>\n","$\\qquad$$\\qquad$h = self.act(self.l1(h))<br>\n","$\\qquad$$\\qquad$h = self.act(self.l2(h))<br>\n","$\\qquad$$\\qquad$h = self.l3(h)<br>\n","$\\qquad$$\\qquad$return h<br>\n","<font color=\"silver\"># ❸ モデルの準備（Model, Criterion, Optimizer）</font><br>\n","model = CNN()<br>\n","<font color=\"silver\"># model = models.resnet18()</font><br>\n","if use_cuda:<br>\n","$\\qquad$model.cuda()<br>\n","criterion = nn.CrossEntropyLoss()<br>\n","if use_cuda:<br>\n","$\\qquad$criterion.cuda()<br>\n","optimizer = torch.optim.SGD(<br>\n","$\\qquad$model.parameters(), <br>\n","$\\qquad$lr=0.01, <br>\n","$\\qquad$momentum=0.9<br>\n","$\\qquad$)<br>\n","<font color=\"silver\"># ❹ 学習の準備（batch size, DataLoader）</font><br>\n","batch_size = 128<br>\n","train_loader = torch.utils.data.DataLoader(<br>\n","$\\qquad$train_data,<br>\n","$\\qquad$batch_size=batch_size,<br>\n","$\\qquad$shuffle=True,<br>\n","$\\qquad$num_workers=2<br>\n","$\\qquad$)<br>\n","test_loader = torch.utils.data.DataLoader(<br>\n","$\\qquad$test_data,<br>\n","$\\qquad$batch_size=100,<br>\n","$\\qquad$shuffle=False<br>\n","$\\qquad$)<br>\n","<font color=\"silver\"># ❺ 学習（epoch, network size）</font><br>\n","epoch_num = 10<br>\n","n_iter = len(train_data) / batch_size<br>\n","model.train()<br>\n","start = time()<br>\n","for epoch in range(1, epoch_num+1):<br>\n","$\\qquad$sum_loss = 0.0<br>\n","$\\qquad$count = 0$\\qquad$<br>\n","$\\qquad$for image, label in train_loader:<br>\n","$\\qquad$$\\qquad$if use_cuda:<br>\n","$\\qquad$$\\qquad$$\\qquad$image = image.cuda()<br>\n","$\\qquad$$\\qquad$$\\qquad$label = label.cuda()<br>\n","$\\qquad$$\\qquad$y = model(image)<br>\n","$\\qquad$$\\qquad$loss = criterion(y, label)  <font color=\"silver\"># 順伝播</font><br>\n","$\\qquad$$\\qquad$l2_loss = torch.tensor(0., requires_grad=True)<br>\n","$\\qquad$$\\qquad$for w in model.parameters():<br>\n","$\\qquad$$\\qquad$$\\qquad$l2_loss = l2_loss + torch.norm(w) ** 2<br>\n","$\\qquad$$\\qquad$loss = loss + 1e-3 * l2_loss  <font color=\"silver\"># 1e-3は正則化係数</font><br>\n","$\\qquad$$\\qquad$model.zero_grad()  <font color=\"silver\"># 勾配の初期化</font><br>\n","$\\qquad$$\\qquad$loss.backward()  <font color=\"silver\"># 逆伝播</font><br>\n","$\\qquad$$\\qquad$optimizer.step()  <font color=\"silver\"># パラメータ更新</font><br>\n","$\\qquad$$\\qquad$sum_loss += loss.item()<br>\n","$\\qquad$$\\qquad$pred = torch.argmax(y, dim=1)<br>\n","$\\qquad$$\\qquad$count += torch.sum(pred == label)  <br>\n","$\\qquad$<font color=\"silver\">#print(\"epoch: {}, mean loss: {}, mean accuracy: {}, elapsed_time :{}\".format(</font><br>\n","$\\qquad$$\\qquad$<font color=\"silver\">#epoch,</font><br>\n","$\\qquad$$\\qquad$<font color=\"silver\">#sum_loss / n_iter,</font><br>\n","$\\qquad$$\\qquad$<font color=\"silver\">#count.item() / len(train_data),</font><br>\n","$\\qquad$$\\qquad$<font color=\"silver\">#time() - start</font><br>\n","$\\qquad$$\\qquad$<font color=\"silver\">#))</font><br>\n","<font color=\"silver\"># ❻ 検証</font><br>\n","model.eval()<br>\n","count = 0<br>\n","with torch.no_grad():<br>\n","$\\qquad$for image, label in test_loader:<br>\n","$\\qquad$$\\qquad$if use_cuda:<br>\n","$\\qquad$$\\qquad$$\\qquad$image = image.cuda()<br>\n","$\\qquad$$\\qquad$$\\qquad$label = label.cuda()<br>\n","$\\qquad$$\\qquad$y = model(image)<br>\n","$\\qquad$$\\qquad$pred = torch.argmax(y, dim=1)<br>\n","$\\qquad$$\\qquad$count += torch.sum(pred == label)<br>\n","print(\"test accuracy: {}\".format(count.item() / len(test_data)))<br>"],"metadata":{"id":"pen-V6fbupJm"}},{"cell_type":"markdown","source":["# <font color=\"silver\">実装│Pytorch転移学習</font><br>\n","<font color=\"silver\"># ❶ データセットの準備</font><br>\n","<font color=\"silver\"># ❷ ネットワークの準備</font><br>\n","<font color=\"silver\"># ❸ モデルの準備（Model, Criterion, Optimizer）</font><br>\n","<font color=\"silver\"># ❹ 学習の準備（batch size, DataLoader）</font><br>\n","<font color=\"silver\"># ❺ 学習（epoch, network size）</font><br>\n","<font color=\"silver\"># ❻ 検証</font><br><br>\n","<font color=\"silver\"># ❶ データセットの準備</font><br>\n","<font color=\"silver\"># ❷ ネットワークの準備</font><br>\n","<font color=\"silver\"># ❸ モデルの準備（Model, Criterion, Optimizer）</font><br>\n","model = models.resnet18(pretrained=True)<br>\n","<font color=\"silver\"> # model = models.wide_resnet50_2(pretrained=True)</font><br>\n","for param in model.parameters(): <font color=\"silver\"> # パラメータを固定</font><br>\n","$\\qquad$param.requires_grad = False<br>\n","fc_dim = model.fc.in_features <font color=\"silver\"> # 出力層の変更（2クラス分類問題に合わせる）</font><br>\n","model.fc = nn.Linear(fc_dim, len(class_names))<br>\n","if use_cuda:<br>\n","$\\qquad$model.cuda()<br>\n","criterion = nn.CrossEntropyLoss()<br>\n","if use_cuda:<br>\n","$\\qquad$criterion.cuda()<br>\n","optimizer = torch.optim.SGD(<br>\n","$\\qquad$model.parameters(), <br>\n","$\\qquad$lr=0.01, <br>\n","$\\qquad$momentum=0.9<br>\n","$\\qquad$)<br>\n","<font color=\"silver\"># ❹ 学習の準備（batch size, DataLoader）</font><br>\n","<font color=\"silver\"># ❺ 学習（epoch, network size）</font><br>\n","<font color=\"silver\"># ❻ 検証</font><br><br>\n","<font color=\"silver\"># ❶ データセットの準備</font><br>\n","<font color=\"silver\"># ❷ ネットワークの準備</font><br>\n","<font color=\"silver\"># ❸ モデルの準備（Model, Criterion, Optimizer）</font><br>\n","model = torchvision.models.wide_resnet50_2(pretrained=False,num_classes=10)<br>\n","<font color=\"silver\"> # model = models.wide_resnet50_2(pretrained=True)</font><br>\n","for param in model.parameters(): <font color=\"silver\"> # パラメータを固定</font><br>\n","$\\qquad$param.requires_grad = False<br>\n","<font color=\"silver\"> # 最初のConv層の入力チャンネル数を変更（グレースケール画像）</font><br>\n","model.conv = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)<br>\n","if use_cuda:<br>\n","$\\qquad$model.cuda()<br>\n","criterion = nn.CrossEntropyLoss()<br>\n","if use_cuda:<br>\n","$\\qquad$criterion.cuda()<br>\n","optimizer = torch.optim.SGD(<br>\n","$\\qquad$model.parameters(), <br>\n","$\\qquad$lr=0.01, <br>\n","$\\qquad$momentum=0.9<br>\n","$\\qquad$)<br>\n","<font color=\"silver\"># ❹ 学習の準備（batch size, DataLoader）</font><br>\n","<font color=\"silver\"># ❺ 学習（epoch, network size）</font><br>\n","<font color=\"silver\"># ❻ 検証</font><br>"],"metadata":{"id":"CRI1r2FWu2ZO"}},{"cell_type":"markdown","source":["# <font color=\"silver\">実装│PytorchGAN\n","<font color=\"silver\"># ❶ データセットの準備</font><br>\n","<font color=\"silver\"># ❷ ネットワークの準備</font><br>\n","<font color=\"silver\"># ❸ モデルの準備（Model, Criterion, Optimizer）</font><br>\n","<font color=\"silver\"># ❹ 学習の準備（batch size, DataLoader）</font><br>\n","<font color=\"silver\"># ❺ 学習（epoch, network size）</font><br>\n","<font color=\"silver\"># ❻ 検証</font><br><br>\n","import os<br>\n","import zipfile<br>\n","import urllib.request<br>\n","import numpy as np<br>\n","from PIL import Image<br>\n","import matplotlib.pyplot as plt<br>\n","import torch<br>\n","import torch.nn as nn<br>\n","import torch.optim as optim<br>\n","from torch.utils.data import DataLoader, Dataset<br>\n","from torchvision import datasets, transforms<br>\n","use_cuda = torch.cuda.is_available()<br>\n","<font color=\"silver\"># ❶ データセットの準備</font><br>\n","transform_train = transforms.Compose([transforms.ToTensor()])<br>\n","mnist_data = datasets.MNIST(<br>\n","$\\qquad$root='./data', <br>\n","$\\qquad$train=True, <br>\n","$\\qquad$transform=transform_train, <br>\n","$\\qquad$download=True<br>\n","$\\qquad$)<br>\n","<font color=\"silver\"># ❷ ネットワークの準備</font><br>\n","class Generator(nn.Module):<br>\n","$\\qquad$def  __ init __ (self, latent_dim=100):<br>\n","$\\qquad$$\\qquad$super(Generator, self). __ init __ ()<br>\n","$\\qquad$$\\qquad$self.layer = nn.Sequential(<br>\n","$\\qquad$$\\qquad$$\\qquad$nn.Linear(latent_dim, 256),<br>\n","$\\qquad$$\\qquad$$\\qquad$nn.ReLU(inplace=True),<br>\n","$\\qquad$$\\qquad$$\\qquad$nn.Linear(256, 512),<br>\n","$\\qquad$$\\qquad$$\\qquad$nn.ReLU(inplace=True),<br>\n","$\\qquad$$\\qquad$$\\qquad$nn.Linear(512, 784))<br>\n","$\\qquad$def forward(self, z):<br>\n","$\\qquad$$\\qquad$return self.layer(z)<br>\n","class Discriminator(nn.Module):<br>\n","$\\qquad$def  __ init __ (self):<br>\n","$\\qquad$$\\qquad$super(Discriminator, self). __ init __ ()<br>\n","$\\qquad$$\\qquad$self.layer = nn.Sequential(<br>\n","$\\qquad$$\\qquad$$\\qquad$nn.Linear(784, 512),<br>\n","$\\qquad$$\\qquad$$\\qquad$nn.ReLU(inplace=True),<br>\n","$\\qquad$$\\qquad$$\\qquad$nn.Linear(512, 256),<br>\n","$\\qquad$$\\qquad$$\\qquad$nn.ReLU(inplace=True),<br>\n","$\\qquad$$\\qquad$$\\qquad$nn.Linear(256, 1))<br>\n","$\\qquad$def forward(self, x):<br>\n","$\\qquad$$\\qquad$return self.layer(x)<br>\n","<font color=\"silver\"># ❸ モデルの準備（Model, Criterion, Optimizer）</font><br>\n","G = Generator()<br>\n","D = Discriminator()<br>\n","if use_cuda:<br>\n","$\\qquad$G = G.cuda()<br>\n","$\\qquad$D = D.cuda()<br>\n","criterion = nn.BCEWithLogitsLoss()<br>\n","opt_g = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))<br>\n","opt_d = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))<br>\n","<font color=\"silver\"># ❹ 学習の準備（batch size, DataLoader）</font><br>\n","batch_size=100<br>\n","training_data = DataLoader(<br>\n","$\\qquad$mnist_data, <br>\n","$\\qquad$batch_size=batch_size,<br>\n","$\\qquad$shuffle=True<br>\n","$\\qquad$)<br>\n","<font color=\"silver\"># ❺ 学習（epoch, network size）</font><br>\n","n_epoch = 10<br>\n","n_critic = 2<br>\n","for epoch in range(1, n_epoch+1):<br>\n","$\\qquad$Tensor = torch.cuda.FloatTensor<br>\n","$\\qquad$for idx, (real_x, _) in enumerate(training_data):<br>\n","$\\qquad$$\\qquad$real_x = real_x.cuda()<br>\n","$\\qquad$$\\qquad$batch = real_x.size(0)<br>\n","$\\qquad$$\\qquad$flag_real = Tensor(batch, 1).fill_(1.0)<br>\n","$\\qquad$$\\qquad$flag_fake = Tensor(batch, 1).fill_(0.0)$\\qquad$$\\qquad$<br>\n","$\\qquad$$\\qquad$for _ in range(n_critic):<br>\n","$\\qquad$$\\qquad$$\\qquad$D.zero_grad()  <font color=\"silver\"># 勾配の初期化</font><br>\n","$\\qquad$$\\qquad$$\\qquad$z = torch.randn(batch, 100)  <font color=\"silver\"># ノイズの生成</font><br>\n","$\\qquad$$\\qquad$$\\qquad$if use_cuda:<br>\n","$\\qquad$$\\qquad$$\\qquad$$\\qquad$z = z.cuda()<br>\n","$\\qquad$$\\qquad$$\\qquad$fake_x = G(z)  <font color=\"silver\"># 偽画像の生成</font><br>\n","$\\qquad$$\\qquad$$\\qquad$out_real = D(real_x.view(batch, -1))  <font color=\"silver\"># 本物画像の識別</font><br>\n","$\\qquad$$\\qquad$$\\qquad$out_fake = D(fake_x.detach().view(batch, -1))  <font color=\"silver\"># 偽画像の識別</font><br>\n","$\\qquad$$\\qquad$$\\qquad$loss_real = criterion(out_real, flag_real)  <font color=\"silver\"># 本物画像の識別の誤差</font><br>\n","$\\qquad$$\\qquad$$\\qquad$loss_fake = criterion(out_fake, flag_fake)  <font color=\"silver\"># 偽画像の識別の誤差</font><br>\n","$\\qquad$$\\qquad$$\\qquad$dis_loss = loss_real + loss_fake<br>\n","$\\qquad$$\\qquad$$\\qquad$dis_loss.backward()<br>\n","$\\qquad$$\\qquad$$\\qquad$opt_d.step()<br>\n","$\\qquad$$\\qquad$G.zero_grad()  <font color=\"silver\"># 勾配の初期化</font><br>\n","$\\qquad$$\\qquad$z = torch.randn(batch, 100)<br>\n","$\\qquad$$\\qquad$if use_cuda:<br>\n","$\\qquad$$\\qquad$$\\qquad$z = z.cuda()<br>\n","$\\qquad$$\\qquad$fake_x = G(z)<br>\n","$\\qquad$$\\qquad$out_gen = D(fake_x)<br>\n","$\\qquad$$\\qquad$gen_loss = criterion(out_gen, flag_real)<br>\n","$\\qquad$$\\qquad$gen_loss.backward()<br>\n","$\\qquad$$\\qquad$opt_g.step()<br>\n","$\\qquad$$\\qquad$if idx % 100 == 0:<br>\n","$\\qquad$$\\qquad$$\\qquad$print('Training epoch: {} [{}/{} ({:.0f}%)] | D loss: {:.6f} | G loss: {:.6f} |'.format(<br>\n","$\\qquad$$\\qquad$$\\qquad$$\\qquad$epoch,<br>\n","$\\qquad$$\\qquad$$\\qquad$$\\qquad$idx * len(real_x), <br>\n","$\\qquad$$\\qquad$$\\qquad$$\\qquad$len(training_data.dataset),<br>\n","$\\qquad$$\\qquad$$\\qquad$$\\qquad$100. * idx / len(training_data),<br>\n","$\\qquad$$\\qquad$$\\qquad$$\\qquad$dis_loss.item(),<br>\n","$\\qquad$$\\qquad$$\\qquad$$\\qquad$gen_loss.item()<br>\n","$\\qquad$$\\qquad$$\\qquad$$\\qquad$))<br>\n","<font color=\"silver\"># ❻ 検証</font><br>\n","num_generate = 100<br>\n","ch = 100<br>\n","z = torch.randn(num_generate, ch)<br>\n","if use_cuda:<br>\n","$\\qquad$z = z.cuda()<br>\n","test_img = G(z)<br>\n","if use_cuda:<br>\n","$\\qquad$test_img = test_img.cpu()<br>\n","test_img_array = (test_img * 256.).clamp(min=0., max=255.).view(num_generate, 28, 28).data.numpy()<br>\n","fig = plt.figure(figsize=(10, 10))<br>\n","for i, im in enumerate(test_img_array):<br>\n","$\\qquad$ax = fig.add_subplot(10, 10, i+1, xticks=[], yticks=[])<br>\n","$\\qquad$ax.imshow(im, 'gray')<br>"],"metadata":{"id":"Kgzz4lyNa5VT"}},{"cell_type":"markdown","source":["# <font color=\"silver\">実装│SimpleRNNLM forward</font><br>\n","class TimeEmbedding:<br>\n","$\\qquad$def forward(self, xs):<br>\n","$\\qquad$$\\qquad$N, T = xs.shape <font color=\"silver\"> # バッチサイズ、時間数</font><br>\n","$\\qquad$$\\qquad$V, D = self.W.shape <font color=\"silver\"> # 語彙数、埋め込みベクトルの要素数</font><br>\n","$\\qquad$$\\qquad$self.layers = [] <font color=\"silver\"> # 逆伝播で使用する</font><br>\n","$\\qquad$$\\qquad$out = np.empty((N, T, D), dtype='f')<font color=\"silver\"> # 出力用</font><br>\n","$\\qquad$$\\qquad$for t in range(T):  <font color=\"silver\"> # 時間方向に計算</font><br>\n","$\\qquad$$\\qquad$$\\qquad$layer = Embedding(self.W)<br>\n","$\\qquad$$\\qquad$$\\qquad$out[:, t, :] = layer.forward(xs[:, t])   <br>\n","$\\qquad$$\\qquad$$\\qquad$self.layers.append(layer)<br>\n","$\\qquad$$\\qquad$return out<br>\n","class Embedding:<br>\n","$\\qquad$def forward(self, idx):<br>\n","$\\qquad$$\\qquad$W = self.params[0]<br>\n","$\\qquad$$\\qquad$self.idx = idx<br>\n","$\\qquad$$\\qquad$out = W[idx] <font color=\"silver\"> # 埋め込み行列から埋め込みベクトルを取り出す</font><br>\n","$\\qquad$$\\qquad$return out<br>\n","class TimeLSTM:<br>\n","$\\qquad$def forward(self, xs):<br>\n","$\\qquad$$\\qquad$Wx, Wh, b = self.params<br>\n","$\\qquad$$\\qquad$N, T, D = xs.shape<br>\n","$\\qquad$$\\qquad$H = Wh.shape[0]<br>\n","$\\qquad$$\\qquad$self.layers = []<br>\n","$\\qquad$$\\qquad$hs = np.empty((N, T, H))<br>\n","$\\qquad$$\\qquad$if not self.stateful or self.h is None:<br>\n","$\\qquad$$\\qquad$$\\qquad$self.h = np.zeros((N, H))<br>\n","$\\qquad$$\\qquad$if not self.stateful or self.c is None:<br>\n","$\\qquad$$\\qquad$$\\qquad$self.c = np.zeros((N, H))<br>\n","$\\qquad$$\\qquad$ for t in range(T): <font color=\"silver\"> # 時間方向に計算</font><br>\n","$\\qquad$$\\qquad$$\\qquad$layer = LSTM(* self.params) <font color=\"silver\"> # *を変数前につけると、各引数に展開される</font><br>\n","$\\qquad$$\\qquad$$\\qquad$self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)<br>\n","$\\qquad$$\\qquad$$\\qquad$hs[:, t, :] = self.h <font color=\"silver\">中間層の出力hをhsに代入する</font><br>\n","$\\qquad$$\\qquad$$\\qquad$self.layers.append(layer)<br>\n","$\\qquad$$\\qquad$return hs<br>\n","class LSTM:<br>\n","$\\qquad$def forward(self, x, h_prev, c_prev): <br>\n","$\\qquad$$\\qquad$Wx, Wh, b = self.params<br>\n","$\\qquad$$\\qquad$N, H = h_prev.shape<br>\n","$\\qquad$$\\qquad$A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b<br>\n","$\\qquad$$\\qquad$f = A[:, :H]<br>\n","$\\qquad$$\\qquad$g = A[:, H:2*H]<br>\n","$\\qquad$$\\qquad$i = A[:, 2*H:3*H]<br>\n","$\\qquad$$\\qquad$o = A[:, 3*H:]<br>\n","$\\qquad$$\\qquad$f = sigmoid(f)<br>\n","$\\qquad$$\\qquad$g = np.tanh(g)<br>\n","$\\qquad$$\\qquad$i = sigmoid(i)<br>\n","$\\qquad$$\\qquad$o = sigmoid(o)<br>\n","$\\qquad$$\\qquad$c_next = f * c_prev + g * i<br>\n","$\\qquad$$\\qquad$tanh_c_next = np.tanh(c_next)<br>\n","$\\qquad$$\\qquad$h_next = o * tanh_c_next<br>\n","$\\qquad$$\\qquad$self.cache = (x, h_prev, c_prev, i, f, g, o, tanh_c_next)<br>\n","$\\qquad$$\\qquad$return h_next, c_next<br>\n","class TimeAffine:<br>\n","$\\qquad$def forward(self, xs):<br>\n","$\\qquad$$\\qquad$N, T, D = xs.shape<br>\n","$\\qquad$$\\qquad$D, M = self.W.shape<br>\n","$\\qquad$$\\qquad$self.layers = []<br></font>\n","$\\qquad$$\\qquad$out = np.empty((N, T, M), dtype='f')<br></font>\n","$\\qquad$$\\qquad$for t in range(T):<br></font>\n","$\\qquad$$\\qquad$$\\qquad$layer = Affine(self.W, self.b)<br>\n","$\\qquad$$\\qquad$$\\qquad$out[:, t, :] = layer.forward(xs[:, t, :])<br>\n","$\\qquad$$\\qquad$$\\qquad$self.layers.append(layer)<br>\n","$\\qquad$$\\qquad$return out<br>\n","class Affine:<br>\n","$\\qquad$def forward(self, x):<br>\n","$\\qquad$$\\qquad$W, b = self.params<br>\n","$\\qquad$$\\qquad$out = np.dot(x, W) + b<br>\n","$\\qquad$$\\qquad$self.x = x<br>\n","$\\qquad$$\\qquad$return out<br>\n","class TimeAttention:<br>\n","$\\qquad$def forward(self, hs_enc, hs_dec):<br>\n","$\\qquad$$\\qquad$N, T, H = hs_dec.shape<br>\n","$\\qquad$$\\qquad$out = np.empty_like(hs_dec)<br>\n","$\\qquad$$\\qquad$self.layers = []<br>\n","$\\qquad$$\\qquad$self.attention_weights = []<br>\n","$\\qquad$$\\qquad$for t in range(T):<br>\n","$\\qquad$$\\qquad$$\\qquad$layer = Attention()<br>\n","$\\qquad$$\\qquad$$\\qquad$out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:]) <br>\n","$\\qquad$$\\qquad$$\\qquad$self.layers.append(layer)<br>\n","$\\qquad$$\\qquad$$\\qquad$self.attention_weights.append(layer.attention_weight)<br>\n","$\\qquad$$\\qquad$return out<br>\n"],"metadata":{"id":"ZdTzvkwGAwEy"}},{"cell_type":"markdown","source":["# <font color=\"silver\">実装│SimpleRNNLM backward</font><br>\n","class TimeEmbedding:<br>\n","$\\qquad$def backward(self, dout):<br>\n","$\\qquad$$\\qquad$N, T, M = dout.shape<br>\n","$\\qquad$$\\qquad$D, M = self.W.shape<br>\n","$\\qquad$$\\qquad$dxs = np.empty((N, T, D), dtype='f')<br>\n","$\\qquad$$\\qquad$self.dW, self.db = 0, 0<br>\n","$\\qquad$$\\qquad$for t in range(T):<br>\n","$\\qquad$$\\qquad$$\\qquad$layer = self.layers[t]<br>\n","$\\qquad$$\\qquad$$\\qquad$dxs[:, t, :] = layer.backward(dout[:, t, :])<br>\n","$\\qquad$$\\qquad$$\\qquad$self.dW += layer.dW<br>\n","$\\qquad$$\\qquad$$\\qquad$self.db += layer.db<br>\n","$\\qquad$$\\qquad$return dxs<br>\n","class Embedding:<br>\n","$\\qquad$def backward(self, dout):<br>\n","$\\qquad$$\\qquad$dW = self.grads[0]  <font color=\"silver\"> # gradsというリストの1要素目を参照する </font><br>\n","$\\qquad$$\\qquad$dW.fill(0)  <font color=\"silver\"># 配列の全ての要素に0を代入する</font><br>\n","$\\qquad$$\\qquad$np.add.at(dW, self.idx, dout) <font color=\"silver\"> # dWのidxの場所にdoutを加える</font><br>\n","$\\qquad$$\\qquad$return None<br>\n","class TimeLSTM:<br>\n","$\\qquad$def backward(self, dhs):<br>\n","$\\qquad$$\\qquad$Wx, Wh, b = self.params<br>\n","$\\qquad$$\\qquad$N, T, H = dhs.shape <font color=\"silver\"> # バッチサイズ、時間数、中間層のノード数</font><br>\n","$\\qquad$$\\qquad$D = Wx.shape[0]<br>\n","$\\qquad$$\\qquad$dxs = np.empty((N, T, D)) <font color=\"silver\"> # dxsは、各時刻におけるdxを格納する変数</font><br>\n","$\\qquad$$\\qquad$dh, dc = 0, 0 <font color=\"silver\"> # 勾配格納用</font><br>\n","$\\qquad$$\\qquad$grads = [0, 0, 0]  <font color=\"silver\"> # Wxの勾配、 Whの勾配、 bの勾配</font><br>\n","$\\qquad$$\\qquad$for t in reversed(range(T)): <font color=\"silver\"> # 時間方向と逆向きに計算を進める</font><br>\n","$\\qquad$$\\qquad$$\\qquad$layer = self.layers[t]<br>\n","$\\qquad$$\\qquad$$\\qquad$dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc) <font color=\"silver\"> # LSTMレイヤの勾配は、2方向から来るので足す</font><br>\n","$\\qquad$$\\qquad$$\\qquad$dxs[:, t, :] = dx<br>\n","$\\qquad$$\\qquad$$\\qquad$for i, grad in enumerate(layer.grads):<br>\n","$\\qquad$$\\qquad$$\\qquad$$\\qquad$grads[i] += grad<br>\n","$\\qquad$$\\qquad$for i, grad in enumerate(grads):<br>\n","$\\qquad$$\\qquad$$\\qquad$self.grads[i][:] = grad<br>\n","$\\qquad$$\\qquad$self.dh = dh <font color=\"silver\"> # 最後の中間層のdhを保持しておく</font><br>\n","$\\qquad$$\\qquad$return dxs<br>\n","class LSTM:<br>\n","$\\qquad$def backward(self, dh_next, dc_next):<br>\n","$\\qquad$$\\qquad$Wx, Wh, b = self.params<br>\n","$\\qquad$$\\qquad$x, h_prev, c_prev, i, f, g, o, tanh_c_next = self.cache<br>\n","$\\qquad$$\\qquad$A2 = (dh_next * o) * (1 - tanh_c_next ** 2)<br>\n","$\\qquad$$\\qquad$ds = dc_next + A2<br>\n","$\\qquad$$\\qquad$dc_prev = ds * f<br>\n","$\\qquad$$\\qquad$di = ds * g<br>\n","$\\qquad$$\\qquad$df = ds * c_prev<br>\n","$\\qquad$$\\qquad$do = dh_next * tanh_c_next<br>\n","$\\qquad$$\\qquad$dg = ds * i<br>\n","$\\qquad$$\\qquad$di * = i * (1 - i)<br>\n","$\\qquad$$\\qquad$df * = f * (1 - f)<br>\n","$\\qquad$$\\qquad$do * = o * (1 - o)<br>\n","$\\qquad$$\\qquad$dg * = (1 - g * * 2)<br>\n","$\\qquad$$\\qquad$dA = np.hstack((df, dg, di, do))<br>\n","$\\qquad$$\\qquad$dWh = np.dot(h_prev.T, dA)<br>\n","$\\qquad$$\\qquad$dWx = np.dot(x.T, dA)<br>\n","$\\qquad$$\\qquad$db = dA.sum(axis=0)<br>\n","$\\qquad$$\\qquad$dx = np.dot(dA, Wx.T)<br>\n","$\\qquad$$\\qquad$dh_prev = np.dot(dA, Wh.T)<br>\n","$\\qquad$$\\qquad$self.grads[0][:] = dWx<br>\n","$\\qquad$$\\qquad$self.grads[1][:] = dWh<br>\n","$\\qquad$$\\qquad$self.grads[2][:] = db<br>\n","$\\qquad$$\\qquad$return dx, dh_prev, dc_prev<br>\n","class TimeAffine:<br>\n","$\\qquad$def backward(self, dout):<br>\n","$\\qquad$$\\qquad$N, T, D = dout.shape<br>\n","$\\qquad$$\\qquad$grad = 0<font color=\"silver\"> # 勾配格納用</font><br>\n","$\\qquad$$\\qquad$for t in range(T): <font color=\"silver\">  # 時間方向に計算（時間方向には独立しているため逆方向に進めなくてよい）</font><br>\n","$\\qquad$$\\qquad$$\\qquad$layer = self.layers[t]<br>\n","$\\qquad$$\\qquad$$\\qquad$layer.backward(dout[:, t, :])  <br>\n","$\\qquad$$\\qquad$$\\qquad$grad += layer.grads[0] <font color=\"silver\"> # 勾配を足し合わせる</font><br>\n","$\\qquad$$\\qquad$self.grads[0][:] = grad<br>\n","$\\qquad$$\\qquad$return None<br>\n","class Affine:<br>\n","$\\qquad$def backward(self, dout):<br>\n","$\\qquad$$\\qquad$W, b = self.params<br>\n","$\\qquad$$\\qquad$dx = np.dot(dout, W.T)<br>\n","$\\qquad$$\\qquad$dW = np.dot(self.x.T, dout)<br>\n","$\\qquad$$\\qquad$db = np.sum(dout, axis=0)<br>\n","$\\qquad$$\\qquad$self.grads[0][...] = dW<br>\n","$\\qquad$$\\qquad$self.grads[1][...] = db<br>\n","$\\qquad$$\\qquad$return dx<br>\n","class TimeAttention:<br>\n","$\\qquad$def backward(self, dout):<br>\n","$\\qquad$$\\qquad$N, T, H = dout.shape<br>\n","$\\qquad$$\\qquad$dhs_enc = 0<br>\n","$\\qquad$$\\qquad$dhs_dec = np.empty_like(dout)<br>\n","$\\qquad$$\\qquad$for t in range(T):<br>\n","$\\qquad$$\\qquad$$\\qquad$layer = self.layers[t]<br>\n","$\\qquad$$\\qquad$$\\qquad$dhs, dh = layer.backward(dout[:, t, :])<br>\n","$\\qquad$$\\qquad$$\\qquad$dhs_enc += dhs<br>\n","$\\qquad$$\\qquad$$\\qquad$dhs_dec[:,t,:] = dh<br>\n","$\\qquad$$\\qquad$return dhs_enc, dhs_dec<br>"],"metadata":{"id":"x_WCjxQTBNqt"}},{"cell_type":"markdown","source":["# <font color=\"silver\">実装│TimeLayer</font><br>\n","class TimeEmbedding:<br>\n","$\\qquad$def __ init __ (self, W):<br>\n","$\\qquad$$\\qquad$self.params = [W]<br>\n","$\\qquad$$\\qquad$self.grads = [np.zeros_like(W)]<br>\n","$\\qquad$$\\qquad$self.layers = None<br>\n","$\\qquad$$\\qquad$self.W = W<br>\n","$\\qquad$def forward(self, xs):<br>\n","$\\qquad$$\\qquad$N, T = xs.shape <font color=\"silver\"> # バッチサイズ、時間数</font><br>\n","$\\qquad$$\\qquad$V, D = self.W.shape <font color=\"silver\"> # 語彙数、埋め込みベクトルの要素数</font><br>\n","$\\qquad$$\\qquad$self.layers = [] <font color=\"silver\"> # 逆伝播で使用する</font><br>\n","$\\qquad$$\\qquad$out = np.empty((N, T, D), dtype='f')<font color=\"silver\"> # 出力用</font><br>\n","$\\qquad$$\\qquad$for t in range(T):  <font color=\"silver\"> # 時間方向に計算</font><br>\n","$\\qquad$$\\qquad$$\\qquad$layer = Embedding(self.W)<br>\n","$\\qquad$$\\qquad$$\\qquad$out[:, t, :] = layer.forward(xs[:, t])   <br>\n","$\\qquad$$\\qquad$$\\qquad$self.layers.append(layer)<br>\n","$\\qquad$$\\qquad$return out<br>\n","$\\qquad$def backward(self, dout):<br>\n","$\\qquad$$\\qquad$N, T, D = dout.shape<br>\n","$\\qquad$$\\qquad$grad = 0<font color=\"silver\"> # 勾配格納用</font><br>\n","$\\qquad$$\\qquad$for t in range(T): <font color=\"silver\">  # 時間方向に計算（時間方向には独立しているため逆方向に進めなくてよい）</font><br>\n","$\\qquad$$\\qquad$$\\qquad$layer = self.layers[t]<br>\n","$\\qquad$$\\qquad$$\\qquad$layer.backward(dout[:, t, :])  <br>\n","$\\qquad$$\\qquad$$\\qquad$grad += layer.grads[0] <font color=\"silver\"> # 勾配を足し合わせる</font><br>\n","$\\qquad$$\\qquad$self.grads[0][:] = grad<br>\n","$\\qquad$$\\qquad$return None<br>\n","class TimeRNN:<br>\n","$\\qquad$def __ init __ (self, Wx, Wh, b, stateful=False):<br>\n","$\\qquad$$\\qquad$self.params = [Wx, Wh, b]<br>\n","$\\qquad$$\\qquad$self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]<br>\n","$\\qquad$$\\qquad$self.layers = None<br>\n","$\\qquad$$\\qquad$self.h, self.dh = None, None<br>\n","$\\qquad$$\\qquad$self.stateful = stateful<br>\n","$\\qquad$def forward(self, xs):<br>\n","$\\qquad$$\\qquad$Wx, Wh, b = self.params<br>\n","$\\qquad$$\\qquad$N, T, D = xs.shape<br>\n","$\\qquad$$\\qquad$D, H = Wx.shape<br>\n","$\\qquad$$\\qquad$self.layers = []<br>\n","$\\qquad$$\\qquad$hs = np.empty((N, T, H))<br>\n","$\\qquad$$\\qquad$if not self.stateful or self.h is None:<br>\n","$\\qquad$$\\qquad$$\\qquad$self.h = np.zeros((N, H))<br>\n","$\\qquad$$\\qquad$for t in range(T):<br>\n","$\\qquad$$\\qquad$$\\qquad$layer = RNN(* self.params) <font color=\"silver\"> # *を変数前につけると、各引数に展開される</font><br>\n","$\\qquad$$\\qquad$$\\qquad$self.h = layer.forward(xs[:, t, :], self.h)<br>\n","$\\qquad$$\\qquad$$\\qquad$hs[:, t, :] = self.h<br>\n","$\\qquad$$\\qquad$$\\qquad$self.layers.append(layer)<br>\n","$\\qquad$$\\qquad$return hs<br>\n","$\\qquad$def backward(self, dhs):<br>\n","$\\qquad$$\\qquad$Wx, Wh, b = self.params<br>\n","$\\qquad$$\\qquad$N, T, H = dhs.shape<br>\n","$\\qquad$$\\qquad$D, H = Wx.shape <br>\n","$\\qquad$$\\qquad$dxs = np.empty((N, T, D)) <br>\n","$\\qquad$$\\qquad$dh = 0<br>\n","$\\qquad$$\\qquad$grads = [0, 0, 0] <br>\n","$\\qquad$$\\qquad$for t in reversed(range(T)):<br>\n","$\\qquad$$\\qquad$$\\qquad$layer = self.layers[t]<br>\n","$\\qquad$$\\qquad$$\\qquad$dx, dh = layer.backward(dhs[:, t, :] + dh) <font color=\"silver\"> # dhs+dh</font><br>\n","$\\qquad$$\\qquad$$\\qquad$dxs[:, t, :] = dx<br>\n","$\\qquad$$\\qquad$$\\qquad$for i, grad in enumerate(layer.grads):<br>\n","$\\qquad$$\\qquad$$\\qquad$$\\qquad$grads[i] += grad<br>\n","$\\qquad$$\\qquad$for i, grad in enumerate(grads):<br>\n","$\\qquad$$\\qquad$$\\qquad$self.grads[i][:] = grad <br>\n","$\\qquad$$\\qquad$self.dh = dh<br>\n","$\\qquad$$\\qquad$return dxs<br>\n","$\\qquad$def set_state(self, h):<br>\n","$\\qquad$$\\qquad$self.h = h<br>\n","$\\qquad$ def reset_state(self):<br>\n","$\\qquad$$\\qquad$self.h = None<br>\n","class TimeLSTM:<br>\n","$\\qquad$def __ init __ (self, Wx, Wh, b, stateful=False): <font color=\"silver\"> # stateful :出力を次のミニバッチ に渡す場合はTrueにする</font><br>\n","$\\qquad$$\\qquad$self.params = [Wx, Wh, b]<br>\n","$\\qquad$$\\qquad$self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]<br>\n","$\\qquad$$\\qquad$self.layers = None<br>\n","$\\qquad$$\\qquad$self.h, self.c = None, None<br>\n","$\\qquad$$\\qquad$self.dh = None<br>\n","$\\qquad$$\\qquad$self.stateful = stateful<br>\n","$\\qquad$def forward(self, xs):<br>\n","$\\qquad$$\\qquad$Wx, Wh, b = self.params<br>\n","$\\qquad$$\\qquad$N, T, D = xs.shape<br>\n","$\\qquad$$\\qquad$H = Wh.shape[0]<br>\n","$\\qquad$$\\qquad$self.layers = []<br>\n","$\\qquad$$\\qquad$hs = np.empty((N, T, H))<br>\n","$\\qquad$$\\qquad$if not self.stateful or self.h is None:<br>\n","$\\qquad$$\\qquad$$\\qquad$self.h = np.zeros((N, H))<br>\n","$\\qquad$$\\qquad$if not self.stateful or self.c is None:<br>\n","$\\qquad$$\\qquad$$\\qquad$self.c = np.zeros((N, H))<br>\n","$\\qquad$$\\qquad$ for t in range(T): <font color=\"silver\"> # 時間方向に計算</font><br>\n","$\\qquad$$\\qquad$$\\qquad$layer = LSTM(* self.params) <font color=\"silver\"> # * を変数前につけると、各引数に展開される</font><br>\n","$\\qquad$$\\qquad$$\\qquad$self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)<br>\n","$\\qquad$$\\qquad$$\\qquad$hs[:, t, :] = self.h <font color=\"silver\">中間層の出力hをhsに代入する</font><br>\n","$\\qquad$$\\qquad$$\\qquad$self.layers.append(layer)<br>\n","$\\qquad$$\\qquad$return hs<br>\n","$\\qquad$def backward(self, dhs):<br>\n","$\\qquad$$\\qquad$Wx, Wh, b = self.params<br>\n","$\\qquad$$\\qquad$N, T, H = dhs.shape <font color=\"silver\"> # バッチサイズ、時間数、中間層のノード数</font><br>\n","$\\qquad$$\\qquad$D = Wx.shape[0]<br>\n","$\\qquad$$\\qquad$dxs = np.empty((N, T, D)) <font color=\"silver\"> # dxsは、各時刻におけるdxを格納する変数</font><br>\n","$\\qquad$$\\qquad$dh, dc = 0, 0 <font color=\"silver\"> # 勾配格納用</font><br>\n","$\\qquad$$\\qquad$grads = [0, 0, 0]  <font color=\"silver\"> # Wxの勾配、 Whの勾配、 bの勾配</font><br>\n","$\\qquad$$\\qquad$for t in reversed(range(T)): <font color=\"silver\"> # 時間方向と逆向きに計算を進める</font><br>\n","$\\qquad$$\\qquad$$\\qquad$layer = self.layers[t]<br>\n","$\\qquad$$\\qquad$$\\qquad$dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc) <font color=\"silver\"> # LSTMレイヤの勾配は、2方向から来るので足す</font><br>\n","$\\qquad$$\\qquad$$\\qquad$dxs[:, t, :] = dx<br>\n","$\\qquad$$\\qquad$$\\qquad$for i, grad in enumerate(layer.grads):<br>\n","$\\qquad$$\\qquad$$\\qquad$$\\qquad$grads[i] += grad<br>\n","$\\qquad$$\\qquad$for i, grad in enumerate(grads):<br>\n","$\\qquad$$\\qquad$$\\qquad$self.grads[i][:] = grad<br>\n","$\\qquad$$\\qquad$self.dh = dh <font color=\"silver\"> # 最後の中間層のdhを保持しておく</font><br>\n","$\\qquad$$\\qquad$return dxs<br>\n","$\\qquad$def set_state(self, h, c=None):<br>\n","$\\qquad$$\\qquad$self.h, self.c = h, c<br>\n","$\\qquad$def reset_state(self):<br>\n","$\\qquad$$\\qquad$self.h, self.c = None, None<br>\n","class TimeAffine:<br>\n","$\\qquad$def __ init __ (self, W, b):<br>\n","$\\qquad$$\\qquad$self.W, self.b = W, b<br>\n","$\\qquad$$\\qquad$self.dW, self.db = None, None<br>\n","$\\qquad$$\\qquad$self.layers = None<br>\n","$\\qquad$def forward(self, xs):<br>\n","$\\qquad$$\\qquad$N, T, D = xs.shape<br>\n","$\\qquad$$\\qquad$D, M = self.W.shape<br>\n","$\\qquad$$\\qquad$self.layers = []<br></font>\n","$\\qquad$$\\qquad$out = np.empty((N, T, M), dtype='f')<br></font>\n","$\\qquad$$\\qquad$for t in range(T):<br></font>\n","$\\qquad$$\\qquad$$\\qquad$layer = Affine(self.W, self.b)<br>\n","$\\qquad$$\\qquad$$\\qquad$out[:, t, :] = layer.forward(xs[:, t, :])<br>\n","$\\qquad$$\\qquad$$\\qquad$self.layers.append(layer)<br>\n","$\\qquad$$\\qquad$return out<br>\n","$\\qquad$def backward(self, dout):<br>\n","$\\qquad$$\\qquad$N, T, M = dout.shape<br>\n","$\\qquad$$\\qquad$D, M = self.W.shape<br>\n","$\\qquad$$\\qquad$dxs = np.empty((N, T, D), dtype='f')<br>\n","$\\qquad$$\\qquad$self.dW, self.db = 0, 0<br>\n","$\\qquad$$\\qquad$for t in range(T):<br>\n","$\\qquad$$\\qquad$$\\qquad$layer = self.layers[t]<br>\n","$\\qquad$$\\qquad$$\\qquad$dxs[:, t, :] = layer.backward(dout[:, t, :])<br>\n","$\\qquad$$\\qquad$$\\qquad$self.dW += layer.dW<br>\n","$\\qquad$$\\qquad$$\\qquad$self.db += layer.db<br>\n","$\\qquad$$\\qquad$return dxs<br>\n","class Affine:<br>\n","$\\qquad$def __ init __ (self, W, b):<br>\n","$\\qquad$$\\qquad$self.params = [W, b]<br>\n","$\\qquad$$\\qquad$self.grads = [np.zeros_like(W), np.zeros_like(b)]<br>\n","$\\qquad$$\\qquad$self.x = None<br>\n","$\\qquad$def forward(self, x):<br>\n","$\\qquad$$\\qquad$W, b = self.params<br>\n","$\\qquad$$\\qquad$out = np.dot(x, W) + b<br>\n","$\\qquad$$\\qquad$self.x = x <font color=\"silver\"># 逆伝播で使う</font><br>\n","$\\qquad$$\\qquad$return out<br>\n","$\\qquad$def backward(self, dout):<br>\n","$\\qquad$$\\qquad$W, b = self.params<br>\n","$\\qquad$$\\qquad$dx = np.dot(dout, W.T)<br>\n","$\\qquad$$\\qquad$dW = np.dot(self.x.T, dout)<br>\n","$\\qquad$$\\qquad$db = np.sum(dout, axis=0)<br>\n","$\\qquad$$\\qquad$self.grads[0][...] = dW<br>\n","$\\qquad$$\\qquad$self.grads[1][...] = db<br>\n","$\\qquad$$\\qquad$return dx<br>\n","class TimeAttention:<br>\n","$\\qquad$def __ init __ (self):<br>\n","$\\qquad$$\\qquad$self.params, self.grads = [], []<br>\n","$\\qquad$$\\qquad$self.layers = None<br>\n","$\\qquad$$\\qquad$self.attention_weights = None<br>\n","$\\qquad$def forward(self, hs_enc, hs_dec):<br>\n","$\\qquad$$\\qquad$N, T, H = hs_dec.shape<br>\n","$\\qquad$$\\qquad$out = np.empty_like(hs_dec)<br>\n","$\\qquad$$\\qquad$self.layers = []<br>\n","$\\qquad$$\\qquad$self.attention_weights = []<br>\n","$\\qquad$$\\qquad$for t in range(T):<br>\n","$\\qquad$$\\qquad$$\\qquad$layer = Attention()<br>\n","$\\qquad$$\\qquad$$\\qquad$out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:]) <br>\n","$\\qquad$$\\qquad$$\\qquad$self.layers.append(layer)<br>\n","$\\qquad$$\\qquad$$\\qquad$self.attention_weights.append(layer.attention_weight)<br>\n","$\\qquad$$\\qquad$return out<br>\n","$\\qquad$def backward(self, dout):<br>\n","$\\qquad$$\\qquad$N, T, H = dout.shape<br>\n","$\\qquad$$\\qquad$dhs_enc = 0<br>\n","$\\qquad$$\\qquad$dhs_dec = np.empty_like(dout)<br>\n","$\\qquad$$\\qquad$for t in range(T):<br>\n","$\\qquad$$\\qquad$$\\qquad$layer = self.layers[t]<br>\n","$\\qquad$$\\qquad$$\\qquad$dhs, dh = layer.backward(dout[:, t, :])<br>\n","$\\qquad$$\\qquad$$\\qquad$dhs_enc += dhs<br>\n","$\\qquad$$\\qquad$$\\qquad$dhs_dec[:,t,:] = dh<br>\n","$\\qquad$$\\qquad$return dhs_enc, dhs_dec<br>"],"metadata":{"id":"F5bSn07ofdkF"}},{"cell_type":"markdown","source":["# <font color=\"silver\">実装│SimpleRNNLM</font><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/35f6f0d26f9e58f01e4e)</font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F19b35dea-8cc6-35f1-da5f-9849395053c3.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=3de1c04d0aca2bc134b64bff46c63d40\" width=\"480\"><br><br>\n","<font color=\"silver\"># 重みの初期値と層の生成<br></font>\n","rn = np.random.randn<br>\n","embed_W = rn(V, D) / 100 <font color=\"silver\"> # 小さな値で初期化するため100で割る<br></font>\n","rnn_Wx = rn(D, H) * np.sqrt(2/(D+H)) <font color=\"silver\"> # Xavierの初期値<br></font>\n","nn_Wh = rn(H, H) * np.sqrt(2/(H+H)) <font color=\"silver\"> # Xavierの初期値<br></font>\n","rnn_b = np.zeros(H)<br>\n"," affine_W = rn(H, V) * np.sqrt(2/(H+V)) <font color=\"silver\"> # Xavierの初期値<br></font>\n","affine_b = np.zeros(V)<br>\n","self.layers = [<br>\n","TimeEmbedding(embed_W),<br>\n","TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True), <font color=\"silver\"> # 中間層を引き継ぐためTrueにする<br></font>\n","TimeAffine(affine_W, affine_b) <br>\n","]<br>\n","self.loss_layer = TimeSoftmaxWithLoss()</font><br><br>\n","<font color=\"silver\"># ❶ 重みの初期値を定義</font><br>\n","<font color=\"silver\"># ❷ 隠れ層ををlistで定義</font><br>\n","<font color=\"silver\"># ❸ 出力層を定義</font><br>\n","<font color=\"silver\"># ❹ 重みと勾配をListでまとめる</font><br>\n","<font color=\"silver\"># ❺ 推論値算出（batch size, DataLoader）</font><br>\n","<font color=\"silver\"># ❻ 損失値算出（epoch, network size）</font><br>\n","<font color=\"silver\"># ❼ 勾配値算出</font><br><br>\n","class SimpleRnnlm:<br>\n","$\\qquad$def __ init __(self, vocab_size, wordvec_size, hidden_size):<br>\n","$\\qquad$$\\qquad$V, D, H = vocab_size, wordvec_size, hidden_size<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❶ 重みの初期値を定義</font><br>\n","$\\qquad$$\\qquad$rn = np.random.randn<font color=\"silver\"> # 正規分布からランダム</font><br>\n","$\\qquad$$\\qquad$embed_W = rn(V, D) / 100 <font color=\"silver\"> # 小さな値で初期化するため100で割る</font><br>\n","$\\qquad$$\\qquad$rnn_Wx = rn(D, H) * np.sqrt(2/(D+H)) <font color=\"silver\"> # Xavier, biasはゼロで初期化</font><br>\n","$\\qquad$$\\qquad$nn_Wh = rn(H, H) * np.sqrt(2/(H+H))<br>\n","$\\qquad$$\\qquad$rnn_b = np.zeros(H)<br>\n","$\\qquad$$\\qquad$affine_W = rn(H, V) * np.sqrt(2/(H+V))<br>\n","$\\qquad$$\\qquad$affine_b = np.zeros(V)<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❷ 隠れ層ををlistで定義</font><br>\n","$\\qquad$$\\qquad$self.layers = [<br>\n","$\\qquad$$\\qquad$$\\qquad$TimeEmbedding(embed_W),<br>\n","$\\qquad$$\\qquad$$\\qquad$TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True), <font color=\"silver\"> # 中間層を引き継ぐためTrue<br></font>\n","$\\qquad$$\\qquad$$\\qquad$TimeAffine(affine_W, affine_b) <br>\n","$\\qquad$$\\qquad$$\\qquad$]<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❸ 出力層を定義</font><br>\n","$\\qquad$$\\qquad$self.loss_layer = TimeSoftmaxWithLoss()</font><br>\n","$\\qquad$$\\qquad$self.rnn_layer = self.layers[1] <font color=\"silver\"> # def reset_state()で使用する値を定義</font><br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❹ 重みと勾配をListでまとめる</font><br>\n","$\\qquad$$\\qquad$self.params, self.grads = [], []<br>\n","$\\qquad$$\\qquad$for layer in self.layers:<br>\n","$\\qquad$$\\qquad$$\\qquad$self.params += layer.params <font color=\"silver\"> # リストを結合する<br></font>\n","$\\qquad$$\\qquad$$\\qquad$self.grads += layer.grads  <font color=\"silver\"> # リストを結合する<br></font>\n","$\\qquad$<font color=\"silver\"># ❺ 推論値算出（batch size, DataLoader）</font><br>\n","$\\qquad$def predict(self, xs):<br>\n","$\\qquad$$\\qquad$for layer in self.layers:<br>\n","$\\qquad$$\\qquad$$\\qquad$xs = layer.forward(xs)<br>\n","$\\qquad$$\\qquad$return xs<br>\n","$\\qquad$<font color=\"silver\"># ❻ 損失値算出（epoch, network size）</font><br>\n","$\\qquad$def forward(self, xs, ts):<br>\n","$\\qquad$$\\qquad$$\\qquad$xs = self.predict(xs)<br>\n","$\\qquad$$\\qquad$$\\qquad$loss = self.loss_layer.forward(xs, ts)<br>\n","$\\qquad$$\\qquad$$\\qquad$return loss<br>\n","$\\qquad$<font color=\"silver\"># ❼ 勾配値算出</font><br>\n","$\\qquad$def backward(self, dout=1):<br>\n","$\\qquad$$\\qquad$dout = self.loss_layer.backward(dout)<br>\n","$\\qquad$$\\qquad$for layer in reversed(self.layers):<br>\n","$\\qquad$$\\qquad$$\\qquad$dout = layer.backward(dout)<br>\n","$\\qquad$$\\qquad$return dout<br>\n","$\\qquad$def reset_state(self):<br>\n","$\\qquad$$\\qquad$self.rnn_layer.reset_state()<br>"],"metadata":{"id":"6kIOBGcl9JG3"}},{"cell_type":"markdown","source":["# <font color=\"silver\">実装│BetterRnnlm</font><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/35f6f0d26f9e58f01e4e)</font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F254c6004-ae48-ff43-a409-edd820d86dd0.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=7ef03e00836c07be1d96d46948cf20cf\" width=\"640\"><br><br>\n","<font color=\"silver\"># ❶ 重みの初期値を定義</font><br>\n","<font color=\"silver\"># ❷ 隠れ層ををlistで定義</font><br>\n","<font color=\"silver\"># ❸ 出力層を定義</font><br>\n","<font color=\"silver\"># ❹ 重みと勾配をListでまとめる</font><br>\n","<font color=\"silver\"># ❺ 推論値算出（batch size, DataLoader）</font><br>\n","<font color=\"silver\"># ❻ 損失値算出（epoch, network size）</font><br>\n","<font color=\"silver\"># ❼ 勾配値算出</font><br><br>\n","class BetterRnnlm(BaseModel):<br>\n","$\\qquad$def __ init __(self, vocab_size=10000, wordvec_size=650,<br>\n","$\\qquad$$\\qquad$$\\qquad$ hidden_size=650, dropout_ratio=0.5):<br>\n","$\\qquad$$\\qquad$V, D, H = vocab_size, wordvec_size, hidden_size<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❶ 重みの初期値をdictで定義</font><br>\n","$\\qquad$$\\qquad$rn = np.random.randn <font color=\"silver\"> # 正規分布からランダム</font><br>\n","$\\qquad$$\\qquad$embed_W = (rn(V, D) / 100).astype('f') <font color=\"silver\"> # 小さな値で初期化するため100で割る</font><br>\n","$\\qquad$$\\qquad$lstm_Wx1 = (rn(D, 4 * H) / np.sqrt(D)).astype('f') <font color=\"silver\"> # Xavier, 4×H, biasはゼロで初期化</font><br>\n","$\\qquad$$\\qquad$lstm_Wh1 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')<br>\n","$\\qquad$$\\qquad$lstm_b1 = np.zeros(4 * H).astype('f')<br>\n","$\\qquad$$\\qquad$lstm_Wx2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')<br>\n","$\\qquad$$\\qquad$lstm_Wh2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')<br>\n","$\\qquad$$\\qquad$lstm_b2 = np.zeros(4 * H).astype('f')<br>\n","$\\qquad$$\\qquad$affine_b = np.zeros(V).astype('f')<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❷ 隠れ層ををlistで定義</font><br>\n","$\\qquad$$\\qquad$self.layers = [<br>\n","$\\qquad$$\\qquad$$\\qquad$TimeEmbedding(embed_W),<br>\n","$\\qquad$$\\qquad$$\\qquad$TimeDropout(dropout_ratio),<br>\n","$\\qquad$$\\qquad$$\\qquad$TimeLSTM(lstm_Wx1, lstm_Wh1, lstm_b1, stateful=True),<br>\n","$\\qquad$$\\qquad$$\\qquad$TimeDropout(dropout_ratio),<br>\n","$\\qquad$$\\qquad$$\\qquad$TimeLSTM(lstm_Wx2, lstm_Wh2, lstm_b2, stateful=True),<br>\n","$\\qquad$$\\qquad$$\\qquad$TimeDropout(dropout_ratio),<br>\n","$\\qquad$$\\qquad$$\\qquad$TimeAffine(embed_W.T, affine_b) <font color=\"silver\"> # weight tying!!</font><font color=\"blue\">$~\\leftarrow\\ast$</font><br>\n","$\\qquad$$\\qquad$$\\qquad$]<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❸ 出力層を定義</font><br>\n","$\\qquad$$\\qquad$self.loss_layer = TimeSoftmaxWithLoss()<br>\n","$\\qquad$$\\qquad$self.lstm_layers = [self.layers[2], self.layers[4]] <font color=\"silver\"> # def reset_state()で使用する値を定義</font><br>\n","$\\qquad$$\\qquad$self.drop_layers = [self.layers[1], self.layers[3], self.layers[5]] <font color=\"silver\"> # def predict()で使用する値を定義</font><br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❹ 重みと勾配をListでまとめる</font><br>\n","$\\qquad$$\\qquad$self.params, self.grads = [], []<br>\n","$\\qquad$$\\qquad$for layer in self.layers:<br>\n","$\\qquad$$\\qquad$$\\qquad$self.params += layer.params<br>\n","$\\qquad$$\\qquad$$\\qquad$self.grads += layer.grads<br>\n","$\\qquad$<font color=\"silver\"># ❺ 推論値算出（batch size, DataLoader）</font><br>\n","$\\qquad$def predict(self, xs, train_flg=False):<br>\n","$\\qquad$$\\qquad$for layer in self.drop_layers:<br>\n","$\\qquad$$\\qquad$$\\qquad$layer.train_flg = train_flg<br>\n","$\\qquad$$\\qquad$for layer in self.layers:<br>\n","$\\qquad$$\\qquad$$\\qquad$xs = layer.forward(xs)<br>\n","$\\qquad$$\\qquad$return xs<br>\n","$\\qquad$<font color=\"silver\"># ❻ 損失値算出（epoch, network size）</font><br>\n","$\\qquad$def forward(self, xs, ts, train_flg=True):<br>\n","$\\qquad$$\\qquad$score = self.predict(xs, train_flg)<br>\n","$\\qquad$$\\qquad$loss = self.loss_layer.forward(score, ts)<br>\n","$\\qquad$$\\qquad$return loss<br>\n","$\\qquad$<font color=\"silver\"># ❼ 勾配値算出</font><br>\n","$\\qquad$def backward(self, dout=1):<br>\n","$\\qquad$$\\qquad$dout = self.loss_layer.backward(dout)<br>\n","$\\qquad$$\\qquad$for layer in reversed(self.layers):<br>\n","$\\qquad$$\\qquad$$\\qquad$dout = layer.backward(dout)<br>\n","$\\qquad$$\\qquad$return dout<br>\n","$\\qquad$def reset_state(self):<br>\n","$\\qquad$$\\qquad$for layer in self.lstm_layers:<br>\n","$\\qquad$$\\qquad$$\\qquad$layer.reset_state()<br>"],"metadata":{"id":"Qe0jNfmB-LMV"}},{"cell_type":"markdown","source":["# <font color=\"silver\">実装│Seq2seq [<font color=\"silver\">…</font>](https://colab.research.google.com/drive/1pUIwnT4DEkmBj3RRmEXkqSbk2gNXAiTI) \n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://d2l.ai/chapter_recurrent-modern/seq2seq.html)</font><br>\n","<img src=\"https://d2l.ai/_images/seq2seq.svg\" width=\"400\"><br>\n","<img src=\"https://d2l.ai/_images/seq2seq-predict.svg\" width=\"400\"><br><br>\n","class Encoder:<br>\n","$\\qquad$def __ init __ (self, vocab_size, wordvec_size, hidden_size):<br>\n","$\\qquad$$\\qquad$V, D, H = vocab_size, wordvec_size, hidden_size<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❶ 重みの初期値を定義</font><br>\n","$\\qquad$$\\qquad$rn = np.random.randn<font color=\"silver\"> # 正規分布からランダム</font><br>\n","$\\qquad$$\\qquad$embed_W = rn(V, D) / 100 <font color=\"silver\"> # 小さな値で初期化<br></font>\n","$\\qquad$$\\qquad$lstm_Wx = rn(D, 4 * H) * np.sqrt(2/(D+H)) <font color=\"silver\"> # Xavier, 4×H, biasはゼロで初期化<font color=\"blue\">$~\\leftarrow\\ast$</font></font><br>\n","$\\qquad$$\\qquad$lstm_Wh = rn(H, 4 * H) * np.sqrt(2/(H+H))<br>\n","$\\qquad$$\\qquad$lstm_b = np.zeros(4 * H)<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❷ 層を定義</font><br>\n","$\\qquad$$\\qquad$self.embed = TimeEmbedding(embed_W)<br>\n","$\\qquad$$\\qquad$self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❸ 重み・勾配・隠れ状態を定義</font><br>\n","$\\qquad$$\\qquad$self.params = self.embed.params + self.lstm.params<br>\n","$\\qquad$$\\qquad$self.grads = self.embed.grads + self.lstm.grads<br>\n","$\\qquad$$\\qquad$self.hs = None<br>\n","$\\qquad$def forward(self, xs):<br>\n","$\\qquad$$\\qquad$xs = self.embed.forward(xs)<br>\n","$\\qquad$$\\qquad$hs = self.lstm.forward(xs)<br>\n","$\\qquad$$\\qquad$self.hs = hs<br>\n","$\\qquad$$\\qquad$return hs[:, -1, :] <font color=\"silver\"> # 中間状態の最後だけreturnする</font><font color=\"blue\">$~\\leftarrow\\ast$ hs [ : , -1 , : ] </font><br>\n","$\\qquad$def backward(self, dh):<br>\n","$\\qquad$$\\qquad$dhs = np.zeros_like(self.hs)<font color=\"silver\"> # dhsの受け皿を作成して、</font><br>\n","$\\qquad$$\\qquad$dhs[:, -1, :] = dh<font color=\"silver\"> # Decoderからのhsをセットする</font><font color=\"blue\">$~\\leftarrow\\ast$ dhs [ : , -1 , : ] </font><br>\n","$\\qquad$$\\qquad$dout = self.lstm.backward(dhs)<br>\n","$\\qquad$$\\qquad$dout = self.embed.backward(dout)<br>\n","$\\qquad$$\\qquad$return dout<br>\n","class Decoder:<br>\n","$\\qquad$def __ init __ (self, vocab_size, wordvec_size, hidden_size):<br>\n","$\\qquad$$\\qquad$V, D, H = vocab_size, wordvec_size, hidden_size<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❶ 重みの初期値を定義</font><br>\n","$\\qquad$$\\qquad$rn = np.random.randn<font color=\"silver\"> # 正規分布からランダム</font><br>\n","$\\qquad$$\\qquad$embed_W = rn(V, D) / 100 <font color=\"silver\"> # 小さな値で初期化<br></font>\n","$\\qquad$$\\qquad$lstm_Wx = rn(D, 4 * H) * np.sqrt(2/(D+H)) <font color=\"silver\"> # Xavier, 4×H, biasはゼロで初期化<font color=\"blue\">$~\\leftarrow\\ast$</font></font><br>\n","$\\qquad$$\\qquad$lstm_Wh = rn(H, 4 * H) * np.sqrt(2/(H+H))<br>\n","$\\qquad$$\\qquad$lstm_b = np.zeros(4 * H)<br>\n","$\\qquad$$\\qquad$affine_W = rn(H, V) * np.sqrt(2/(H+V))<br>\n","$\\qquad$$\\qquad$affine_b = np.zeros(V)<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❷ 層を定義</font><br>\n","$\\qquad$$\\qquad$self.embed = TimeEmbedding(embed_W)<br>\n","$\\qquad$$\\qquad$self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)<br>\n","$\\qquad$$\\qquad$self.affine = TimeAffine(affine_W, affine_b)<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❸ 重みと勾配をlistにまとめる</font><br>\n","$\\qquad$$\\qquad$self.params, self.grads = [], []<br>\n","$\\qquad$$\\qquad$for layer in (self.embed, self.lstm, self.affine):<br>\n","$\\qquad$$\\qquad$$\\qquad$self.params += layer.params<br>\n","$\\qquad$$\\qquad$$\\qquad$self.grads += layer.grads<br>\n","$\\qquad$def forward(self, xs, h): <font color=\"silver\"> # xsは教師強制用</font><font color=\"blue\">$~\\leftarrow\\ast$</font><br>\n","$\\qquad$$\\qquad$self.lstm.set_state(h) <font color=\"silver\"> # 隠れ状態hをセットする</font><font color=\"blue\">$~\\leftarrow\\ast$</font><br>\n","$\\qquad$$\\qquad$out = self.embed.forward(xs)<br>\n","$\\qquad$$\\qquad$out = self.lstm.forward(out)<br>\n","$\\qquad$$\\qquad$score = self.affine.forward(out)<br>\n","$\\qquad$$\\qquad$return score<br>\n","$\\qquad$def backward(self, dscore):<br>\n","$\\qquad$$\\qquad$dout = self.affine.backward(dscore)<br>\n","$\\qquad$$\\qquad$dout = self.lstm.backward(dout)<br>\n","$\\qquad$$\\qquad$dout = self.embed.backward(dout)<br>\n","$\\qquad$$\\qquad$dh = self.lstm.dh<br>\n","$\\qquad$$\\qquad$return dh<br>\n","$\\qquad$def generate(self, h, start_id, sample_size):<br>\n","$\\qquad$$\\qquad$sampled = []<br>\n","$\\qquad$$\\qquad$sample_id = start_id<br>\n","$\\qquad$$\\qquad$self.lstm.set_state(h)<br>\n","$\\qquad$$\\qquad$for _ in range(sample_size):<br>\n","$\\qquad$$\\qquad$$\\qquad$x = np.array(sample_id).reshape((1, 1))<br>\n","$\\qquad$$\\qquad$$\\qquad$out = self.embed.forward(x)<br>\n","$\\qquad$$\\qquad$$\\qquad$out = self.lstm.forward(out)<br>\n","$\\qquad$$\\qquad$$\\qquad$score = self.affine.forward(out)<br>\n","$\\qquad$$\\qquad$$\\qquad$sample_id = np.argmax(score.flatten())<br>\n","$\\qquad$$\\qquad$$\\qquad$sampled.append(int(sample_id))<br>\n","$\\qquad$$\\qquad$return sampled<br>\n","class Seq2seq(BaseModel):<br>\n","$\\qquad$def __ init __ (self, vocab_size, wordvec_size, hidden_size):<br>\n","$\\qquad$$\\qquad$V, D, H = vocab_size, wordvec_size, hidden_size<br>\n","$\\qquad$$\\qquad$self.encoder = Encoder(V, D, H) <br>\n","$\\qquad$$\\qquad$self.decoder = Decoder(V, D, H)<br>\n","$\\qquad$$\\qquad$self.softmax = TimeSoftmaxWithLoss()<br>\n","$\\qquad$$\\qquad$self.params = self.encoder.params + self.decoder.params<br>\n","$\\qquad$$\\qquad$self.grads = self.encoder.grads + self.decoder.grads<br>\n","$\\qquad$def forward(self, xs, ts):<br>\n","$\\qquad$$\\qquad$decoder_xs = ts[:, :-1] <font color=\"silver\"> # 最後の単語以外を入力とする</font><font color=\"blue\">$~\\leftarrow\\ast$ ts [: , :-1] </font><br>\n","$\\qquad$$\\qquad$decoder_ts = ts[:, 1:] <font color=\"silver\"> # 最初の単語以外を教師とする</font><font color=\"blue\">$~\\leftarrow\\ast$ ts[: , 1:] </font><br>\n","$\\qquad$$\\qquad$h = self.encoder.forward(xs)<br>\n","$\\qquad$$\\qquad$score = self.decoder.forward(decoder_xs, h)<br>\n","$\\qquad$$\\qquad$loss = self.softmax.forward(score, decoder_ts)<br>\n","$\\qquad$$\\qquad$return loss<br>\n","$\\qquad$def backward(self, dout=1):<br>\n","$\\qquad$$\\qquad$dout = self.softmax.backward(dout)<br>\n","$\\qquad$$\\qquad$dh = self.decoder.backward(dout)<br>\n","$\\qquad$$\\qquad$dout = self.encoder.backward(dh)<br>\n","$\\qquad$$\\qquad$return dout<br>\n","$\\qquad$def generate(self, xs, start_id, sample_size):<br>\n","$\\qquad$$\\qquad$h = self.encoder.forward(xs)<br>\n","$\\qquad$$\\qquad$sampled = self.decoder.generate(h, start_id, sample_size)<br>\n","$\\qquad$$\\qquad$return sampled<br>"],"metadata":{"id":"_YzD0UHQ-4mO"}},{"cell_type":"markdown","source":["# <font color=\"silver\">実装│AttentionBiSeq2seq  [<font color=\"silver\">…</font>](https://colab.research.google.com/drive/1Aj__Nmnz0YbJaY0ExzKk7cReBzxy3lXg) [<font color=\"silver\">…</font>](https://colab.research.google.com/drive/1s80J0hX0uxg6WRQrvDtekM6ORNGkdMXj)</font><br>\n","class TimeBiLSTM:<br>\n","$\\qquad$def __ init __ (self, Wx1, Wh1, b1, Wx2, Wh2, b2, stateful=False):<br>\n","$\\qquad$$\\qquad$self.forward_lstm = TimeLSTM(Wx1, Wh1, b1, stateful)<br>\n","$\\qquad$$\\qquad$self.backward_lstm = TimeLSTM(Wx2, Wh2, b2, stateful)<br>\n","$\\qquad$$\\qquad$self.params = self.forward_lstm.params + self.backward_lstm.params<br>\n","$\\qquad$$\\qquad$self.grads = self.forward_lstm.grads + self.backward_lstm.grads<br>\n","$\\qquad$def forward(self, xs):<br>\n","$\\qquad$$\\qquad$o1 = self.forward_lstm.forward(xs)<br>\n","$\\qquad$$\\qquad$o2 = self.backward_lstm.forward(xs[:, ::-1,:]) <font color=\"silver\"> # xsを逆順にして入力する<br></font>\n","$\\qquad$$\\qquad$o2 = o2[:, ::-1,:] <font color=\"silver\"> # 結果を逆順にする<br></font>\n","$\\qquad$$\\qquad$out = np.concatenate((o1, o2), axis=2)  <font color=\"silver\"> # 順方向と逆方向の結果を結合する<br></font>\n","$\\qquad$$\\qquad$return out<br>\n","$\\qquad$def backward(self, dhs):<br>\n","$\\qquad$$\\qquad$H = dhs.shape[2] // 2<br>\n","$\\qquad$$\\qquad$do1 = dhs[:, :, :H]<br>\n","$\\qquad$$\\qquad$do2 = dhs[:, :, H:]<br>\n","$\\qquad$$\\qquad$dxs1 = self.forward_lstm.backward(do1)<br>\n","$\\qquad$$\\qquad$do2 = do2[:, ::-1]<br>\n","$\\qquad$$\\qquad$dxs2 = self.backward_lstm.backward(do2)<br>\n","$\\qquad$$\\qquad$dxs2 = dxs2[:, ::-1]<br>\n","$\\qquad$$\\qquad$dxs = dxs1 + dxs2<br>\n","$\\qquad$$\\qquad$return dxs<br>\n","class AttentionBiEncoder:<br>\n","$\\qquad$def __ init __ (self, vocab_size, wordvec_size, hidden_size):<br>\n","$\\qquad$$\\qquad$V, D, H = vocab_size, wordvec_size, hidden_size<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❶ 重みの初期値を定義</font><br>\n","$\\qquad$$\\qquad$rn = np.random.randn<font color=\"silver\"> # 正規分布からランダム</font><br>\n","$\\qquad$$\\qquad$embed_W = rn(V, D) / 100 <font color=\"silver\"> # 小さな値で初期化<br></font>\n","$\\qquad$$\\qquad$lstm_Wx_f = rn(D, 4 * H) * np.sqrt(2/(D+H))<font color=\"silver\"> # Xavier, 4×H, biasはゼロで初期化<font color=\"blue\">$~\\leftarrow\\ast$</font></font><br>\n","$\\qquad$$\\qquad$lstm_Wh_f = rn(H, 4 * H) * np.sqrt(2/(H+H))<br>\n","$\\qquad$$\\qquad$lstm_b_f = np.zeros(4 * H)<br>\n","$\\qquad$$\\qquad$lstm_Wx_b = rn(D, 4 * H) * np.sqrt(2/(D+H))<br>\n","$\\qquad$$\\qquad$lstm_Wh_b = rn(H, 4 * H) * np.sqrt(2/(H+H))<br>\n","$\\qquad$$\\qquad$lstm_b_b = np.zeros(4 * H)  <br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❷ 層を定義</font><br>\n","$\\qquad$$\\qquad$self.embed = TimeEmbedding(embed_W)<br>\n","$\\qquad$$\\qquad$self.lstm = TimeBiLSTM(lstm_Wx_f, lstm_Wh_f, lstm_b_f, lstm_Wx_b, lstm_Wh_b, lstm_b_b, stateful=False)<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❸ 重みと勾配をlistにまとめる</font><br>\n","$\\qquad$$\\qquad$self.params = self.embed.params + self.lstm.params<br>\n","$\\qquad$$\\qquad$self.grads = self.embed.grads + self.lstm.grads<br>\n","$\\qquad$$\\qquad$self.hs = None<br>\n","$\\qquad$<font color=\"silver\"># AttentionEncodeから継承<br></font>\n","$\\qquad$def forward(self, xs):<br>\n","$\\qquad$$\\qquad$xs = self.embed.forward(xs)<br>\n","$\\qquad$$\\qquad$hs = self.lstm.forward(xs) <font color=\"silver\"> # 中間状態の全てをreturnする</font><font color=\"blue\">$~\\leftarrow\\ast$</font></font><br>\n","$\\qquad$$\\qquad$return hs<br>\n","$\\qquad$<font color=\"silver\"># AttentionEncodeから継承<br></font>\n","$\\qquad$def backward(self, dhs):<br>\n","$\\qquad$$\\qquad$dout = self.lstm.backward(dhs) <font color=\"silver\"> #  Decoderから伝わってきた勾配を全て伝える</font><font color=\"blue\">$~\\leftarrow\\ast$</font></font><br>\n","$\\qquad$$\\qquad$dout = self.embed.backward(dout)<br>\n","$\\qquad$$\\qquad$return dout<br>\n","class AttentionDecoder:<br>\n","$\\qquad$def __ init __ (self, vocab_size, wordvec_size, hidden_size):<br>\n","$\\qquad$$\\qquad$V, D, H = vocab_size, wordvec_size, hidden_size<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❶ 重みの初期値を定義</font><br>\n","$\\qquad$$\\qquad$rn = np.random.randn<font color=\"silver\"> # 正規分布からランダム</font><br>\n","$\\qquad$$\\qquad$embed_W = rn(V, D) / 100 <font color=\"silver\"> # 小さな値で初期化<br></font>\n","$\\qquad$$\\qquad$lstm_Wx = rn(D, 4 * H) * np.sqrt(2/(D+H)) <font color=\"silver\"> # Xavier, 4×H, biasはゼロで初期化<font color=\"blue\">$~\\leftarrow\\ast$</font></font><br>\n","$\\qquad$$\\qquad$lstm_Wh = rn(H, 4 * H) * np.sqrt(2/(H+H))<br>\n","$\\qquad$$\\qquad$lstm_b = np.zeros(4 * H)<br>\n","$\\qquad$$\\qquad$affine_W_c = rn(2* H, V) * np.sqrt(2/(2*H+V))<font color=\"silver\"> # $c^t+h^t$を入力するため2×H<font color=\"blue\">$~\\leftarrow\\ast$</font></font><br>\n","$\\qquad$$\\qquad$affine_b_c = np.zeros(V)<br>\n","$\\qquad$$\\qquad$affine_W_s = rn(V, V) * np.sqrt(2/(V+V))<br>\n","$\\qquad$$\\qquad$affine_b_s = np.zeros(V)<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❷ 層を定義</font><br>\n","$\\qquad$$\\qquad$self.embed = TimeEmbedding(embed_W)<br>\n","$\\qquad$$\\qquad$self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)<br>\n","$\\qquad$$\\qquad$self.attention = TimeAttention()<br>\n","$\\qquad$$\\qquad$self.affine_c = TimeAffine(affine_W_c, affine_b_c)<br>\n","$\\qquad$$\\qquad$self.tanh = Tanh()<br>\n","$\\qquad$$\\qquad$self.affine_s = TimeAffine(affine_W_s, affine_b_s)<br>\n","$\\qquad$$\\qquad$layers = [self.embed, self.lstm, self.attention, self.affine_c, self.tanh, self.affine_s]<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❸ 重みと勾配をlistにまとめる</font><br>\n","$\\qquad$$\\qquad$self.params, self.grads = [], []<br>\n","$\\qquad$$\\qquad$for layer in layers:<br>\n","$\\qquad$$\\qquad$$\\qquad$self.params += layer.params<br>\n","$\\qquad$$\\qquad$$\\qquad$self.grads += layer.grads<br>\n","$\\qquad$def forward(self, xs, enc_hs):<br>\n","$\\qquad$$\\qquad$h = enc_hs[:,-1] <font color=\"silver\"> # enc_hsの最後だけを取り出す</font><font color=\"blue\">$~\\leftarrow\\ast$ enc_hs [ : , -1 ] </font><br>\n","$\\qquad$$\\qquad$self.lstm.set_state(h) <font color=\"silver\"> # 取り出した隠れ状態hをセットする</font><font color=\"blue\">$~\\leftarrow\\ast$</font><br>\n","$\\qquad$$\\qquad$out = self.embed.forward(xs)<br>\n","$\\qquad$$\\qquad$dec_hs = self.lstm.forward(out)<br>\n","$\\qquad$$\\qquad$c = self.attention.forward(enc_hs, dec_hs)<br></font>\n","$\\qquad$$\\qquad$out = np.concatenate((c, dec_hs), axis=2)<font color=\"blue\">$~\\leftarrow\\ast$ np.concatenate, axis=2</font><br>\n","$\\qquad$$\\qquad$out = self.affine_c.forward(out)<br>\n","$\\qquad$$\\qquad$out = self.tanh.forward(out)<br>\n","$\\qquad$$\\qquad$out = self.affine_s.forward(out)<br>\n","$\\qquad$$\\qquad$return out<br>\n","$\\qquad$def backward(self, dscore):<br>\n","$\\qquad$$\\qquad$dout = self.affine_s.backward(dscore)<br>\n","$\\qquad$$\\qquad$dout = self.tanh.backward(dout)<br>\n","$\\qquad$$\\qquad$dout = self.affine_c.backward(dout)<br>\n","$\\qquad$$\\qquad$N, T, H2 = dout.shape<br>\n","$\\qquad$$\\qquad$H = H2 // 2<br>\n","$\\qquad$$\\qquad$dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]<br>\n","$\\qquad$$\\qquad$denc_hs, ddec_hs1 = self.attention.backward(dc)<br>\n","$\\qquad$$\\qquad$ddec_hs = ddec_hs0 + ddec_hs1<br>\n","$\\qquad$$\\qquad$dout = self.lstm.backward(ddec_hs)<br>\n","$\\qquad$$\\qquad$dh = self.lstm.dh<br>\n","$\\qquad$$\\qquad$denc_hs[:, -1] += dh<br>\n","$\\qquad$$\\qquad$self.embed.backward(dout)<br>\n","$\\qquad$$\\qquad$return denc_hs<br>\n","$\\qquad$def generate(self, enc_hs, start_id, sample_size):<br>\n","$\\qquad$$\\qquad$sampled = []<br>\n","$\\qquad$$\\qquad$sample_id = start_id<br>\n","$\\qquad$$\\qquad$h = enc_hs[:, -1]<br>\n","$\\qquad$$\\qquad$self.lstm.set_state(h)<br>\n","$\\qquad$$\\qquad$for _ in range(sample_size):<br>\n","$\\qquad$$\\qquad$$\\qquad$x = np.array([sample_id]).reshape((1, 1))<br>\n","$\\qquad$$\\qquad$$\\qquad$out = self.embed.forward(x)<br>\n","$\\qquad$$\\qquad$$\\qquad$dec_hs = self.lstm.forward(out)<br>\n","$\\qquad$$\\qquad$$\\qquad$c = self.attention.forward(enc_hs, dec_hs)<br>\n","$\\qquad$$\\qquad$$\\qquad$out = np.concatenate((c, dec_hs), axis=2)<br>\n","$\\qquad$$\\qquad$$\\qquad$out = self.affine_c.forward(out)<br>\n","$\\qquad$$\\qquad$$\\qquad$out = self.tanh.forward(out)<br>\n","$\\qquad$$\\qquad$$\\qquad$out = self.affine_s.forward(out) <br>\n","$\\qquad$$\\qquad$$\\qquad$sample_id = np.argmax(out.flatten())<br>\n","$\\qquad$$\\qquad$$\\qquad$sampled.append(sample_id)<br>\n","$\\qquad$$\\qquad$return sampled<br>\n","class AttentionBiSeq2seq(Seq2seq):<br>\n","$\\qquad$def __ init __ (self, vocab_size, wordvec_size, hidden_size):<br>\n","$\\qquad$$\\qquad$V, D, H = vocab_size, wordvec_size, hidden_size<br>\n","$\\qquad$$\\qquad$self.encoder = AttentionBiEncoder(V, D, H)<br>\n","$\\qquad$$\\qquad$self.decoder = AttentionDecoder(V, D, H*2) <font color=\"silver\"> # 双方向の中間層を引数に取るためH×2</font><font color=\"blue\">$~\\leftarrow\\ast$</font><br>\n","$\\qquad$$\\qquad$self.softmax = TimeSoftmaxWithLoss()<br>\n","$\\qquad$$\\qquad$self.params = self.encoder.params + self.decoder.params<br>\n","$\\qquad$$\\qquad$self.grads = self.encoder.grads + self.decoder.grads<br>\n","$\\qquad$<font color=\"silver\"># Seq2seqから継承</font><br>\n","$\\qquad$def forward(self, xs, ts):<br>\n","$\\qquad$$\\qquad$decoder_xs = ts[:, :-1] <font color=\"silver\"> # 最後の単語以外を入力とする</font><font color=\"blue\">$~\\leftarrow\\ast$ ts [: , :-1] </font><br>\n","$\\qquad$$\\qquad$decoder_ts = ts[:, 1:] <font color=\"silver\"> # 最初の単語以外を教師とする</font><font color=\"blue\">$~\\leftarrow\\ast$ ts[: , 1:] </font><br>\n","$\\qquad$$\\qquad$h = self.encoder.forward(xs)<br>\n","$\\qquad$$\\qquad$score = self.decoder.forward(decoder_xs, h)<br>\n","$\\qquad$$\\qquad$loss = self.softmax.forward(score, decoder_ts)<br>\n","$\\qquad$$\\qquad$return loss<br>\n","$\\qquad$<font color=\"silver\"># Seq2seqから継承</font><br>\n","$\\qquad$def backward(self, dout=1):<br>\n","$\\qquad$$\\qquad$dout = self.softmax.backward(dout)<br>\n","$\\qquad$$\\qquad$dh = self.decoder.backward(dout)<br>\n","$\\qquad$$\\qquad$dout = self.encoder.backward(dh)<br>\n","$\\qquad$$\\qquad$return dout<br>\n","$\\qquad$<font color=\"silver\"># 生成</font><br>\n","$\\qquad$def generate(self, xs, start_id, sample_size):<br>\n","$\\qquad$$\\qquad$h = self.encoder.forward(xs)<br>\n","$\\qquad$$\\qquad$sampled = self.decoder.generate(h, start_id, sample_size)<br>\n","$\\qquad$$\\qquad$return sampled<br>\n"],"metadata":{"id":"8f_wbkMb_OCz"}},{"cell_type":"markdown","source":["# <font color=\"silver\">実装│PytorchLSTM [<font color=\"silver\">…</font>](https://colab.research.google.com/drive/1qRQcCIyHKN4fzL0aTx2p8PKHv14nIG8i#scrollTo=TEDuS8lJwxii)</font><br>\n","\n","class BiLSTM(nn.Module):<br>\n","$\\qquad$def __ init __ (self):<br>\n","$\\qquad$$\\qquad$super(BiLSTM, self). __ init __ ()<br>\n","$\\qquad$$\\qquad$self.lstm = nn.LSTM(1, 1024, 2, batch_first = True, bidirectional = True)<font color=\"silver\">  # (input, hidden, num_layers) </font><br>\n","$\\qquad$$\\qquad$self.fc = nn.Linear(2 * 1024, 1)<font color=\"silver\">  # (Bidirectional×hidden, output)</font><br>\n","$\\qquad$def forward(self, x):<br>\n","$\\qquad$$\\qquad$num_layers = 2 <font color=\"silver\">  # (Bidirectional×num_layers)</font><br>\n","$\\qquad$$\\qquad$h0 = torch.zeros(2 * num_layers, x.size(0), 1024).requires_grad_().to(device)<font color=\"silver\">  # hの初期化</font><br>\n","$\\qquad$$\\qquad$c0 = torch.zeros(2 * num_layers, x.size(0), 1024).requires_grad_().to(device)<font color=\"silver\">  # cの初期化</font><br>\n","$\\qquad$$\\qquad$output, (h_n, c_n) = self.lstm(x, (h0.detach(), c0.detach())<font color=\"silver\">  # .detach()は勾配を計算しない</font><br>\n","$\\qquad$$\\qquad$output = self.fc(output[:, -1, :]) <font color=\"silver\">  # 最後のLSTMの値</font><br>\n","$\\qquad$$\\qquad$return output<br>\n","class BiLSTM(nn.Module):<br>\n","$\\qquad$def__ init __  (self, in_dim, h_dim, out_dim, num_layers=1):<br>\n","$\\qquad$$\\qquad$super(BiLSTM, self). __ init __ ()<br>\n","$\\qquad$$\\qquad$self.h_dim = h_dim<br>\n","$\\qquad$$\\qquad$self.num_layers = num_layers<br>\n","$\\qquad$$\\qquad$self.lstm = nn.LSTM(<br>\n","$\\qquad$$\\qquad$$\\qquad$input_size = in_dim, <br>\n","$\\qquad$$\\qquad$$\\qquad$hidden_size = h_dim, <br>\n","$\\qquad$$\\qquad$$\\qquad$num_layers = num_layers, <br>\n","$\\qquad$$\\qquad$$\\qquad$bias = True, <br>\n","$\\qquad$$\\qquad$$\\qquad$batch_first = True, <br>\n","$\\qquad$$\\qquad$$\\qquad$dropout = 0, <br>\n","$\\qquad$$\\qquad$$\\qquad$bidirectional = True)<br>\n","$\\qquad$$\\qquad$self.fc = nn.Linear(h_dim * 2, out_dim)<br>\n","$\\qquad$def forward(self, x):<br>\n","$\\qquad$$\\qquad$h0 = torch.zeros(self.num_layers * 2, x.size(0), self.h_dim).requires _ grad_().to(device) <font color=\"silver\"> # hの初期化</font><br>\n","$\\qquad$$\\qquad$c0 = torch.zeros(self.num_layers * 2, x.size(0), self.h_dim).requires _ grad_().to(device) <font color=\"silver\"> # hの初期化</font><br>\n","$\\qquad$$\\qquad$output, (h_n, c_n) = self.lstm(x, (h0.detach(), c0.detach())) <font color=\"silver\"> # .detach()は勾配を計算しない</font><br>\n","$\\qquad$$\\qquad$output = self.fc(output[:, -1, :])<br>\n","$\\qquad$$\\qquad$return output<br>"],"metadata":{"id":"lEgkvhgKDOTU"}},{"cell_type":"markdown","source":["# <font color=\"silver\">実装│PytorchSeq2Seq  [<font color=\"silver\">…</font>](https://colab.research.google.com/drive/1SRegu9UpX7LS9eWQykxx5PVbVvwFWmTF#scrollTo=1Kmpa-cMH-GF) [<font color=\"silver\">…</font>](https://colab.research.google.com/drive/1X-KVnHyFFj0wmWgnfQ_2plWTNNGP9SSQ)</font><br>\n","<font color=\"silver\"># ❶ データセットの準備</font><br>\n","<font color=\"silver\"># ❷ ネットワークの準備</font><br>\n","class Encoder(nn.Module):</font><br>\n","$\\qquad$def  __ init __ (self):</font><br>\n","$\\qquad$$\\qquad$super(). __ init __ ()</font><br>\n","$\\qquad$$\\qquad$self.embedding = nn.Embedding(19215, 32)  <font color=\"silver\"># (vocab_dim_SRC, emb_dim)</font><br>\n","$\\qquad$$\\qquad$self.rnn = nn.GRU(32 ,64 , bidirectional = True)  <font color=\"silver\"># (emb_dim, enc_hid_dim)</font><br>\n","$\\qquad$$\\qquad$self.fc = nn.Linear(2 * 64, 64)  <font color=\"silver\"># (Bidirectional * enc_hid_dim, dec_hid_dim)</font><br>\n","$\\qquad$$\\qquad$self.dropout = nn.Dropout(0.5)</font><br>\n","$\\qquad$def forward(self, src):</font><br>\n","$\\qquad$$\\qquad$embedded = self.dropout(self.embedding(src))</font><br>\n","$\\qquad$$\\qquad$outputs, hidden = self.rnn(embedded)</font><br>\n","$\\qquad$$\\qquad$hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))</font><br>\n","$\\qquad$$\\qquad$return outputs, hidden</font><br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># 中間状態[batch_size, enc_hid_dim]</font><br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># 双方向GRUでは、[順方向、逆方向、順方向...]といった形で中間状態が保存される</font><br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># 最後の層における順方向と逆方向の中間状態を、enc_hid_dimの軸に沿って連結</font><br>\n","class Decoder(nn.Module):</font><br>\n","$\\qquad$def  __ init __ (self):</font><br>\n","$\\qquad$$\\qquad$super(). __ init __ ()</font><br>\n","$\\qquad$$\\qquad$self.embedding = nn.Embedding(10838, 32)  <font color=\"silver\"># (10838, emb_dim)</font><br>\n","$\\qquad$$\\qquad$self.rnn = nn.GRU(32, 64)  <font color=\"silver\"># (emb_dim, enc_hid_dim)</font><br>\n","$\\qquad$$\\qquad$self.out = nn.Linear(64 + 32, 10838)  <font color=\"silver\"># (enc_hid_dim + emb_dim, output_dim)</font><br>\n","$\\qquad$$\\qquad$self.dropout = nn.Dropout(0.5)</font><br>\n","$\\qquad$def forward(self, input, decoder_hidden, encoder_outputs) </font><br>\n","$\\qquad$$\\qquad$input = input.unsqueeze(0)<br>\n","$\\qquad$$\\qquad$embedded = self.dropout(self.embedding(input))</font><br>\n","$\\qquad$$\\qquad$output, decoder_hidden = self.rnn(embedded, decoder_hidden.unsqueeze(0))<br>\n","$\\qquad$$\\qquad$embedded = embedded.squeeze(0)<br>\n","$\\qquad$$\\qquad$output = output.squeeze(0)<br>\n","$\\qquad$$\\qquad$output = self.out(torch.cat((output, embedded), dim = 1) )  <font color=\"silver\"># embeddedとGRUのoutputのベクトルを連結して全結合層へ</font><br>\n","$\\qquad$$\\qquad$return output, decoder_hidden.squeeze(0)</font><br>\n","class Seq2Seq(nn.Module):</font><br>\n","$\\qquad$def  __ init __ (self, encoder, decoder, device):</font><br>\n","$\\qquad$$\\qquad$super(). __ init __ ()</font><br>\n","$\\qquad$$\\qquad$self.encoder = encoder</font><br>\n","$\\qquad$$\\qquad$self.decoder = decoder</font><br>\n","$\\qquad$$\\qquad$self.device = device</font><br>\n","$\\qquad$def forward(self, src, trg, teacher_forcing_ratio: float = 0.5):</font><br>\n","$\\qquad$$\\qquad$batch_size = src.shape[1]</font><br>\n","$\\qquad$$\\qquad$max_len = trg.shape[0]</font><br>\n","$\\qquad$$\\qquad$trg_vocab_size = self.decoder.output_dim</font><br>\n","$\\qquad$$\\qquad$outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)</font><br>\n","$\\qquad$$\\qquad$outputs[:, :, BOS_IDX] = 1  <font color=\"silver\"># [BOS] のIDをセット（onehot）</font><br>\n","$\\qquad$$\\qquad$encoder_outputs, hidden = self.encoder(src)</font><br>\n","$\\qquad$$\\qquad$output = trg[0,:]  <font color=\"silver\"># デコーダの最初の入力 [BOS] トークン</font><br>\n","$\\qquad$$\\qquad$for t in range(1, max_len):</font><br>\n","$\\qquad$$\\qquad$$\\qquad$output, hidden = self.decoder(output, hidden, encoder_outputs)</font><br>\n","$\\qquad$$\\qquad$$\\qquad$outputs[t] = output</font><br>\n","$\\qquad$$\\qquad$$\\qquad$teacher_force = random.random() < teacher_forcing_ratio</font><br>\n","$\\qquad$$\\qquad$$\\qquad$top1 = output.max(1)[1]  <font color=\"silver\"># 予測したトークンのID取得</font><br>\n","$\\qquad$$\\qquad$$\\qquad$output = (trg[t] if teacher_force else top1)</font><br>\n","$\\qquad$$\\qquad$return outputs</font><br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># 教師強制オンの場合はターゲットソースのIDを</font><br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># オフの場合は予測したトークンのIDを次の入力にする</font><br>\n","<font color=\"silver\"># ❸ モデルの準備（Model, Criterion, Optimizer）</font><br>\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'</font><br>\n","enc = Encoder()</font><br>\n","dec = Decoder()</font><br>\n","def init_weights(m: nn.Module):</font><br>\n","$\\qquad$for name, param in m.named_parameters():</font><br>\n","$\\qquad$$\\qquad$if 'weight' in name:</font><br>\n","$\\qquad$$\\qquad$$\\qquad$nn.init.normal_(param.data, mean=0, std=0.01)</font><br>\n","$\\qquad$$\\qquad$else:</font><br>\n","$\\qquad$$\\qquad$$\\qquad$nn.init.constant_(param.data, 0)</font><br>\n","model.apply(init_weights)</font><br>\n","<font color=\"silver\"># ❹ 学習の準備（batch size, DataLoader）</font><br>\n","def train(model, optimizer, criterion, clip):</font><br>\n","$\\qquad$model.train()</font><br>\n","$\\qquad$epoch_loss = 0</font><br>\n","$\\qquad$train_iter = Multi30k(root='./data', split='train', language_pair=(SRC_LANG, TRG_LANG))</font><br>\n","$\\qquad$train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)</font><br>\n","$\\qquad$for src, trg in train_dataloader:</font><br>\n","$\\qquad$$\\qquad$src = src.to(device)</font><br>\n","$\\qquad$$\\qquad$trg = trg.to(device)</font><br>\n","$\\qquad$$\\qquad$output = model(src, trg)  <font color=\"silver\"># 教師強制 オン</font><br>\n","$\\qquad$$\\qquad$output = output[1:].view(-1, output.shape[-1])  <font color=\"silver\"># 出力単語のIDを取り出す</font><br>\n","$\\qquad$$\\qquad$trg = trg[1:].view(-1)</font><br>\n","$\\qquad$$\\qquad$loss = criterion(output, trg)</font><br>\n","$\\qquad$$\\qquad$optimizer.zero_grad()</font><br>\n","$\\qquad$$\\qquad$loss.backward()</font><br>\n","$\\qquad$$\\qquad$torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  <font color=\"silver\"># 勾配クリッピング</font><br>\n","$\\qquad$$\\qquad$optimizer.step()</font><br>\n","$\\qquad$$\\qquad$epoch_loss += loss.item()</font><br>\n","$\\qquad$return epoch_loss / len(train_dataloader)</font><br>\n","def evaluate(model, criterion, mode='valid'):</font><br>\n","$\\qquad$model.eval()</font><br>\n","$\\qquad$epoch_loss = 0</font><br>\n","$\\qquad$tmp_iter = Multi30k(root='./data', split=mode, language_pair=(SRC_LANG, TRG_LANG))</font><br>\n","$\\qquad$tmp_dataloader = DataLoader(tmp_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)</font><br>\n","$\\qquad$with torch.no_grad():  <font color=\"silver\"># 勾配を更新しない</font><br>\n","$\\qquad$$\\qquad$for src, trg in tmp_dataloader:</font><br>\n","$\\qquad$$\\qquad$$\\qquad$src = src.to(device)</font><br>\n","$\\qquad$$\\qquad$$\\qquad$trg = trg.to(device)</font><br>\n","$\\qquad$$\\qquad$$\\qquad$output = model(src, trg, 0)  <font color=\"silver\"># 教師強制 オフ</font><br>\n","$\\qquad$$\\qquad$$\\qquad$output = output[1:].view(-1, output.shape[-1])  <font color=\"silver\"># 出力単語のIDを取り出す</font><br>\n","$\\qquad$$\\qquad$$\\qquad$trg = trg[1:].view(-1)</font><br>\n","$\\qquad$$\\qquad$$\\qquad$loss = criterion(output, trg)</font><br>\n","$\\qquad$$\\qquad$$\\qquad$epoch_loss += loss.item()</font><br>\n","$\\qquad$$\\qquad$return epoch_loss / len(tmp_dataloader)</font><br>\n","def test_output(model):</font><br>\n","$\\qquad$$\\qquad$model.eval()</font><br>\n","$\\qquad$$\\qquad$tmp_iter = Multi30k(root='./data', split='test', language_pair=(SRC_LANG, TRG_LANG))</font><br>\n","$\\qquad$$\\qquad$tmp_dataloader = DataLoader(tmp_iter, batch_size=1, collate_fn=collate_fn)</font><br>\n","$\\qquad$$\\qquad$with torch.no_grad():  <font color=\"silver\"># 勾配を更新しない</font><br>\n","$\\qquad$$\\qquad$$\\qquad$src, trg = next(iter(tmp_dataloader))  <font color=\"silver\"># データの取り出し</font><br>\n","$\\qquad$$\\qquad$$\\qquad$src = src.to(device)</font><br>\n","$\\qquad$$\\qquad$$\\qquad$trg = trg.to(device)   </font><br>\n","$\\qquad$$\\qquad$$\\qquad$output = model(src, trg, 0)  <font color=\"silver\"># 教師強制をオフ</font><br>\n","$\\qquad$$\\qquad$return src, trg, output</font><br>\n","<font color=\"silver\"># ❺ 学習（epoch, network size）</font><br>\n","<font color=\"silver\"># ❻ 検証</font><br>"],"metadata":{"id":"nAzvMGPbDhOM"}},{"cell_type":"markdown","source":["# <font color=\"silver\">実装│PytorchTrasnformer</font>  [<font color=\"silver\">…</font>](https://colab.research.google.com/drive/1sz1WL6gj8SuvT-JNK5QQsMxHTDqJ2e9u) <br>\n","class Embedder(nn.Module):<br>\n","$\\qquad$def  __ init __ (self, text_embedding_vectors):<br>\n","$\\qquad$$\\qquad$super(Embedder, self). __ init __ ()<br>\n","$\\qquad$$\\qquad$self.embeddings = nn.Embedding.from_pretrained(text_embedding_vectors, freeze=True) <font color=\"silver\"># freeze=Trueで更新しない</font><br>\n","$\\qquad$def forward(self, x):<br>\n","$\\qquad$$\\qquad$x_vec = self.embeddings(x)<br>\n","$\\qquad$$\\qquad$return x_vec<br>\n","class PositionalEncoder(nn.Module):<br>\n","$\\qquad$def  __ init __ (self, d_model=300, max_seq_len=256):<br>\n","$\\qquad$$\\qquad$super(). __ init __ ()<br>\n","$\\qquad$$\\qquad$self.d_model = d_model  <font color=\"silver\"># 埋め込みベクトルの次元数</font><br>\n","$\\qquad$$\\qquad$pe = torch.zeros(max_seq_len, d_model) <font color=\"silver\"># PositionalEncodeを格納する受け皿, 零テンソル</font><br>\n","$\\qquad$$\\qquad$for pos in range(max_seq_len):<br>\n","$\\qquad$$\\qquad$$\\qquad$for i in range(0, d_model, 2):<br>\n","$\\qquad$$\\qquad$$\\qquad$$\\qquad$pe[pos, i] = math.sin(pos / (10000 ** ((2*i)/d_model)))<br>\n","$\\qquad$$\\qquad$$\\qquad$$\\qquad$pe[pos, i+1] = math.cos(pos / (10000 ** ((2*i)/d_model)))<br>\n","$\\qquad$$\\qquad$$\\qquad$self.pe = pe.unsqueeze(0)  <font color=\"silver\"># peの先頭に、ミニバッチを表す次元を追加</font><br>\n","$\\qquad$$\\qquad$$\\qquad$self.pe.requires_grad = False  <font color=\"silver\"># 勾配を計算しないようにする</font><br>\n","$\\qquad$def forward(self, x): <font color=\"silver\"># 入力xとpeを足し合わせる, xがpeよりも小さいため次元数の平方根を掛けて拡大</font><br>\n","$\\qquad$$\\qquad$$\\qquad$ret = math.sqrt(self.d_model)*x + self.pe<br>\n","$\\qquad$$\\qquad$$\\qquad$return ret<br>\n","class Attention(nn.Module):<br>\n","$\\qquad$def  __ init __ (self, d_model=300):<br>\n","$\\qquad$$\\qquad$super(). __ init __ ()<br>\n","$\\qquad$$\\qquad$self.q_linear = nn.Linear(d_model, d_model)<br>\n","$\\qquad$$\\qquad$self.k_linear = nn.Linear(d_model, d_model)<br>\n","$\\qquad$$\\qquad$self.v_linear = nn.Linear(d_model, d_model)<br>\n","$\\qquad$$\\qquad$self.out = nn.Linear(d_model, d_model)<br>\n","$\\qquad$$\\qquad$self.d_k = d_model<br>\n","$\\qquad$def forward(self, q, k, v, mask):<br>\n","$\\qquad$$\\qquad$q = self.q_linear(q)<br>\n","$\\qquad$$\\qquad$k = self.k_linear(k)<br>\n","$\\qquad$$\\qquad$v = self.v_linear(v)<br>\n","$\\qquad$$\\qquad$weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.d_k)<br>\n","$\\qquad$$\\qquad$mask = mask.unsqueeze(1)<br>\n","$\\qquad$$\\qquad$weights = weights.masked_fill(mask == 0, -1e9)<br>\n","$\\qquad$$\\qquad$normlized_weights = F.softmax(weights, dim=-1)<br>\n","$\\qquad$$\\qquad$output = torch.matmul(normlized_weights, v)<br>\n","$\\qquad$$\\qquad$output = self.out(output)<br>\n","$\\qquad$$\\qquad$return output, normlized_weights<br>\n","class FeedForward(nn.Module):<br>\n","$\\qquad$def  __ init __ (self, d_model, d_ff=1024, dropout=0.1):<br>\n","$\\qquad$$\\qquad$super(). __ init __ ()<br>\n","$\\qquad$$\\qquad$self.linear_1 = nn.Linear(d_model, d_ff)<br>\n","$\\qquad$$\\qquad$self.dropout = nn.Dropout(dropout)<br>\n","$\\qquad$$\\qquad$self.linear_2 = nn.Linear(d_ff, d_model)<br>\n","$\\qquad$def forward(self, x):<br>\n","$\\qquad$$\\qquad$x = self.linear_1(x)<br>\n","$\\qquad$$\\qquad$x = self.dropout(F.relu(x))<br>\n","$\\qquad$$\\qquad$x = self.linear_2(x)<br>\n","$\\qquad$$\\qquad$return x<br>\n","class TransformerBlock(nn.Module):<br>\n","$\\qquad$def  __ init __ (self, d_model, dropout=0.1):<br>\n","$\\qquad$$\\qquad$super(). __ init __ ()<br>\n","$\\qquad$$\\qquad$self.norm_1 = nn.LayerNorm(d_model)<br>\n","$\\qquad$$\\qquad$self.norm_2 = nn.LayerNorm(d_model)<br>\n","$\\qquad$$\\qquad$self.attn = Attention(d_model)<br>\n","$\\qquad$$\\qquad$self.ff = FeedForward(d_model)<br>\n","$\\qquad$$\\qquad$self.dropout_1 = nn.Dropout(dropout)<br>\n","$\\qquad$$\\qquad$self.dropout_2 = nn.Dropout(dropout)<br>\n","$\\qquad$def forward(self, x, mask):<br>\n","$\\qquad$$\\qquad$x_normlized = self.norm_1(x)<br>\n","$\\qquad$$\\qquad$output, normlized_weights = self.attn(x_normlized, x_normlized, x_normlized, mask)<br>\n","$\\qquad$$\\qquad$x2 = x + self.dropout_1(output)<br>\n","$\\qquad$$\\qquad$x_normlized2 = self.norm_2(x2)<br>\n","$\\qquad$$\\qquad$output = x2 + self.dropout_2(self.ff(x_normlized2))<br>\n","$\\qquad$$\\qquad$return output, normlized_weights<br>\n"],"metadata":{"id":"OCdtmua8E8tc"}},{"cell_type":"markdown","source":["# <font color=\"silver\">実装│PytorchBert\n","https://drive.google.com/drive/folders/1w-l99a34dGbbmu1O93vluMObS8d1bdqq\n","\n","https://colab.research.google.com/github/machine-perception-robotics-group/JDLALectureNotebooks/blob/master/notebooks/32_bert.ipynb"],"metadata":{"id":"3iAyVC_0vxYu"}},{"cell_type":"markdown","source":["# <font color=\"silver\">実装│appendix, PytorchCNNNet</font><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://rightcode.co.jp/blog/information-technology/pytorch-cifar-10-cnn-learning)</font></font><br>\n","<img src=\"https://rightcode.co.jp/wp-content/uploads/2020/02/pytorch_lenet.png\" width=\"640\"><br><br>\n","class LeNet(torch.nn.Module):<br>\n","$\\qquad$def __ init __ (self):<br>\n","$\\qquad$$\\qquad$super(MyCNN, self). __ init __ ()<br>\n","$\\qquad$$\\qquad$self.conv1 = torch.nn.Conv2d(3, 6, 5) <font color=\"silver\"> # (in_channels, out_channels, kernel, stride=1)</font><br>\n","$\\qquad$$\\qquad$self.conv2 = torch.nn.Conv2d(6, 16, 5)<br>\n","$\\qquad$$\\qquad$self.pool = torch.nn.MaxPool2d(2, 2) <font color=\"silver\"> # (kernel,  stride)</font><br>\n","$\\qquad$$\\qquad$self.fc1 = torch.nn.Linear (16 * 5 * 5, 120) <font color=\"silver\"> # (in_features, out_features)</font><br>\n","$\\qquad$$\\qquad$self.fc2 = torch.nn.Linear(120, 84)<br>\n","$\\qquad$$\\qquad$self.fc3 = torch.nn.Linear(84, 10) <br>\n","$\\qquad$def forward(self, x):<br>\n","$\\qquad$$\\qquad$x = f.relu(self.conv1(x))<br>\n","$\\qquad$$\\qquad$x = self.pool(x)<br>\n","$\\qquad$$\\qquad$x = f.relu(self.conv2(x))<br>\n","$\\qquad$$\\qquad$x = self.pool(x)<br>\n","$\\qquad$$\\qquad$x = x.view(-1, 16 * 5 * 5) <br>\n","$\\qquad$$\\qquad$x = f.relu(self.fc1(x))<br>\n","$\\qquad$$\\qquad$x = f.relu(self.fc2(x))<br>\n","$\\qquad$$\\qquad$x = self.fc3(x) <br>\n","$\\qquad$$\\qquad$return x<br>\n","class LeNet(nn.Module):<br>\n","$\\qquad$def __ init __ (self):<br>\n","$\\qquad$$\\qquad$super(Net, self).__ init __ ()<br>\n","$\\qquad$$\\qquad$self.conv1 = nn.Conv2d(1, 6, 3)<br>\n","$\\qquad$$\\qquad$self.conv2 = nn.Conv2d(6, 16, 3)<br>\n","$\\qquad$$\\qquad$self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension<br>\n","$\\qquad$$\\qquad$self.fc2 = nn.Linear(120, 84)<br>\n","$\\qquad$$\\qquad$self.fc3 = nn.Linear(84, 10)<br>\n","$\\qquad$def forward(self, x):<br>\n","$\\qquad$$\\qquad$# Max pooling over a (2, 2) window<br>\n","$\\qquad$$\\qquad$x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))<br>\n","$\\qquad$$\\qquad$x = F.max_pool2d(F.relu(self.conv2(x)), 2)<br>\n","$\\qquad$$\\qquad$x = x.view(-1, self.num_flat_features(x))<br>\n","$\\qquad$$\\qquad$x = F.relu(self.fc1(x))<br>\n","$\\qquad$$\\qquad$x = F.relu(self.fc2(x))<br>\n","$\\qquad$$\\qquad$x = self.fc3(x)<br>\n","$\\qquad$$\\qquad$return x<br>\n","$\\qquad$def num_flat_features(self, x):<br>\n","$\\qquad$$\\qquad$size = x.size()[1:]  # all dimensions except the batch dimension<br>\n","$\\qquad$$\\qquad$num_features = 1<br>\n","$\\qquad$$\\qquad$for s in size:<br>\n","$\\qquad$$\\qquad$$\\qquad$num_features *= s<br>\n","$\\qquad$$\\qquad$return num_features<br>\n","class CNN_Drop(nn.Module):<br>\n","$\\qquad$def  __ init __ (self, p=0.5):<br>\n","$\\qquad$$\\qquad$super(). __ init __ ()<br>\n","$\\qquad$$\\qquad$self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)<br>\n","$\\qquad$$\\qquad$self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)<br>\n","$\\qquad$$\\qquad$self.l1 = nn.Linear(8 * 8 * 32, 1024)<br>\n","$\\qquad$$\\qquad$self.l2 = nn.Linear(1024, 1024)<br>\n","$\\qquad$$\\qquad$self.l3 = nn.Linear(1024, 10)<br>\n","$\\qquad$$\\qquad$self.act = nn.ReLU()<br>\n","$\\qquad$$\\qquad$self.pool = nn.MaxPool2d(2, 2)<br>\n","$\\qquad$$\\qquad$self.d_out1 = nn.Dropout(p=p)<br>\n","$\\qquad$$\\qquad$self.d_out2 = nn.Dropout(p=p)<br>\n","$\\qquad$def forward(self, x):<br>\n","$\\qquad$$\\qquad$h = self.pool(self.act(self.conv1(x)))<br>\n","$\\qquad$$\\qquad$h = self.pool(self.act(self.conv2(h)))<br>\n","$\\qquad$$\\qquad$h = h.view(h.size()[0], -1)<br>\n","$\\qquad$$\\qquad$h = self.d_out1(self.act(self.l1(h)))<br>\n","$\\qquad$$\\qquad$h = self.d_out2(self.act(self.l2(h)))<br>\n","$\\qquad$$\\qquad$h = self.l3(h)<br>\n","$\\qquad$$\\qquad$return h<br>\n","class CNN_Norm(nn.Module):<br>\n","$\\qquad$def  __ init __ (self, norm='batch'):<br>\n","$\\qquad$$\\qquad$super(). __ init __ ()<br>\n","$\\qquad$$\\qquad$assert norm in ['batch', 'layer', 'instance'], \"norm should be selected from 'batch', 'layer', 'instance'\"<br>\n","$\\qquad$$\\qquad$self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)<br>\n","$\\qquad$$\\qquad$if norm == 'batch':<br>\n","$\\qquad$$\\qquad$$\\qquad$self.norm1 = nn.BatchNorm2d(16)<br>\n","$\\qquad$$\\qquad$elif norm == 'layer':<br>\n","$\\qquad$$\\qquad$$\\qquad$self.norm1 = nn.LayerNorm((16, 32, 32))<br>\n","$\\qquad$$\\qquad$elif norm == 'instance':<br>\n","$\\qquad$$\\qquad$$\\qquad$self.norm1 = nn.InstanceNorm2d(16)$\\qquad$$\\qquad$<br>\n","$\\qquad$$\\qquad$self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)<br>\n","$\\qquad$$\\qquad$if norm == 'batch':<br>\n","$\\qquad$$\\qquad$$\\qquad$self.norm2 = nn.BatchNorm2d(32)<br>\n","$\\qquad$$\\qquad$elif norm == 'layer':<br>\n","$\\qquad$$\\qquad$$\\qquad$self.norm2 = nn.LayerNorm((32, 16, 16))<br>\n","$\\qquad$$\\qquad$elif norm == 'instance':<br>\n","$\\qquad$$\\qquad$$\\qquad$self.norm2 = nn.InstanceNorm2d(32)<br>\n","$\\qquad$$\\qquad$self.l1 = nn.Linear(8 * 8 * 32, 1024)<br>\n","$\\qquad$$\\qquad$self.l2 = nn.Linear(1024, 1024)<br>\n","$\\qquad$$\\qquad$self.l3 = nn.Linear(1024, 10)<br>\n","$\\qquad$$\\qquad$self.act = nn.ReLU()<br>\n","$\\qquad$$\\qquad$self.pool = nn.MaxPool2d(2, 2)<br>\n","$\\qquad$def forward(self, x):<br>\n","$\\qquad$$\\qquad$h = self.pool(self.act(self.norm1(self.conv1(x))))<br>\n","$\\qquad$$\\qquad$h = self.pool(self.act(self.norm2(self.conv2(h))))<br>\n","$\\qquad$$\\qquad$h = h.view(h.size()[0], -1)<br>\n","$\\qquad$$\\qquad$h = self.act(self.l1(h))<br>\n","$\\qquad$$\\qquad$h = self.act(self.l2(h))<br>\n","$\\qquad$$\\qquad$h = self.l3(h)<br>\n","$\\qquad$$\\qquad$return h<br>"],"metadata":{"id":"XB4qv_MBvV3r"}},{"cell_type":"markdown","source":["# <font color=\"silver\">実装│appendix, AttentionSeq2seq  [<font color=\"silver\">…</font>](https://colab.research.google.com/drive/1Y85gUZDHARFMXOvEbZvckmvHBP6k3dCo)</font><br>\n","class AttentionEncoder(Encoder):<br>\n","$\\qquad$<font color=\"silver\"># Seq2seqから継承<br></font>\n","$\\qquad$def __ init __ (self, vocab_size, wordvec_size, hidden_size):<br>\n","$\\qquad$$\\qquad$V, D, H = vocab_size, wordvec_size, hidden_size<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❶ 重みの初期値を定義</font><br>\n","$\\qquad$$\\qquad$rn = np.random.randn<font color=\"silver\"> # 正規分布からランダム</font><br>\n","$\\qquad$$\\qquad$embed_W = rn(V, D) / 100 <font color=\"silver\"> # 小さな値で初期化<br></font>\n","$\\qquad$$\\qquad$lstm_Wx = rn(D, 4 * H) * np.sqrt(2/(D+H)) <font color=\"silver\"> # Xavier, 4×H, biasはゼロで初期化<font color=\"blue\">$~\\leftarrow\\ast$</font></font><br>\n","$\\qquad$$\\qquad$lstm_Wh = rn(H, 4 * H) * np.sqrt(2/(H+H)) <br>\n","$\\qquad$$\\qquad$lstm_b = np.zeros(4 * H) <br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❷ 層を定義</font><br>\n","$\\qquad$$\\qquad$self.embed = TimeEmbedding(embed_W)<br>\n","$\\qquad$$\\qquad$self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❸ 重み・勾配・隠れ状態を定義</font><br>\n","$\\qquad$$\\qquad$self.params = self.embed.params + self.lstm.params<br>\n","$\\qquad$$\\qquad$self.grads = self.embed.grads + self.lstm.grads<br>\n","$\\qquad$$\\qquad$self.hs = None<br>\n","$\\qquad$def forward(self, xs):<br>\n","$\\qquad$$\\qquad$xs = self.embed.forward(xs)<br>\n","$\\qquad$$\\qquad$hs = self.lstm.forward(xs) <font color=\"silver\"> # 中間状態の全てをreturnする</font><font color=\"blue\">$~\\leftarrow\\ast$</font></font><br>\n","$\\qquad$$\\qquad$return hs<br>\n","$\\qquad$def backward(self, dhs):<br>\n","$\\qquad$$\\qquad$dout = self.lstm.backward(dhs) <font color=\"silver\"> #  Decoderから伝わってきた勾配を全て伝える</font><font color=\"blue\">$~\\leftarrow\\ast$</font></font><br>\n","$\\qquad$$\\qquad$dout = self.embed.backward(dout)<br>\n","$\\qquad$$\\qquad$return dout<br>\n","class AttentionDecoder:<br>\n","$\\qquad$def __ init __ (self, vocab_size, wordvec_size, hidden_size):<br>\n","$\\qquad$$\\qquad$V, D, H = vocab_size, wordvec_size, hidden_size<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❶ 重みの初期値を定義</font><br>\n","$\\qquad$$\\qquad$rn = np.random.randn<font color=\"silver\"> # 正規分布からランダム</font><br>\n","$\\qquad$$\\qquad$embed_W = rn(V, D) / 100 <font color=\"silver\"> # 小さな値で初期化<br></font>\n","$\\qquad$$\\qquad$lstm_Wx = rn(D, 4 * H) * np.sqrt(2/(D+H)) <font color=\"silver\"> # Xavier, 4×H, biasはゼロで初期化<font color=\"blue\">$~\\leftarrow\\ast$</font></font><br>\n","$\\qquad$$\\qquad$lstm_Wh = rn(H, 4 * H) * np.sqrt(2/(H+H))<br>\n","$\\qquad$$\\qquad$lstm_b = np.zeros(4 * H)<br>\n","$\\qquad$$\\qquad$affine_W_c = rn(2* H, V) * np.sqrt(2/(2*H+V))<font color=\"silver\"> # $c^t+h^t$を入力するため2×H<font color=\"blue\">$~\\leftarrow\\ast$</font></font><br>\n","$\\qquad$$\\qquad$affine_b_c = np.zeros(V)<br>\n","$\\qquad$$\\qquad$affine_W_s = rn(V, V) * np.sqrt(2/(V+V))<br>\n","$\\qquad$$\\qquad$affine_b_s = np.zeros(V)<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❷ 層を定義</font><br>\n","$\\qquad$$\\qquad$self.embed = TimeEmbedding(embed_W)<br>\n","$\\qquad$$\\qquad$self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)<br>\n","$\\qquad$$\\qquad$self.attention = TimeAttention()<br>\n","$\\qquad$$\\qquad$self.affine_c = TimeAffine(affine_W_c, affine_b_c)<br>\n","$\\qquad$$\\qquad$self.tanh = Tanh()<br>\n","$\\qquad$$\\qquad$self.affine_s = TimeAffine(affine_W_s, affine_b_s)<br>\n","$\\qquad$$\\qquad$layers = [self.embed, self.lstm, self.attention, self.affine_c, self.tanh, self.affine_s]<br>\n","$\\qquad$$\\qquad$<font color=\"silver\"># ❸ 重みと勾配をlistにまとめる</font><br>\n","$\\qquad$$\\qquad$self.params, self.grads = [], []<br>\n","$\\qquad$$\\qquad$for layer in layers:<br>\n","$\\qquad$$\\qquad$$\\qquad$self.params += layer.params<br>\n","$\\qquad$$\\qquad$$\\qquad$self.grads += layer.grads<br>\n","$\\qquad$def forward(self, xs, enc_hs):<br>\n","$\\qquad$$\\qquad$h = enc_hs[:,-1] <font color=\"silver\"> # enc_hsの最後だけを取り出す</font><font color=\"blue\">$~\\leftarrow\\ast$ enc_hs [ : , -1 ] </font><br>\n","$\\qquad$$\\qquad$self.lstm.set_state(h) <font color=\"silver\"> # 取り出した隠れ状態hをセットする</font><font color=\"blue\">$~\\leftarrow\\ast$</font><br>\n","$\\qquad$$\\qquad$out = self.embed.forward(xs)<br>\n","$\\qquad$$\\qquad$dec_hs = self.lstm.forward(out)<br>\n","$\\qquad$$\\qquad$c = self.attention.forward(enc_hs, dec_hs)<br></font>\n","$\\qquad$$\\qquad$out = np.concatenate((c, dec_hs), axis=2)<font color=\"blue\">$~\\leftarrow\\ast$ np.concatenate, axis=2</font><br>\n","$\\qquad$$\\qquad$out = self.affine_c.forward(out)<br>\n","$\\qquad$$\\qquad$out = self.tanh.forward(out)<br>\n","$\\qquad$$\\qquad$out = self.affine_s.forward(out)<br>\n","$\\qquad$$\\qquad$return out<br>\n","$\\qquad$def backward(self, dscore):<br>\n","$\\qquad$$\\qquad$dout = self.affine_s.backward(dscore)<br>\n","$\\qquad$$\\qquad$dout = self.tanh.backward(dout)<br>\n","$\\qquad$$\\qquad$dout = self.affine_c.backward(dout)<br>\n","$\\qquad$$\\qquad$N, T, H2 = dout.shape<br>\n","$\\qquad$$\\qquad$H = H2 // 2<br>\n","$\\qquad$$\\qquad$dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]<br>\n","$\\qquad$$\\qquad$denc_hs, ddec_hs1 = self.attention.backward(dc)<br>\n","$\\qquad$$\\qquad$ddec_hs = ddec_hs0 + ddec_hs1<br>\n","$\\qquad$$\\qquad$dout = self.lstm.backward(ddec_hs)<br>\n","$\\qquad$$\\qquad$dh = self.lstm.dh<br>\n","$\\qquad$$\\qquad$denc_hs[:, -1] += dh<br>\n","$\\qquad$$\\qquad$self.embed.backward(dout)<br>\n","$\\qquad$$\\qquad$return denc_hs<br>\n","$\\qquad$def generate(self, enc_hs, start_id, sample_size):<br>\n","$\\qquad$$\\qquad$sampled = []<br>\n","$\\qquad$$\\qquad$sample_id = start_id<br>\n","$\\qquad$$\\qquad$h = enc_hs[:, -1]<br>\n","$\\qquad$$\\qquad$self.lstm.set_state(h)<br>\n","$\\qquad$$\\qquad$for _ in range(sample_size):<br>\n","$\\qquad$$\\qquad$$\\qquad$x = np.array([sample_id]).reshape((1, 1))<br>\n","$\\qquad$$\\qquad$$\\qquad$out = self.embed.forward(x)<br>\n","$\\qquad$$\\qquad$$\\qquad$dec_hs = self.lstm.forward(out)<br>\n","$\\qquad$$\\qquad$$\\qquad$c = self.attention.forward(enc_hs, dec_hs)<br>\n","$\\qquad$$\\qquad$$\\qquad$out = np.concatenate((c, dec_hs), axis=2)<br>\n","$\\qquad$$\\qquad$$\\qquad$out = self.affine_c.forward(out)<br>\n","$\\qquad$$\\qquad$$\\qquad$out = self.tanh.forward(out)<br>\n","$\\qquad$$\\qquad$$\\qquad$out = self.affine_s.forward(out) <br>\n","$\\qquad$$\\qquad$$\\qquad$sample_id = np.argmax(out.flatten())<br>\n","$\\qquad$$\\qquad$$\\qquad$sampled.append(sample_id)<br>\n","$\\qquad$$\\qquad$return sampled<br>\n","class AttentionSeq2seq(Seq2seq):<br>\n","$\\qquad$def __ init __ (self, vocab_size, wordvec_size, hidden_size):<br>\n","$\\qquad$$\\qquad$args = vocab_size, wordvec_size, hidden_size<br>\n","$\\qquad$$\\qquad$self.encoder = AttentionEncoder( * args)<br>\n","$\\qquad$$\\qquad$self.decoder = AttentionDecoder( * args)<br>\n","$\\qquad$$\\qquad$self.softmax = TimeSoftmaxWithLoss()<br>\n","$\\qquad$$\\qquad$self.params = self.encoder.params + self.decoder.params<br>\n","$\\qquad$$\\qquad$self.grads = self.encoder.grads + self.decoder.grads<br>\n","$\\qquad$<font color=\"silver\"># Seq2seqから継承<br></font>\n","$\\qquad$def forward(self, xs, ts):<br>\n","$\\qquad$$\\qquad$decoder_xs = ts[:, :-1]  <font color=\"silver\"> # 最後の単語以外を入力とする</font><font color=\"blue\">$~\\leftarrow\\ast$ ts [: , :-1] </font><br>\n","$\\qquad$$\\qquad$decoder_ts = ts[:, 1:] <font color=\"silver\"> # 最初の単語以外を教師とする</font><font color=\"blue\">$~\\leftarrow\\ast$ ts[: , 1:] </font><br>\n","$\\qquad$$\\qquad$h = self.encoder.forward(xs)<br>\n","$\\qquad$$\\qquad$score = self.decoder.forward(decoder_xs, h)<br>\n","$\\qquad$$\\qquad$loss = self.softmax.forward(score, decoder_ts)<br>\n","$\\qquad$$\\qquad$return loss<br>\n","$\\qquad$<font color=\"silver\"># Seq2seqから継承<br></font>\n","$\\qquad$def backward(self, dout=1):<br>\n","$\\qquad$$\\qquad$dout = self.softmax.backward(dout)<br>\n","$\\qquad$$\\qquad$dh = self.decoder.backward(dout)<br>\n","$\\qquad$$\\qquad$dout = self.encoder.backward(dh)<br>\n","$\\qquad$$\\qquad$return dout<br>\n","$\\qquad$<font color=\"silver\"># Seq2seqから継承<br></font>\n","$\\qquad$def generate(self, xs, start_id, sample_size):<br>\n","$\\qquad$$\\qquad$h = self.encoder.forward(xs)<br>\n","$\\qquad$$\\qquad$sampled = self.decoder.generate(h, start_id, sample_size)<br>\n","$\\qquad$$\\qquad$return sampled<br>"],"metadata":{"id":"WzKIodmje0HY"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Conv [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!5782&cid=b0f01606242a6ed3&CT=1671033994240&OR=ItemsView)</font>"],"metadata":{"id":"_GB9n-4aLpUe"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Conv│Affine</font>\n","> - Description<br>\n"," - \n","$\\scriptsize\\begin{bmatrix}\n","9 & 2 \\\\\n","6 & 3\n","\\end{bmatrix}\n","\\rightarrow\n","\\begin{bmatrix}\n","9 \\\\\n","2 \\\\\n","6 \\\\\n","3\n","\\end{bmatrix}$<br><br>\n",">$\\pmb{Y} = \\pmb{X} \\pmb{W}+ \\pmb{B} \\qquad \\cfrac{\\partial L}{\\partial \\pmb{X}}= \\cfrac{\\partial L}{\\partial \\pmb{Y}}\\pmb{W}^{\\top} \\qquad \\cfrac{\\partial L}{\\partial \\pmb{W}}= \\pmb{X}^{\\top} \\cfrac{\\partial L}{\\partial \\pmb{Y}}  \\qquad \\cfrac{\\partial L}{\\partial \\pmb{B}} = \\cfrac{\\partial L}{\\partial \\pmb{Y}}$\n","<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/poisuke/items/33d2a454a6ebd1908aee)</font></font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F188565%2Ffe94ac75-4de5-43c7-a90a-314a7c15389b.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=eae50b12629e93ad3786d5ff3185c261\" width=\"480\"><br><br>\n","$\\qquad\\pmb{X}$<font color=\"silver\">（N, C, H, W）</font><br>\n","$\\qquad\\downarrow$<br>\n","（reshape）x.reshape(x.shape[0], -1)<br>\n","$\\qquad\\downarrow$<br>\n","$\\qquad\\pmb{X}$<font color=\"silver\">（N, C×H×W）</font><br>\n","$\\qquad\\quad\\quad\\searrow$<br>\n","$\\qquad\\quad\\quad\\quad$（dot）$\\longrightarrow \\pmb{X}\\pmb{Y}\\longrightarrow$（＋）$\\longrightarrow\\pmb{Y}$<font color=\"silver\">（N, WN）</font><br>\n","$\\qquad\\quad\\quad\\nearrow\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\nearrow$<br>\n","$\\qquad\\pmb{W}$<font color=\"silver\">（C×H×W, WN）$\\quad\\quad$</font>\n","$\\pmb{B}$<font color=\"silver\">（WN,）\n","<br><br><font color=\"black\">"],"metadata":{"id":"Zn2NAgiwzc7c"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Conv│Convolution </font>\n","> - Description<br>\n"," - <font color=\"Blue\">$\\tiny{\\rm Link}$ […</font>](https://qiita.com/icoxfog417/items/5fd55fad152231d706c2) […</font>](https://cvml-expertguide.net/terms/dl/cnn/)  […</font>](https://qiita.com/cu_klb_16/items/2e1959cfd9b5dd1322cc)<br>\n",">  - 疎な結合\n",">    - 畳み込みは、画像内の複数箇所に対して同一の動 作を実行する処理\n",">    - MLPは，出力ユニットすべてが入力ユニットすべてから影響を受ける。一方で、Convは入力より小さなカーネルを利用することにより、疎な結合を利用。\n","パラメータ数と計算処理を減らすことが可能\n",">    - 計算処理を減らすことが可能\n",">      - 入力がm個，出力がn個の全結合の場合の計算量はO(m×n)\n",">      - 出力がk個の結合を持つように制限すると計算量はO(k×n)\n",">  - パラメータ共有\n",">    - エッジのような複数箇所に現れる特徴を単一 のパラメータの組み合わせで効率よく捉えることに適している\n",">    - 画像内の複数箇所に対して同一の操作を実行するため、 同じ 重みを使い回すという点でメモリ効率 が良いという利点がある。\n",">    - パラメータ共有は、パラメータの自由度を制限しているという点で正則化の一種であるといえる。\n",">    - MLPは，出力を計算するときの重み行列の各要素は一度だけ使われる。一方で、 畳み込みのカーネルの要素は入力の全ての位置で利用される。\n",">    - モデルの保存に必要なメモリ容量を低減する\n",">      - 入力m個，出力n個の全結合に必要なメモリ容量はm×n\n",">      - カーネルサイズkの畳み込みに必要なメモリ容量はk\n",">  - 等価性\n",">    - ある関数が等価であるとは，入力が変化した場合に出力が同じように変化すること。CNNは、パラメータ共有によりこの性質を持つ<br>\n",">    - 入力をシフトしたものに畳み込みを適用した結果は，入力に畳み込みを適用したものをシフトした結果と一致する<br>\n","エッジ検出（エッジは画像中のほとんどの場所に現れる）など、近接する少数のピクセルの関数を，複数の入力位置に　適用することが役立つと知っているときに等価性は有用<br>\n",">    - 畳み込みは画像のスケーリングや回転などの変換に対しては等価ではない。入力画像中の対象物の位置がずれた場 合に、特徴マップ中の対象物に対応する部分も同様にずれるため、「元の対象物とずらしたあとの対象物が同一物体である」と モデルが認識できない、という問題がある<br><br>\n",">  - $\\displaystyle (I*K)(i,j) = \\sum_{m}\\sum_{n}I_{I+m,j+n}K_{m,n}$<br>\n","$K$：<font color=\"silver\">カーネル<br></font>\n","$I$：<font color=\"silver\">入力画像<br></font><br>\n","> - Description<br>\n",">  - 畳み込みフィルタの幅や高さが大きいほど特徴マップの１つのピクセルは元画像における広い範囲の特徴を含む<br>\n",">  - ストライドを大きくすると特徴マップのサイズが減少する<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://muscle-programmer.hatenablog.com/entry/2018/06/07/190221)</font></font><br>\n","<img src=\"https://qiita-image-store.s3.amazonaws.com/0/108729/5e4bb20f-127e-4d9e-10fb-110ba4694360.png\" width=\"480\"><br>\n","<img src=\"https://qiita-image-store.s3.amazonaws.com/0/108729/72ca6fe6-f6a0-7dd3-3b24-7aa3aa185ab6.png\" width=\"480\"><br>\n","> - 実装上のAlgorithm<br>\n",">  - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://colab.research.google.com/github/machine-perception-robotics-group/JDLALectureNotebooks/blob/master/notebooks/06_im2col.ipynb#scrollTo=975KXuwqkHsK)</font><br></font>\n","<img src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/143078/abaf537b-0763-b92a-e714-477497b1f8d4.png\" width=\"320\"><br><br>\n"," - <font color=\"black\">(input_size + 2*pad - filter_size) / stride + 1<br>\n","<font color=\"silver\">$\\scriptsize \\rm OH = \\cfrac{H + 2P - FH}{S} + 1 \\qquad OW = \\cfrac{W + 2P - FW}{S} + 1$<br>\n","<font color=\"silver\">np.pad(img, [(上, 下), (左, 右)], 'constant')<br><br></font>\n",">  - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/MA-fn/items/45a45a7417dfb37a5248)<br></font>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F386487%2Fda35406a-a9e4-e989-82f6-5d7e82f1a6d8.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=1902516f71dd337af68ef68c359af877\" width=\"800\"><br></font>\n"," - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/be48afbdd9da19f1e43e)<br></font>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2Fdbc63c54-5af4-58ce-4e60-afdd60fa66ae.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=64f069c6a2ee316857848529e8cad72c\" width=\"480\"><br><br>\n"," - def forward(self, x):<br>\n","$\\qquad$FN, C, FH, FW = self.W.shape<br>\n","$\\qquad$N, C, H, W = x.shape<br>\n","$\\qquad$out_h = (H + 2 * self.pad - FH) // self.stride + 1<br>\n","$\\qquad$out_w =(W + 2 * self.pad - FW) // self.stride + 1<br>\n","$\\qquad$col = im2col(x, FH, FW, self.stride, self.pad) <font color=\"silver\"> # (N, C, H, W) → (N×OH×OW, C×FH×FW)<br></font>\n","$\\qquad$col_W = self.W.reshape(FN, -1).T <font color=\"silver\"> # (FN, C, FH, FW) → (C×FH×FW, FN)<br></font>\n","$\\qquad$out = np.dot(col, col_W) + self.b <font color=\"silver\"> # (N×OH×OW, C×FH×FW) ･ (C×FH×FW, FN) → (N×OH×OW, FN)<br></font>\n","$\\qquad$out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2) <font color=\"silver\"> # (N×OH×OW, FN) → (N, FN, OH, OW) <br></font>\n","$\\qquad$self.x = x<br>\n","$\\qquad$self.col = col<br>\n","$\\qquad$self.col_W = col_W<br>\n","$\\qquad$return out<br>"],"metadata":{"id":"xRJqGuTDenDn"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Conv│Pooling\n","> - Description<br>\n"," - <font color=\"Blue\">$\\tiny{\\rm Link}$ […</font>](https://cvml-expertguide.net/terms/dl/layers/pooling-layer/)<br>\n"," - 畳み込み層で抽出された特徴の位置感度を若干低下させることで、対象とする特徴量の画像内での位置が若干変化した場合でも、出力が普遍になる<br>\n",">  - 画像の空間サイズの大きさを小さくすることで、調整するパラメーターの数を減らし、過学習を防止する<br>\n",">  - プーリング層は入力の小さな移動に対して不変な表現を作ることに役立つ。移動に対する不変性とは、入力を水平または垂直方向に多少変化させても出力にはほとんど変化がないという性質。例えば、手書き文字認識の場合、画像中の厳密な位置ではなく、画像中にどのような文字があるのかに関心がある場合に有用となる。<br><br>\n","$\\scriptsize \\begin{bmatrix}\n","1 & 3 & 2 & 1 \\\\\n","2 & 9 & 1 & 1 \\\\\n","1 & 3 & 2 & 3 \\\\\n","5 & 6 & 1 & 2 \\\\\n","\\end{bmatrix}\n","\\underrightarrow{\\text{max pooling}} \n","\\begin{bmatrix}\n","9 & 2 \\\\\n","6 & 3\n","\\end{bmatrix}$\n","<br><br>\n","$\\scriptsize \\begin{bmatrix}\n","1 & 3 & 2 & 1 \\\\\n","2 & 9 & 1 & 1 \\\\\n","1 & 3 & 2 & 3 \\\\\n","5 & 6 & 1 & 2 \\\\\n","\\end{bmatrix}\n","\\underrightarrow{\\text{average pooling}} \n","\\begin{bmatrix}\n","3.75 & 1.25 \\\\\n","3.75 & 2\n","\\end{bmatrix}$\n","<br><br>\n","<img src=\"https://blog.liang2.tw/2015Talk-DeepLearn-CNN/pics/external/cs231n_note_conv_pooling.png\" width=\"640\"><br>\n","> - 実装上のAlgorithm<br>\n"," - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/be48afbdd9da19f1e43e)<br></font>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F61f173ac-cd49-b11a-dff0-3dc09ca6e2fa.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=56e79a9ecd6420ab0b0221ebf34708d9\" width=\"480\"><br><br>\n","def forward(self, x): <br>\n","$\\qquad$N, C, H, W = x.shape<br>\n","$\\qquad$out_h = (H  + 2 * self.pad - self.pool_h) // self.stride + 1<br>\n","$\\qquad$out_w = (W + 2 * self.pad - self.pool_w) // self.stride + 1 <font color=\"silver\"> # (N, C, H, W) → (N×OH×OW, C×PH×PW)<br></font>\n","$\\qquad$col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad, constant_values=-np.inf)<br>\n","$\\qquad$col = col.reshape(-1, self.pool_h*self.pool_w) <font color=\"silver\"> # (N×OH×OW×C, PH×PW)<br></font>\n","$\\qquad$arg_max = np.argmax(col, axis=1)<br>\n","$\\qquad$out = np.max(col, axis=1) <font color=\"silver\"> # (N×OH×OW×C, )<br></font>\n","$\\qquad$out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)<br>\n","$\\qquad$self.x = x<br>\n","$\\qquad$self.arg_max = arg_max<br>\n","$\\qquad$return out"],"metadata":{"id":"OjCa9qGLxNnc"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Conv│Grouped Convolution\n","> - Description<br>\n"," - <font color=\"Blue\">$\\tiny{\\rm Link}$ […</font>](https://drive.google.com/drive/folders/16Y2-9uUm5qbdyX9TETtr9qeBkKNMYqKc?lfhs=2)<br>\n"," - 計算速度向上およびワイド化による表現力向上が狙える<br><font> <br>\n","<font color=\"black\">$N_p = c_i*k^2*c_o$ <font>$\\quad\\longrightarrow\\quad$$N_p = c_i//G*k^2*c_o$</font>\n","<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://www.mdpi.com/2072-4292/13/16/3211/htm)</font></font><br>\n","<img src=\"https://www.mdpi.com/remotesensing/remotesensing-13-03211/article_deploy/html/images/remotesensing-13-03211-g003-550.jpg\" width=\"640\"><br>\n","<img src=\"https://www.mdpi.com/remotesensing/remotesensing-13-03211/article_deploy/html/images/remotesensing-13-03211-g002-550.jpg\" width=\"640\">"],"metadata":{"id":"cAe_MQU1mi02"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Conv│Dilated Convolution\n","> - Description<br>\n"," - <font color=\"Blue\">$\\tiny{\\rm Link}$ […</font>](https://cvml-expertguide.net/terms/dl/layers/convolution-layer/dilated-convolution/)<br>\n"," - 少ないパラメータ数で、大域的な特徴を捉えるConvolution\n"," - CNNやFCNでは，計算速度のリアルタイム性が重視されるので，計算効率性も高いうえに認識性能も高い各種タスク向けのネットワークを設計する上で，膨張畳み込みが重宝される\n"," - 画像生成では，計算効率性をそれほど求めないのもあるが，それ以上に「高画質画像の生成のために，全ての画素の情報を網羅していきたい」ので，畳み込み層は膨張はさせず，普通の(密なカーネルの)畳み込み層のまま使用することが多い\n","<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://www.researchgate.net/figure/An-illustration-of-the-receptive-field-for-one-dilated-convolution-with-different_fig1_336002670)</font></font><br>\n","<img src=\"https://www.researchgate.net/publication/336002670/figure/fig1/AS:806667134455815@1569335840531/An-illustration-of-the-receptive-field-for-one-dilated-convolution-with-different.png\" width=\"480\"><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://cvml-expertguide.net/terms/dl/layers/convolution-layer/dilated-convolution/)</font></font><br>\n","<img src=\"https://i0.wp.com/cvml-expertguide.net/wp-content/uploads/2022/06/3edfe57ae96358f689c6639ad5e262a0.png?resize=768%2C342&ssl=1\" width=\"640\">\n","<br><br>\n","<img src=\"https://velog.velcdn.com/images%2Fvivilsh1226%2Fpost%2F43479bab-39ab-4158-ae96-00b3c19e8074%2Fimage.png\" width=\"640\">"],"metadata":{"id":"hWQAGVuOmjSH"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Conv│Transposed Convolution</font>\n","> - Description<br>\n"," - <font color=\"Blue\">$\\tiny{\\rm Link}$ […</font>](https://cvml-expertguide.net/terms/dl/layers/convolution-layer/transposed-convolution/)<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://www.researchgate.net/figure/Schematic-of-a-convolution-and-transposed-convolution-a-The-convolution-operation-for_fig4_335845675)</font></font><br>\n","<img src=\"https://www.researchgate.net/profile/Junxi-Feng/publication/335845675/figure/fig4/AS:804124836757506@1568729709926/Schematic-of-a-convolution-and-transposed-convolution-a-The-convolution-operation-for.ppm\" width=\"640\"><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://aismiley.co.jp/ai_news/cnn/)</font></font><br>\n","<img src=\"https://aismiley.co.jp/wp-content/uploads/2021/11/Example-of-a-discrete-convolution-a-and-equivalent-transposed-convolution-operation-b.png\" width=\"640\"><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://techplay.jp/event/846663)</font></font><br>\n","<img src=\"https://i.gyazo.com/6378b31f3cf24a2cd84f9b67935587fa.png\" width=\"640\">\n","<br><br>\n","\n","><font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://blog.negativemind.com/2019/03/11/semantic-segmentation-by-fully-convolutional-network/)</font></font><br>\n","\n",">|  |  |  ||\n","| :---: | :---: | :---: | :---: |\n","| $\\quad$kernel size=3, padding=0, stride=0$\\quad$ | $\\quad$kernel size=3, padding=0, stride=1$\\quad$ | $\\quad$kernel size=3, padding=1, stride=1$\\quad$ |\n","\n","><img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides_transposed.gif\" width=\"200\"><img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_strides_transposed.gif\" width=\"200\"><img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides_transposed.gif\" width=\"200\">"],"metadata":{"id":"bxzndnY_mjbp"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Conv│Global Average Pooling</font>\n","> - Description<br>\n"," - <font color=\"Blue\">$\\tiny{\\rm Link}$ […</font>](https://cvml-expertguide.net/terms/dl/layers/pooling-layer/global-average-pooling/)<br>\n"," - GAPとは、特徴マップの各チャネルごとに空間方向に対する平均値を算出し、その平均値を各特徴マップの値とする処理で全結合層におけるパラメータ数を大幅に削減することができる一方で、空間的な情報が失われてしまうというデメリットがある。<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://products.sint.co.jp/aisia/blog/vol1-19)</font></font><br>\n","<img src=\"https://products.sint.co.jp/hs-fs/hubfs/blog/images_compress/19_6.png?width=1281&name=19_6.png\" width=\"480\"><br>\n"],"metadata":{"id":"xUDzrT-znw6Z"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Conv│TwoLayerNetの構造</font><br>\n","> - 入力層+隠れ層+出力層<br>\n"," - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://knowledge.sakura.ad.jp/12871/)</font></font><br>\n","<img src=\"https://knowledge.sakura.ad.jp/images/2017/12/ml03-440x220.png\" width=\"480\"><br><br>\n","> - 入力層+隠れ層+隠れ層+出力層<br>\n"," - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://knowledge.sakura.ad.jp/12871/)</font></font><br>\n","<img src=\"https://knowledge.sakura.ad.jp/images/2017/12/ml04-440x230.png\" width=\"480\"><br><br>\n","> - 入力層+隠れ層+隠れ層+出力層<br>\n"," - MINIST(n, 28, 28)<br>\n","flatten(n, 784)<br>\n","x (n, 784)<br>\n","x (n, 784) × W1 (784, 50) + b1 (n, 50) = x (n, 50) <br>\n","x (n, 50) × W2 (50, 100) + b2 (n, 100) = x (n, 100) <br>\n","x (n, 100) × W3 (100, 10) + b3 (n, 10) = x (n, 10) <br>\n","Softmax(x (n, 10))<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://ebi-works.com/deeplearning-4/)</font></font><br>\n","<img src=\"https://ebi-works.com/wp-content/uploads/2020/02/deeplearning4-3.jpg\" width=\"480\"><br>"],"metadata":{"id":"Vs4fxiOxFIH7"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Conv│ConvNetの構造</font><br>\n","><font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://cvml-expertguide.net/terms/dl/cnn/)</font></font>\n","<br><br>\n","<img src=\"https://i0.wp.com/cvml-expertguide.net/wp-content/uploads/2021/09/CNN_BN.png?w=957&ssl=1\" width=\"640\"><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://exture-ri.com/2021/01/11/pytorch-cnn/)</font></font><br>\n","<img src=\"https://exture-ri.com/wp-content/uploads/2021/01/CNN_images.png\" width=\"640\"><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://aismiley.co.jp/ai_news/cnn/)</font></font><br>\n","<img src=\"https://aismiley.co.jp/wp-content/uploads/2021/11/CNN-model.jpeg\" width=\"640\"><br><br>\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"53B9INoZn1-r"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Conv│appendix, Conv Algorithm</font><br>\n","> - Algorithm<br>\n",">  - $\\displaystyle (X*I)_{mn} = \\sum_{k}\\sum_{l}I_{k,l}X_{m+k,n+l}$\n","<br><br>\n","$I$：<font color=\"silver\">カーネル<br></font>\n","$X$：<font color=\"silver\">入力画像<br></font><br>\n",">  - $\\displaystyle (I*K)(i,j) = \\sum_{m}\\sum_{n}I_{I+m,j+n}K_{m,n}$\n","<br><br>\n","$K$：<font color=\"silver\">カーネル<br></font>\n","$I$：<font color=\"silver\">入力画像<br></font><br>\n",">  - $\\displaystyle y_{i,j,m} = f\\left(\\sum_{k = 0}^{K - 1} \\sum_{p,q=0}^{H-1} (w_{p,q,k,m} x_{Si+p, Sj+q, k} + b_{i,j,m})\\right)$\n","<br><br>\n","$K$：<font color=\"silver\">出力チャンネルの枚数<br></font>\n","$H$：<font color=\"silver\">入力チャンネルのサイズ<br></font>\n","$S$：<font color=\"silver\">ストライド<br></font>\n","$p,q$：<font color=\"silver\">フィルタの左上を重ねる場所の行番号と列番号<br></font>\n","$x_{Si+p,Sj+q,k}$：<font color=\"silver\">$k$枚目の$Si+p$行$Sj+q列$目の入力チャンネルの値<br></font>\n","$w_{p,q,k,m}$：<font color=\"silver\">$p$行$q$列目の, $k$番目の入力チャンネルから$m$番目の出力チャンネルへの値<br></font>\n","$b_{i,j,m}$：<font color=\"silver\">$k$枚目の$i$行$j$列目の出力チャンネルのバイアス<br></font>\n","$y_{i,j,m}$：<font color=\"silver\">$m$枚目の$i$行$j$列目の出力チャンネルの値<br></font>\n","$f()$：<font color=\"silver\">活性化関数<br></font><br>\n",">  - $\\displaystyle G_{k, l, m}= \\sum_{i, j, m}K_{i, j, m, n}F_{k+i-1, l+j-1, m}$<br>\n","$G$：<font color=\"silver\">出直画像<br></font>\n","$F$：<font color=\"silver\">入力画像<br></font>\n","$K$：<font color=\"silver\">カーネル<br></font>\n","$m$：<font color=\"silver\">入力チャンネル数<br></font>\n","$n$：<font color=\"silver\">出力チャンネル数</font>"],"metadata":{"id":"bbulQM1FrNVC"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Conv│FullyConnectedNeuralNetwork</font><br>\n"," - <font color=\"Blue\">$\\tiny{\\rm Link}$ […</font>](https://photos.google.com/photo/AF1QipOB5aW4NPIpkuaX2DAbMk2o0C_Y75T8NzwoQDbO) […</font>](https://photos.google.com/photo/AF1QipNV8au7kr4hPA4o5z8yGP7QDdCZBMu1PW5PWsmX)<br>"],"metadata":{"id":"F11vT2U2yOPB"}},{"cell_type":"code","source":["import numpy as np\n","def relu(X):\n","    return np.maximum(0, X)\n","def softmax(X):\n","    X = X - np.max(X, axis=1, keepdims=True)\n","    return np.exp(X) / np.sum(np.exp(X), axis=1, keepdims=True)\n","def relu_backward(Z, delta):\n","    delta[Z <= 0] = 0\n","def cross_entropy_error(y, t):\n","    batch_size = y.shape[0]\n","    return -np.sum(t * np.log(y + 1e-7)) / batch_size\n","class FullyConnectedNeuralNetwork():\n","    def __init__(self, layer_units): # layer_units:各層のノード数を格納したリスト\n","        self.n_iter_ = 0\n","        self.t_ = 0\n","        self.layer_units = layer_units\n","        self.n_layers_ = len(layer_units)\n","        # 重みの初期値をlistで定義\n","        self.coefs_ = []\n","        self.intercepts_ = []\n","        for i in range(self.n_layers_ - 1):\n","            coef_init, intercept_init = self._init_coef(layer_units[i],layer_units[i+1])\n","            self.coefs_.append(coef_init)\n","            self.intercepts_.append(intercept_init)\n","        # 勾配の格納listの定義\n","        self.coef_grads_ = [np.empty((n_in_, n_out_)) for n_in_,n_out_ in zip(layer_units[:-1],layer_units[1:])]\n","        self.intercept_grads_ = [np.empty(n_out_) for n_out_ in layer_units[1:]]\n","    # 層間のパラメータを初期化するメソッド\n","    def _init_coef(self, n_in, n_out):\n","        std = np.sqrt(2/n_in)  # Heの初期値\n","        coef_init = np.random.randn(n_in, n_out) * std\n","        intercept_init = np.zeros(n_out)\n","        return coef_init, intercept_init\n","    # 推論値算出\n","    # activations:各層の出力を納めたリスト, activations[0]は入力データ, activations[i].shape=(バッチサイズ,ノード数)\n","    def _forward(self, activations): \n","        affine = [None] * (self.n_layers_ - 1)\n","        for i in range(self.n_layers_ - 1):\n","            affine[i] = np.dot(activations[i], self.coefs_[i]) + self.intercepts_[i]\n","            if (i + 1) == ( self.n_layers_ - 1 ): # 出力層の場合\n","                activations[i + 1] = softmax(affine[i])\n","            else: # 隠れ層の場合\n","                activations[i + 1] = relu(affine[i])\n","        return activations\n","    # 勾配値算出\n","    # j: アフィンの番号, activations:各層の出力を納めたリスト, deltas:出力層側から伝わってきた勾配を納めたリスト\n","    def _grad(self, j, activations, deltas):\n","        self.coef_grads_[j] = np.dot(activations[j].T,deltas[j])\n","        self.intercept_grads_[j] = np.sum(deltas[j], axis=0)\n","\n","    # 勾配値算出\n","    # activations:各層の出力を納めたリスト, activations[0]は入力データ, activations[i].shape=(バッチサイズ,ノード数)\n","    def _backward(self, t, activations):\n","        deltas = [None] * (self.n_layers_ - 1)\n","        last = self.n_layers_ - 2\n","\n","        # 交差エントロピー誤差とソフトマックス関数を合わせて勾配を算出\n","        n_samples = t.shape[0]\n","        deltas[last] = (activations[-1] - t)/n_samples\n","\n","        # 出力層の1つ手前のパラメータの勾配を算出\n","        self._grad(last, activations, deltas)\n","\n","        # 残りのパラメータの勾配を算出\n","        for i in range(self.n_layers_ - 2, 0, -1):\n","            # 入力(activations)の勾配を算出\n","            deltas[i - 1] = np.dot(deltas[i], self.coefs_[i].T)\n","\n","            # 活性化関数ReLUの勾配を算出\n","            relu_backward(activations[i], deltas[i - 1])\n","\n","            # パラメータの勾配を算出\n","            self._grad(i-1, activations, deltas)\n","\n","        return\n","\n","\n","    def _forward_and_back(self, x, t):\n","        \"\"\"\n","        順伝播処理を実行した後、逆伝播処理を実行するメソッド\n","        x : array-like, 入力データ, x.shape=(バッチサイズ, 入力層ノード数)\n","        t : array-like, 正解ラベル, t.shape=(バッチサイズ, 出力層ノード数)\n","        \"\"\"\n","        activations = [x] + [None] * (self.n_layers_ - 1)\n","\n","        # 順伝播\n","        activations = self._forward(activations)\n","        loss = cross_entropy_error(activations[-1], t)\n","\n","        # 逆伝播\n","        self._backward(t, activations)\n","\n","        return loss\n","    \n","    \n","if __name__==\"__main__\":    \n","    X = np.arange(21).reshape(3,7)\n","    print(\"X=\\n\",X)\n","    \n","    t = np.array([[0,1,0],[0,1,0],[1,0,0]])\n","    print(\"t=\\n\",t)\n","\n","    layer_units=[7,4,3]\n","\n","    mp = FullyConnectedNeuralNetwork(layer_units=layer_units)\n","    loss = mp._forward_and_back(X, t)\n","    print(\"loss=\",loss)"],"metadata":{"id":"IJwhlKGjZSDh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <font color=\"silver\">Recurrent [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!5919&cid=b0f01606242a6ed3&CT=1671553657456&OR=ItemsView)</font>"],"metadata":{"id":"IU8UBsr9XGac"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Recurrent│Embedding</font><br>\n","> - Description<br>\n"," - [<font color=\"Blue\">$\\tiny{\\rm Link}$…</font>](https://gotutiyan.hatenablog.com/entry/2020/09/02/200144)</font><br>\n",">  -  (バッチサイズ, Timeサイズ）→（バッチサイズ, ）×（語彙数, 分散表現次元数）→（バッチサイズ, Timeサイズ, 分散表現次元数）</font><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://agirobots.com/word2vec-and-embeddinglayer/)</font><br>\n","<img src=\"https://agirobots.com/wp/wp-content/uploads/2021/08/image-32-10-1024x366.jpg\" width=\"640\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/04a9adc2857f2a403cab)</font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F76cfed43-9b2b-bc02-b059-72b21c385747.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=9a1881b63d6af1673c509a4298e7a444\" width=\"240\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/35f6f0d26f9e58f01e4e)</font><br></font>\n","<img src=\"https://camo.qiitausercontent.com/32654fed87f53f2b82b773d50c36e94a97f9bec0/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f31363033633765302d653532392d346237632d653936312d3039396561653661663433632e706e67\" width=\"400\">"],"metadata":{"id":"HKw9MEjot4bs"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Recurrent│RNN</font><br>\n","> - Description<br>\n",">  - $\\mathbf{H}_t^{(l)} = \\tanh(\\mathbf{H}_t^{(l-1)} \\mathbf{W}_{xh}^{(l)} + \\mathbf{H}_{t-1}^{(l)} \\mathbf{W}_{hh}^{(l)}  + \\mathbf{b}_h^{(l)})$<br><br>\n","t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b<br>\n","h_next = np.tanh(t)<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/35f6f0d26f9e58f01e4e)</font><br>\n","<img src=\"https://camo.qiitausercontent.com/d0046c189cf724199009651c48433d0097df591a/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f34303634653037662d623962622d303432392d616339322d6338366233313837616164622e706e67\" width=\"480\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/35f6f0d26f9e58f01e4e)</font><br>\n","<img src=\"https://camo.qiitausercontent.com/3d6925879a4d36d8cd7562a9adf75d76f43f281e/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f65363761373333612d623134662d373232362d646262382d3061393135396663396434312e706e67\" width=\"480\">"],"metadata":{"id":"YU_4dDyRxeQJ"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Recurrent│LSTM</font><br>\n","> - Description<br>\n",">  - 入力ゲートおよび出力ゲートは時間依存性があるデータを受け取ったときのみ活性化を起こすための仕組み。これにより過去の値を必要な時にのみ取り出すことが出来るため、長期的な依存関係を効率的に学習することが出来る。<br>\n",">  - 忘却ゲートは、入力となる時系列データ内でパターンが劇的に変化して、過去のデータが必要なくなった時 CEC の値を書き換えるためのゲート。<br>\n",">  - LSTMは従来のRNNに比べてゲート制御用の重みが追加されているため重みやバイアスの種類が多く、似たような計算をなんども行う必要がある。計算を効率よく行うために中間層伝播、ゲート3種類を計算するために必要な、「入力」と「前の時刻の中間層出力」の重み付き線形和はまとめて計算する。<br>\n",">  - 例題9答(d)np.random.randn(n_input+n_hidden, n_hidden*4)<br><br>\n","<img src=\"https://d2l.ai/_images/lstm-3.svg\" width=\"480\"><br><br>\n","$\\mathbf{F}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xf} + \\mathbf{H}_{t-1} \\mathbf{W}_{hf} + \\mathbf{b}_f)$<br>\n","$\\mathbf{I}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xi} + \\mathbf{H}_{t-1} \\mathbf{W}_{hi} + \\mathbf{b}_i)$<br>\n","$\\mathbf{O}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xo} + \\mathbf{H}_{t-1} \\mathbf{W}_{ho} + \\mathbf{b}_o)$<br>\n","$\\tilde{\\mathbf{C}}_t = \\text{tanh}(\\mathbf{X}_t \\mathbf{W}_{xc} + \\mathbf{H}_{t-1} \\mathbf{W}_{hc} + \\mathbf{b}_c)$<br>\n","$\\mathbf{C}_t = \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t$<br>\n","$\\mathbf{H}_t = \\mathbf{O}_t \\odot \\tanh(\\mathbf{C}_t)$<br><br>\n","$\\rm{f}_t = \\sigma (\\rm{W}_{f}[\\rm{x}_{t},\\rm{h}_{t-1}]+ \\rm{b}_{f})$<br>\n","$\\rm{i}_t = \\sigma ( \\rm{W}_{i}[\\rm{x}_{t},\\rm{h}_{t-1}]+ \\rm{b}_{i})$<br>\n","$\\tilde{\\rm{c}}_t = \\tanh(\\rm{W}_{h}[\\rm{x}_{t},\\rm{h}_{t-1}] + \\rm{b}_c)$<br>\n","$\\rm{o}_t = \\sigma (\\rm{W}_{o}[\\rm{x}_{t},\\rm{h}_{t-1}]+\\rm{b}_{o})$<br>\n","$\\rm{c}_t = \\rm{f}_t \\circ \\rm{c}_{t-1} + \\rm{i}_t \\circ \\tilde{\\rm{c}}_t$<br>\n","$\\rm{h}_t = \\rm{o}_t \\circ \\tanh(\\rm{c}_t)$<br><br>\n","<img src=\"https://camo.qiitausercontent.com/eb9b28507d1d0e415e4cdb514639377a75e08756/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f36313235643137372d373333642d636265342d643565622d3336393762346161316361662e706e67\" width=\"400\">\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F60969%2Fad3d229e-3dda-e8ad-eeff-4a8b447e22d3.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=000c0833913731c595391f4c3c44a855\" width=\"320\"><br><br>\n","> - Description<br><br>\n","$\\mathbf{F}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xf} + \\mathbf{H}_{t-1} \\mathbf{W}_{hf} + \\mathbf{b}_f)$<br>\n","$\\mathbf{I}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xi} + \\mathbf{H}_{t-1} \\mathbf{W}_{hi} + \\mathbf{b}_i)$<br>\n","$\\mathbf{O}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xo} + \\mathbf{H}_{t-1} \\mathbf{W}_{ho} + \\mathbf{b}_o)$<br>\n","$\\tilde{\\mathbf{C}}_t = \\text{tanh}(\\mathbf{X}_t \\mathbf{W}_{xc} + \\mathbf{H}_{t-1} \\mathbf{W}_{hc} + \\mathbf{b}_c)$<br>\n","$\\mathbf{C}_t = \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t$<br>\n","$\\mathbf{H}_t = \\mathbf{O}_t \\odot \\tanh(\\mathbf{C}_t)$<br><br>\n","A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b<br>\n","f = A[:, :H]<br>\n","g = A[:, H:2H]<br>\n","i = A[:, 2H:3H]<br>\n","o = A[:, 3H:]<br>\n","f = sigmoid(f)<br>\n","g = np.tanh(g)<br>\n","i = sigmoid(i)<br>\n","o = sigmoid(o)<br>\n","c_next = f * c_prev + g * i<br>\n","tanh_c_next = np.tanh(c_next)<br>\n","h_next = o * tanh_c_next<br><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F1792a937-3a62-e163-e9a1-5f26665a7f42.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=f6b0e6bfc05b09c45dd5c3a2d500e443\" width=\"480\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm)</font>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/e690dfe80faa6512049f)</font><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/e690dfe80faa6512049f)</font><br>"],"metadata":{"id":"ud3HI1OaxuYE"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Recurrent│GRU</font><br>\n","> - Description<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://d2l.ai/chapter_recurrent-modern/gru.html)</font><br>\n","<img src=\"https://d2l.ai/_images/gru-3.svg\" width=\"480\"><br><br>\n","$\\mathbf{R}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xr} + \\mathbf{H}_{t-1} \\mathbf{W}_{hr} + \\mathbf{b}_r)$<br>\n","$\\tilde{\\mathbf{H}}_t = \\tanh(\\mathbf{X}_t \\mathbf{W}_{xh} + \\left(\\mathbf{R}_t \\odot \\mathbf{H}_{t-1}\\right) \\mathbf{W}_{hh} + \\mathbf{b}_h)$<br>\n","$\\mathbf{Z}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xz} + \\mathbf{H}_{t-1} \\mathbf{W}_{hz} + \\mathbf{b}_z)$<br>\n","$\\mathbf{H}_t = \\mathbf{Z}_t \\odot \\mathbf{H}_{t-1}  + (1 - \\mathbf{Z}_t) \\odot \\tilde{\\mathbf{H}}_t$<br><br>\n","$\\rm{r}_t = \\sigma (\\rm{W}_{r}[\\rm{x}_{t}, \\rm{h}_{t-1}]) $<br>\n","$\\tilde{\\rm{h}}_t = \\tanh (\\rm{W}_{h}[\\rm{x}_{t}, \\rm{r}_t \\circ \\rm{h}_{t-1}  ]) $<br>\n","$\\rm{z}_t = \\sigma (\\rm{W}_{z} [\\rm{x}_{t}, \\rm{h}_{t-1}])$<br>\n","$\\rm{h}_t = (1 -\\rm{z}_t) \\circ \\tilde{\\rm{h}}_{t-1} + \\rm{z}_t \\circ \\rm{h}_t$<br>"],"metadata":{"id":"V8n89vq3x9or"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Recurrent│Peep Hole</font><br>\n","> - Description<br>\n",">  - 覗き穴結合(peephole connections)とは、記憶セルの状態を各ゲートに伝えるための結合のこと。<br>\n",">  - これにより、ゲートの開き具合を決めるときに、記憶セルの情報を利用することができるようになる。<br><br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/k/konchangakita/20210130/20210130013304.png\" width=\"480\"><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F22648%2Fba2dc6b7-6d94-75c6-8e94-033a56708249.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=83e7b7658918128c25c826a989f76f55\" width=\"480\"><br>\n","<img src=\"https://camo.qiitausercontent.com/d4f3ecf4f5d255420dfa44c9f109453cb21c61e4/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f32323634382f33623330643832392d363961662d303331642d396536322d3236323365323463333737332e706e67\" width=\"480\"><br>"],"metadata":{"id":"dcp7F9dDyIli"}},{"cell_type":"markdown","source":["# <font color=\"silver\">Recurrent│Affine</font>\n","> - Description<br>\n",">  -  $\\pmb{Y} = \\pmb{X} \\pmb{W}+ \\pmb{B} \\qquad \\cfrac{\\partial L}{\\partial \\pmb{X}}= \\cfrac{\\partial L}{\\partial \\pmb{Y}}\\pmb{W}^{\\top} \\qquad \\cfrac{\\partial L}{\\partial \\pmb{W}}= \\pmb{X}^{\\top} \\cfrac{\\partial L}{\\partial \\pmb{Y}}  \\qquad \\cfrac{\\partial L}{\\partial \\pmb{B}} = \\cfrac{\\partial L}{\\partial \\pmb{Y}}$\n","<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/35f6f0d26f9e58f01e4e)</font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F793ab60b-3bc3-e0c3-f1f4-0d1d6e083705.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=1e06b4ab57bd1b11998116a1dafc108c\" width=\"480\"><br>\n","<img src=\"https://camo.qiitausercontent.com/f3d966fe9033fbbf936a1e0afbb046cb1d8a95d6/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3335323833362f61353233666639322d663132352d333234352d383165362d3935663161333630333839302e706e67\" width=\"480\"><br>"],"metadata":{"id":"vg4PYuw_y4Ma"}},{"cell_type":"markdown","source":["# <font color=\"silver\">画像 [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4932&cid=b0f01606242a6ed3&CT=1671705116047&OR=ItemsView)</font>\n","\n","\n"],"metadata":{"id":"-9Sktnv3fSTD"}},{"cell_type":"markdown","source":["# <font color=\"silver\">画像│LeNet</font><br>\n","> - Description<br>\n"," - LeNet-5は、6層構造（CNN層が5つ）のネットワーク。<br>\n"," - 32x32pxのパッチ画像を入力とする。<br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F22648%2F2b15f91d-3a3a-1ff2-9bbb-9e364c78dc1c.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=a828cf09f8a3a9c4f618f885128f189c\" width=\"640\">\n"],"metadata":{"id":"xZLqjLUSI8RO"}},{"cell_type":"markdown","source":["# <font color=\"silver\">画像│AlexNet</font><br>\n","> - Description<br>\n"," - ReLU<br>\n"," - Local Response Normalization（局所応答正規化）<br>\n"," - Overlapping Pooling（重複付き最大値プーリング）<br>\n"," - DropOut<br>\n"," - マルチGPU<br>\n"," - 2012年のILSVRCで1になったネットワーク<br>\n"," - 5層の畳み込み層と3層の全結合層で構成される<br>\n"," - パラメータ数は約6,000万個\n","<br><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F22648%2F1e13bd02-d58a-96be-2131-c79d466b57d1.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=6aba04e200bbf58eb2489b49e062d190\" width=\"640\">"],"metadata":{"id":"bQDQj7XOQggp"}},{"cell_type":"markdown","source":["# <font color=\"silver\">画像│ZFNet</font><br>\n","> - Description<br>\n"," - CNNの可視化を行ったうえで、AlexNetの次の課題を解決するアプローチをとった。<br>\n"," - 第一層のフィルタは極端に高い周波数と低い周波数の情報で成り立っており、<br>\n","その中間の周波数情報がほとんどない。<br>\n"," - 第二層の可視化からは、第一層の畳み込みで使用されている4という<br>\n","大きなストライドによって、エイリアシングが発生している。\n","<br><br>\n"," - そこで、この問題を解決するために、下記のアプローチをとった。<br>\n","１．第一層のフィルタサイズを11×11から7×7に変更<br>\n","２．ストライドを４から2に変えた。<br>\n","その結果、AlexNetを超えるパフォーマンスを示した。\n","<br><br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/t/tibet/20220117/20220117090425.png\" width=\"640\"><br>\n"],"metadata":{"id":"blpfOBXIRXFa"}},{"cell_type":"markdown","source":["# <font color=\"silver\">画像│VGG</font><br>\n","> - Description<br>\n"," - AlexNetよりも深いネットワーク構造（16/19）<br>\n"," - パラメータ数は約1億4,000万個<br>\n"," - 3×3の畳み込みのみを利用する<br>\n"," - Max Poolingを使ってCNNの深さを増加させていくと精度が改善した。<br>\n"," - 同一出力チャネル数の畳み込み層をいくつか重ねた後にMax Poolingで特徴マップを半分に縮小<br>\n"," - Max Poolingの後の畳み込み層の出力チャネル数を2倍に増加。<br>\n"," - AlexnetやZFNetで使用されていたLRNは、深いネットワークでは効果がないため、使用しない。\n","<br><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F22648%2Fb80c933f-75c0-f8fe-2c4d-c314d82d0c63.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=22acf9d8963dbf84e278e5fa56df799b\" width=\"480\">\n"],"metadata":{"id":"dGf0BxS0RX5W"}},{"cell_type":"markdown","source":["# <font color=\"silver\">画像│<font color=\"silver\">GoogleNet [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4855&cid=b0f01606242a6ed3&CT=1666714515497&OR=ItemsView)</font><br></font>\n","\n","> - Description<br>\n"," - Inception Module<br>\n","   - 異なるサイズの畳み込みを独立して行っているため、非零のパラメータ数が大きく減る<br>\n","   - 小さな畳込みフィルタを並列に並べて近似することで、 表現力とパラメータ数のトレードオフを改善<br>\n","   - 1×1の畳み込み層を挿入することで、次元削減を行い、さらにパラメータ数を削減している<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/jun40vn/items/5ac97a6f1d8f82a49194)</font></font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F73d58375-38ca-b8f3-49dd-a5dc2b7ea2cd.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=14759e901deefd4e04e04fca3d6832aa\" width=\"320\"><br><br>\n","<img src=\"https://axa.biopapyrus.jp/media/objectclassification_ref_googlenet_02.png\" width=\"640\"><br>\n"," - Auxiliary Classifier<br>\n","   - 学習時は、出力層以外に2つの補助の分類器の出力を重み付き平均をとり、損失を計算する。<br>\n","   - ネットワークの中間層に直接誤差を伝搬させることで、勾配消失防止と正則化を実現している。<br>\n","   - アンサンブル学習と同様の効果が得られるため、汎化性能の向上が期待できる。<br>\n","   - AuxililaryLossを導入しない場合でもBatchNormalizationを加えることにより、同様に学習がうまく進むことがある。<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/jun40vn/items/5ac97a6f1d8f82a49194)</font></font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F8d251aa2-54e4-a886-390b-b5aa87519e8c.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=cd3c041fb8b150ca4daa86514cdfbffb\" width=\"480\"><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F67f98213-a8ff-de52-2d7e-18a3ef37cdd2.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=b114488e13f5781fbb2ebc72856b3a3b\" width=\"480\">\n"," - Inception v3</font><br>\n","   - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://www.researchgate.net/figure/Basic-Inception-v3-structure_fig3_332428603)</font></font><br>\n","<img src=\"https://www.researchgate.net/publication/332428603/figure/fig3/AS:748134426755072@1555380555179/Basic-Inception-v3-structure.png\" width=\"480\"><br><br>\n","<img src=\"https://cloud.google.com/static/tpu/docs/images/inceptionv3onc--oview.png\" width=\"640\">\n"," - 3×3 Convolution<br>\n","   - 3×3 の畳み込み層を複数使用することで、受容野を同じにしつつ、計算量の削減を図る。<br>\n","<img src=\"https://miro.medium.com/max/1100/1*gyc_dcvBf51JLIu986LteQ.png\" width=\"240\">\n","<img src=\"https://pystyle.info/wp/wp-content/uploads/2021/11/pytorch-inceptionv3_01.jpg\" Height=\"160\">\n","<img src=\"https://miro.medium.com/max/1100/1*UvfZWRbPS8d1RGtBuKhIgw.png\" width=\"240\">\n"," - 1×n Convolution, n×1 Convolution<br>\n","   - nx1 の畳み込み層と 1xn の畳み込み層を使用することで、受容野を同じにしつつ、計算量の削減を図る<br>\n","<img src=\"https://miro.medium.com/max/1100/1*UvfZWRbPS8d1RGtBuKhIgw.png\" width=\"240\">\n","<img src=\"https://pystyle.info/wp/wp-content/uploads/2021/11/pytorch-inceptionv3_03.jpg\" Height=\"160\">\n","<img src=\"https://miro.medium.com/max/1100/1*QPKnhTEjA4GELiGnLnvi8g.png\" width=\"240\">"],"metadata":{"id":"tbkHQjIISYmL"}},{"cell_type":"markdown","source":["# <font color=\"silver\">画像│ResNet [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4853&cid=b0f01606242a6ed3&CT=1666715412254&OR=ItemsView)</font><br>\n","> - 劣化問題, degradation problem<br>\n"," - <font color=\"Blue\">$\\tiny{\\rm Link}$ […</font>](https://qiita.com/ikeyasu/items/ea9ced2b8e0fcb3da2be) […</font>](●) <br>\n"," - 深いネットワークでは、エラー率高い\n","   - Figure 1<br>\n","   - 左はトレーニングエラー。右は、テストエラー。<br>\n","   <img src=\"https://sike6054.github.io/blog/images/ResNet,%20Fig.1(removed).png\" width=\"480\"><br><br>\n"," - Identity Mapping by Shortcuts\n","   - 勾配消失を抑え、「層を深くすると学習できない」という劣化問題を解決した。<br>\n","   - バイパスすることによって入力層に近い層にも誤差が伝わるため、勾配消失を防ぐ。<br>\n","   - 色々なバイパスの組み合わせが存在することになりアンサンブル効果がある。<br><br>\n","   - 入力をショートカットして足すだけなので、計算コストはほとんど増えずに、実装も容易。<br>\n","   - 𝐹(𝑥) は、入力𝑥と出力 H(𝑥) の残差 H𝑥−𝑥 を学習している<br>\n","   - 入力$x$に変換が不要であれば𝐹(𝑥)はゼロ写像となるように学習される<br><br>\n","   - ブロックへの入力にこれ以上の変換が必要ない場合は重みが0となり、小さな変換が求められる場合は対応する小さな変動をより見つけやすくなる。<br>\n","   - Residual Block内のパラメータが０になった場合、Residual Blockは恒等写像と等価である <br><br>\n","<font color=\"black\">$H(x)=F(x)+x$<br><br>\n","$\\cfrac{\\partial H}{\\partial x} = \\cfrac{\\partial F}{\\partial x}+1\\quad\\cfrac{\\partial F}{\\partial x}$がゼロの時に勾配$\\cfrac{\\partial H}{\\partial x}$がゼロにならない<br><br>\n","　通常の畳み込み層は入力$x$に変換が不要であれば、恒等 写像になるように学習する必要がありますが、さまざまな入力に対して常に 入力と同じ値を返す恒等写像を、学習によって獲得するのは簡単ではありま せん。しかし、ゼロ写像であれば、 恒等写像に比べて学習するのは簡単です。 𝐹(𝑥)中の畳み込み層にて、値の大きさに関係なくすべての値を負にするよう に学習できれば、活性化関数にReLUを使うことでゼロ写像を得ることができ ます。 このように差分を学習するように設定することで、 変換の必要不要を より学習させやすくなりました。 <br><br>\n","   - Figure 2<br>\n","<img src=\"https://sike6054.github.io/blog/images/ResNet,%20Fig.2(removed).png\" width=\"320\"><br><br>\n"," - Network Architectures <br>\n","    - Figure 3<br>\n","><img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20200424011138/ResNet.PNG\" width=\"640\"><br><br>\n",">   - Bottleneck Block\n","      - 1×1 Convで次元削減、3×3 Conv、1×1 Convで次元を復元するという形<br>\n","      - 同等の計算量を保ちながら、より深いモデルを構築することが出来る。<br>\n","      - 左図のパラメーター数：3×3×64×64、3×3×64×64≒70k<br>\n","      - 右図のパラメーター数：256×64、64×3×3×64、64×256≒70k（ボトルネック）<br>\n","      - Figure 5<br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/s/shikiponn/20190603/20190603173844.png\" width=\"480\"><br>\n","      - Table 1<br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F38555%2F1557d5bd-50bc-a65e-cf9a-dad10649a6c0.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=21774d16a1f02b2759fa9fb941212d13\" width=\"800\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/be48afbdd9da19f1e43e)<br></font>\n","<img src=\"https://camo.qiitausercontent.com/310591eea55adba318520a682e19baac8ab64d19/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3331353036332f36303336393736332d643734382d386334382d396262322d3734616564623262663263392e706e67\" width=\"480\">\n"," - 実験, 4.1. ImageNet Classification\n","   - Figure 4において、ResNetとPlainネットワークで、Parameter数は同じ。にもかかわらず、ResNetはError率が下がっている<br>\n","   - Figure 4において、Plain-34はPlain-18よりも悪い。これは、劣化問題 が起こっている事を示している。一方で、ResNet-34は、ResNet-18よりも良く、劣化問題を回避できていることが分かる。<br><br>\n","   - Figure 4<br>\n","   - 左： Plaing network, 右： ResNet\n","   - 細い線は、Training Error。太い線は、Validation error\n","   <img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F38555%2F220dc8ef-05c2-3cd0-9a3d-98573e02ed45.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=32b0b54e2f34392a73d9d072ef62e329\" width=\"640\"><br><br>\n","   - Figure 4において、ResNetとPlainネットワークで、Parameter数は同じ。にもかかわらず、ResNetはError率が下がっている<br>\n","   - Table 2において、top-1エラーが、ResNetとPlainネットワークで比べると、3.5%良化している。よって、Residual learningが深いネットワークで有効である。\n","   - Table 2において、18レイヤでは、ResNetとPlainネットワークでエラーは変わらない。が、Figure 4において、収束が速いことが分かる。<br><br>\n","   - Table 2<br>\n","   <img src=\"https://sike6054.github.io/blog/images/ResNet,%20Table.2(removed).png\" width=\"320\"><br><br>\n","   - Table 3, Image Net で検証した結果。10-crop testing をしている。<br>\n","   <img src=\"https://sike6054.github.io/blog/images/ResNet,%20Table.3(removed).png\" width=\"320\"><br><br>\n","   - Table 4, ImageNet validation setの single-model での、Error rates。<br>\n","   <img src=\"https://sike6054.github.io/blog/images/ResNet,%20Table.4(removed).png\" width=\"320\"><br><br>\n","   - Table 5, EnsemblesをしたときのError rates。<br>\n","   <img src=\"https://sike6054.github.io/blog/images/ResNet,%20Table.5(removed).png\" width=\"320\"><br><br>\n"," - 実験, 4.2 CIFAR-10 and Analysis\n","   - Table 6, CIFAR-10 を使って、さらに深いネットワークでの挙動を検証する。<br>\n","   <img src=\"https://sike6054.github.io/blog/images/ResNet,%20Table.6(removed).png\" width=\"320\"><br><br>\n","   - ResNet-110 は、一番良い結果になっている。収束も早い、かつ、他の深いレイヤーの手法よりも、パラメータ数が少ない\n","   - ResNet-1202の結果もそれほど悪くないが、ResNet-110より結果が悪く、これほど深いモデルには、以前問題がある\n","   - Figure 6\n","   - 破線は、training error、太線はtesting error\n","   - 左： Plain networks。中央： ResNet。右： ResNet-110とResNet-1202。\n","   <img src=\"https://sike6054.github.io/blog/images/ResNet,%20Fig.6(removed).png\" width=\"800\"><br><br>\n","   - 他手法に比べて、偏差が0に近いことを示している。（これは、Residual functionの元々のモチベーションである）\n","   - また、ネットワークが深いほど、偏差が少ない\n","   - Figure 7, CIFAR-10を学習させたときの、レイヤー毎の標準偏差。<br>\n","   - 上：レイヤーは、そのままの順番\n","   - 下：レイヤーを、偏差の高い順に並べている\n","   <img src=\"https://sike6054.github.io/blog/images/ResNet,%20Fig.7(removed).png\" width=\"480\"><br><br>\n"," - 実験, 4.3 Object Detection on PASCAL and MS COCO\n","   - Table 7, PASCAL VOC2007/2012 test set の Object detction mAP(%)の結果。<br>\n","   <img src=\"https://camo.qiitausercontent.com/43fbdce6dc6d9dfc6d68ca517dc84137d96296e8/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f33383535352f36323138613330632d353336302d326235632d373835332d3864353363633732323863392e706e67\" width=\"320\"><br><br>\n","   - Table 8, COCO validation set の Object detection mAP(%)の結果。<br>\n","   <img src=\"https://camo.qiitausercontent.com/1a73b3a1ffc51ad8118484672ce84d2be5ee50bd/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f33383535352f36323063626466352d383339372d383238392d353166312d3061636463346431633936612e706e67\" width=\"320\"><br><br>\n","\n"],"metadata":{"id":"znayWp8lNmWQ"}},{"cell_type":"markdown","source":["# <font color=\"silver\">画像│WideResNet  [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4863&cid=b0f01606242a6ed3&CT=1666720554388&OR=ItemsView)</font><br>\n","\n","https://zenn.dev/robes/scraps/75458003205dde\n","\n","https://pystyle.info/pytorch-wide-resnet/\n","\n","> - Description<br>\n"," - チャンネル数（増）、層（浅）\n"," - ResNetが残差接続によって層を深くしたのに対し、 WideResNet は残差ブロック内の畳み込み層の出力チャンネル数を増やし、層を浅くしたモデル。<br></font>\n","   - depth：<font color=\"silver\">全体のdepth（畳み込みの深さ、畳み込みの数）<br></font>\n","   - <font color=\"Black\">l：<font color=\"silver\">ブロック内のdepth（畳み込みの深さ、畳み込みの数）<br></font>\n","   - <font color=\"Black\">k：<font color=\"silver\">ブロック内のwidth（畳み込みの広さ）特徴平面の数（チャネル数）を k 倍することを意味する。<br></font>\n","   - <font color=\"Black\">B(M)：<font color=\"silver\"> Residial Block（残差ブロック）<br></font>\n","   - <font color=\"Black\">M：<font color=\"silver\">ブロック内の畳み込み層のカーネルサイズのリスト<br></font>\n","   - <font color=\"Black\">WRN-n-k：<font color=\"silver\"> n層の畳み込みをもち、幅k をもつ Wide Residual Networks<br>\n","   </font><img src=\"https://norman3.github.io/papers/images/wrn/f01.png\" width=\"640\"><br>\n"," - Baseline Model\n","   - ベースとなるWide Residua Networks<br>\n","   <img src=\"https://norman3.github.io/papers/images/wrn/f02.png\" width=\"480\"><br>\n","   （参考）<br>\n","   <img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F38555%2F1557d5bd-50bc-a65e-cf9a-dad10649a6c0.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=21774d16a1f02b2759fa9fb941212d13\" width=\"640\"><br>\n"," - Block Type<br>\n","   - ブロックの組み合わせ結果\n","   - Residual Block内の畳み込み層の組み合わせを検証<br>\n","   - チャンネル数K＝2倍のときの性能比較では、B(3, 3)のエラー率が最も低い5.73であり、性能が最も良い。<br><br>\n","<img src=\"https://norman3.github.io/papers/images/wrn/f03.png\" width=\"320\"><br>\n","B(1, 3, 1): 1×1 -> 3×3 -> 1×1。オリジナルの Bottleneck Block<br>\n","B(3, 1): 3×3 -> 1×1<br>\n","B(1, 3): 1×1 -> 3×3<br>\n","B(3, 1, 1): 3×3 -> 1×1 -> 1×1。Network In Network と同じ構造。<br>\n","B(3, 3): 3×3 -> 3×3。オリジナルの Building Block<br>\n","B(3, 1, 3): 3×3 -> 1×1 -> 3×3<br>\n"," - Layers per Block\n","   - 層の深さの組み合わせの結果\n","   - ネットワーク全体の層数を40として、1 ブロック内の畳み込み層の数$\\,l\\,$を変化させて検証<br>\n","   - 層の数$\\,l=1\\,$では表現力が足りず、$\\,l=3\\,$以上のときは残差接続が減少したため最適化が難しくなった結果といえる。<br><br>\n","<img src=\"https://norman3.github.io/papers/images/wrn/f04.png\" width=\"240\"><br>\n","l=1はB(3)<br>\n","l=2はB(3, 3)<br>\n","l=3はB(3, 3, 3)<br>\n","l=4はB(3, 3, 3, 3)<br>\n"," - Experimental results<br>\n","   - depth（全体の層の深さ）と k（幅の広さ）の組み合わせの結果<br><br>\n","<img src=\"https://norman3.github.io/papers/images/wrn/f05.png\" width=\"480\"><br>\n","   - 他のモデルとの性能比較の結果<br><br>\n","<img src=\"https://norman3.github.io/papers/images/wrn/f06.png\" width=\"480\"><br>\n","   - ResNetとWideResNetとの比較<br><br>\n","<img src=\"https://norman3.github.io/papers/images/wrn/f07.png\" width=\"480\"><br>\n","   - ドロップアウト有と無の性能比較<br><br>\n","<img src=\"https://norman3.github.io/papers/images/wrn/f08.png\" width=\"480\"><br>\n","   - ResNet と WideResNet の比較、WideResNet のドロップアウト有と無の比較<br>\n","   - Dropout なしの場合は、学習率を下げると過剰適合により急激に訓練損失が下がる。<br><br>\n","<img src=\"https://norman3.github.io/papers/images/wrn/f09.png\" width=\"480\"><br>\n","   - WideResNet の幅や深さを変更したときの性能比較<br><br>\n","<img src=\"https://norman3.github.io/papers/images/wrn/f10.png\" width=\"480\"><br>\n","   - ResNetとWideResNetとの比較<br><br>\n","<img src=\"https://norman3.github.io/papers/images/wrn/f11.png\" width=\"480\"><br>\n","   - 各データセットに対する WideResNet の結果<br><br>\n","<img src=\"https://norman3.github.io/papers/images/wrn/f12.png\" width=\"480\"><br>\n","   - オリジナルのResNetとの精度と計算時間の比較<br>\n","<img src=\"https://norman3.github.io/papers/images/wrn/f13.png\" width=\"200\"><br>\n","\n"],"metadata":{"id":"psTYjOEZfeoW"}},{"cell_type":"markdown","source":["# <font color=\"silver\">画像│DenseNet [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4861&cid=b0f01606242a6ed3&CT=1666715758082&OR=ItemsView)</font><br>\n","> - Description<br>\n"," - <font color=\"Blue\">$\\tiny{\\rm Link}$ […</font>](https://deepsquare.jp/2020/04/resnet-densenet/#outline__4) […</font>](https://qiita.com/takeuchiseijin/items/1da157de57601fa69d5e) <br>\n","  - Denseなスキップ接続群により，ResNetよりも勾配消失問題を緩和した。<br>\n","  - Denseブロック内で層間が全てスキップ接続されているため、特徴マップの伝搬を強化している。<br>\n","  - 特徴量を再利用する結合になっているため、パラメータ数を少なくすることができる。<br>\n","  - 実際には、次元削減のためBottleneckを使用。<br>\n","  - DenseNetとResNet <br>\n","   - ResNet：$x_{l+1} = x_{l} + f_l(x_l)$<br>\n","  DenseNet：$x_{l+1} = f_l(x_0,\\cdots,x_l)$<br><br>\n","   - CNN：${x}^{(l)} = H^{(l)}({x}^{l-1})$<br><br>\n","  ResNet：${x}^{(l)} = H^{(l)}({x}^{l-1},{x}) + {x}^{l-1}$<br><br>\n","  DenseNet：${x}^{(l+1)} = H^{(l)}([{x}^{l-1},{x}^{l-2},…,{x}^{0}]) + {x}^{l}$<br><br>\n","  $xl$：<font color=\"silver\">第l層の出力</font><br>\n","$Hl$：<font color=\"silver\">Batch normalization、ReLU、3×3 Convolution</font>\n","<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://techplay.jp/event/846663)</font><br>\n","<img src=\"https://i.gyazo.com/8cae5dd02904890ca66f11470603cfd3.png\" width=\"640\"><br><br>\n","  - Grow Rate\n","   - DenseBlock 内部では Growth Rate と呼ばれるハイパーパラメータによって､各畳み込み層の出力チャンネル数を設定する。\n","   - Growth Rate が大きくなるほど､DenseBlock の出力特徴マップのチャンネル数は大きくなる。\n","   - k=32に設定されることが多い <br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://ai-kenkyujo.com/)</font><br>\n","<img src=\"https://ai-kenkyujo.com/wp-content/uploads/2021/12/ai-img2-20.png.webp\" width=\"640\"><br>\n","  - Transition Layer <br>\n","   - Dense Blockで大きくなったチャンネル数を圧縮するダウンサンプリングの役割を持つレイヤー<br>\n","   - 1×1 Convolution、2×2 Average Poolingで構成されている<br>\n","   - DenseBlock の個数や  Growth Rate の設定次第では、特徴マップの個数が非常に多くなり､学習や推論に必要とする計算資源も増大してしまうことから､DenseBlock 間に配置する畳み込み層とプーリング層によって構成されるモジュールである Transition Layer によって幅・高さ・チャンネル数を適宜調整を行う。\n","   - ダウンサンプリングを行う層は重要な要素になるため、Dense Block間にPooling層を導入した。<br>\n","<img src=\"https://ai-kenkyujo.com/wp-content/uploads/2021/12/ai-img2-21.png.webp\" width=\"640\">\n","<img src=\"https://images.velog.io/images/skhim520/post/9e72346d-ef5e-4a42-95f3-1c0e2feb235d/image.png\" width=\"640\">"],"metadata":{"id":"CmdR6XV2U-bk"}},{"cell_type":"markdown","source":["# <font color=\"silver\">画像│MobileNet [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4869&cid=b0f01606242a6ed3&CT=1666816954089&OR=ItemsView)</font>\n","> - Description<br>\n"," - バイアス分は❓<br>\n"," - Depthwise Separable Convolution<br><br>\n","   - ${\\cfrac{D_{K} · D_{K} · M · D_{F} · D_{F} ＋M · N · D_{F} · D_{F} }{D_{K} · D_{K} · M · N · D_{F} · D_{F} } = \\cfrac{1}{N} + \\cfrac{1}{D_{K}^2}}$\n","<br><br>\n","<font color=\"black\">$M$：<font color=\"silver\">入力チャネル数</font><br>\n","<font color=\"black\">$N$：<font color=\"silver\">出力チャネル数</font><br>\n","<font color=\"black\">$D_{K}×D_{K}$：<font color=\"silver\">カーネルサイズ</font><br>\n","<font color=\"black\">$D_{F}×D_{F}$：<font color=\"silver\">特徴マップサイズ</font>\n","<br><br>\n","   - ${\\rm Depthwise}+{\\rm Pointwise} = c_{in}･k^2 + c_{in}･c_{out} = c_{in}(c_{out}+k^2)$\n","<br><br>\n","$\\cfrac{c_{in}(c_{out}+k^2)}{c_{in}･c_{out}･k^2} ＝\\cfrac{c_{out}＋k^2}{c_{out}･k^2} ＝\\cfrac{1}{k^2} ＋\\cfrac{1}{c_{out}}$<br><br>\n","   - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://muscle-programmer.hatenablog.com/entry/2018/06/07/190221)</font></font><br>\n","<img src=\"https://qiita-image-store.s3.amazonaws.com/0/108729/5e4bb20f-127e-4d9e-10fb-110ba4694360.png\" width=\"640\"><img src=\"https://qiita-image-store.s3.amazonaws.com/0/108729/72ca6fe6-f6a0-7dd3-3b24-7aa3aa185ab6.png\" width=\"640\"><br><br>\n"," - Width Multiplier $α$: Thinner Models\n","   - ネットワーク内のチャネル数を調整するハイパーパラメータ<br>\n","   - 各レイヤでネットワークを一様に薄くする役割を持つ<br>\n","   - 計算コストとパラメータ数を約$α$の二乗の二次関数的に削減する<br><br>\n","<font color=\"black\">$D_{K} · D_{K} · αM · \\rho D_{F} · \\rho D_{F} ＋αM · αN · \\rho D_{F} · \\rho D_{F} $<br><br>\n"," - Resolution Multiplier $\\rho$: Reduced Representation\n","   - 画像やネットワーク内での中間表現の解像度を調整するハイパーパラメータ<br>\n","   - 入力画像に適用すると、各層の内部表現が同じ乗数で削減される。<br>\n","   - 各層のチャネル数 / 解像度を小さくし、精度は下がってしまうものの、計算コストを削減<br><br>\n","<font color=\"black\">$D_{K} · D_{K} · αM · \\rho D_{F} · \\rho D_{F} ＋αM · αN · \\rho D_{F} · \\rho D_{F} $<br><br>\n","<img src=\"https://greeksharifa.github.io/public/img/2022-02-01-MobileNetV1/tab06.png\" width=\"480\"><br>"],"metadata":{"id":"Sd-WZFcuxean"}},{"cell_type":"markdown","source":["# <font color=\"silver\">画像│EfficientNet [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4875&cid=b0f01606242a6ed3&CT=1666816283914&OR=ItemsView)</font>\n","> - Description<br>\n","<font color=\"Blue\">$\\tiny{\\rm Link}$ […</font>](https://qiita.com/omiita/items/83643f78baabfa210ab1) […</font>](https://nuka137.hatenablog.com/entry/2020/09/09/075422) <br>\n"," - Compound Model Scaling\n","   - Grid Searchによりスケーリング係数を決め、モデルをスケールアップするための関係式を提供する<br>\n","     - b, ネットワークの幅（広さ=1レイヤーのサイズ=カーネル数=出力のchannel数）(上図b)\n","     - c, ネットワークの層（レイヤーの数）\n","     - d, 入力画像の解像度（入力画像の大きさ）\n","     <img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/nuka137/20200904/20200904095539.png\" width=\"800\"><br>\n","     - モデルスケーリングは、広さ・深さ・解像度を変えるだけで、レイヤーのアーキテクチャを変えない。\n","     - 全てのレイヤーの3つの値をある定数倍で同じようにスケーリングする。最適化問題として定式化する<br><br>\n","<font color=\"black\">$\\displaystyle \\max_{d, w, r} \\quad Accuracy\\left(\\mathcal{N}(d, w, r)\\right)$<br>\n","$\\displaystyle \\mathcal{N}(d, w, r) = \\bigodot_{i=1,...s}\\hat{\\mathcal{F}}_i^{d \\cdot \\hat{L}_i}\\left(X_{<r \\cdot \\hat{H}_i, r \\cdot \\hat{W}_i, w \\cdot \\hat{C}_i>}\\right)$<br>\n","$\\displaystyle \\rm Memory(\\mathcal{N}) \\le target\\_memory$<br>\n","$\\displaystyle \\rm FLOPS(\\mathcal{N}) \\le target\\_flops$<br><br>\n","<font color=\"black\">$\\mathcal{N}$：<font color=\"silver\">Baseline Modelのネットワーク<br>\n","<font color=\"black\">$α, β ,γ$：<font color=\"silver\">Baseline Modelにおける層数、幅、解像度のスケーリング係数<br>\n","<font color=\"black\">$\\displaystyle\\bigodot_{i=1...s}$：<font color=\"silver\">モデル$\\mathcal{N}$が$s$個のステージ(ブロック)で成り立っている<br>\n","<font color=\"black\">$\\rm FLOPS$：<font color=\"silver\">1秒あたりにできる浮動小数点演算の回数（FLoating-point Operations Per Second）<br><br></font>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/nuka137/20200908/20200908105522.png\" width=\"480\"><br>\n","     - 各スケーリングパラメータを程よい具合にスケールアップさせるための式を提案した。<br>\n","     - ユーザーが決めるcompound coefficient$\\phi$で広さ、深さ、解像度を下式のように全て同じように決定する <br>\n","     - 制約条件において β,γ が2乗されているのは、畳み込み演算が主となるCNNにおいて「ネットワークの幅」と「入力画像の解像度」が演算量に2乗のオーダーで効いてくるため<br><br>\n","   <font color=\"black\">${\\rm depth}: d=\\alpha^{\\phi}$<br>\n","<font color=\"black\">${\\rm width}: w=\\beta^{\\phi}$<br>\n","<font color=\"black\">${\\rm resolution}: r=\\gamma^{\\phi}$<br>\n","$\\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\simeq 2 \\qquad (\\alpha \\ge 1, \\beta \\ge 1, \\gamma \\ge 1)$<br>\n","<br>\n","<font color=\"black\">$\\phi$：<font color=\"silver\">利用できる計算資源の上限に基づいて決定されるべきハイパーパラメータ<br>\n","<font color=\"black\">$α, β ,γ$：<font color=\"silver\">Baseline Modelにおける層数、幅、解像度のスケーリング係数<br>\n","<font color=\"black\">$\\mathcal{F}, \\hat{L_i}, \\hat{H_i}, \\hat{W_i}, \\hat{C_i}$：<font color=\"silver\">H1を参考<br><br>\n","   <img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/n/nuka137/20200908/20200908105552.png\" width=\"400\"><br>"],"metadata":{"id":"fxF7fX8XvTCj"}},{"cell_type":"markdown","source":["# <font color=\"silver\">画像│appendix,モデル性能比較</font><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://keras.io/ja/applications/)</font></font><br>\n","<img src=\"https://deepsquare.jp/wp-content/uploads/2020/04/308a2eb78cf033e7438f2075c820fbb3.png\" width=\"640\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://towardsdatascience.com/neural-network-architectures-156e5bad51ba)</font></font><br>\n","<img src=\"https://miro.medium.com/max/828/1*n16lj3lSkz2miMc_5cvkrA.jpeg\" width=\"640\"><br>"],"metadata":{"id":"ccrq85SdAr1o"}},{"cell_type":"markdown","source":["# <font color=\"silver\">物体  [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!6827&cid=b0f01606242a6ed3&CT=1673578448468&OR=ItemsView)</font><br>"],"metadata":{"id":"vUhwswavDvHU"}},{"cell_type":"markdown","source":["# <font color=\"silver\">物体│2-stageと1-stage</font>\n","> - 2-stageと1-stage<br>\n","  - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://wikidocs.net/167502)</font></font><br><br>\n","<img src=\"https://wikidocs.net/images/page/167508/1-stage_det_vs_2-stage_det_detail.png\" width=\"640\"><br><br>\n","  - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://pyimagesearch.com/2022/04/11/understanding-a-real-time-object-detection-network-you-only-look-once-yolov1/)</font></font><br>\n","<img src=\"https://b2633864.smushcdn.com/2633864/wp-content/uploads/2022/04/single_vs_two_stage-768x560.png?lossy=1&strip=1&webp=1p\" width=\"480\">"],"metadata":{"id":"zw_SYcxTcNnU"}},{"cell_type":"markdown","source":["# <font color=\"silver\">物体│SPP</font>\n","> - Description<br>\n"," - <font color=\"Blue\">$\\tiny{\\rm Link}$ […</font>](https://cvml-expertguide.net/terms/dl/layers/pooling-layer/spatial-pyramid-pooling/)<br>\n","<img src=\"https://axa.biopapyrus.jp/media/sppnet-desc03.png\" width=\"320\">\n","<img src=\"https://axa.biopapyrus.jp/media/sppnet-desc01.png\" width=\"320\"><br><br>\n","<img src=\"https://i0.wp.com/cvml-expertguide.net/wp-content/uploads/2021/09/spp-layer.png?w=451&ssl=1\" width=\"320\"><br>\n","\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://cvml-expertguide.net/terms/dl/layers/pooling-layer/spatial-pyramid-pooling/)</font></font><br>"],"metadata":{"id":"FIfA-p239Bx4"}},{"cell_type":"markdown","source":["# <font color=\"silver\">物体│RPN</font>\n","> - <font color=\"silver\">Description</font><br>\n"," -  RPN</font><br>\n","   RPN, Region Proposal Network, 領域提案ネットワーク<br>\n","<font color=\"Blue\">$\\tiny{\\rm Link}$ […</font>](https://blog.negativemind.com/2019/02/20/general-object-recognition-faster-r-cnn/) […</font>](https://qiita.com/shtmr/items/4283c851bc3d9721ed96#2-rpnregion-proporsal-network) <br>\n","<img src=\"https://i0.wp.com/cvml-expertguide.net/wp-content/uploads/2021/09/RPN-1.png?resize=1024%2C662&ssl=1\" width=\"480\">\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F62555%2F267eaff6-4462-e413-e9d7-3239f728b4a7.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=84b488754133bd3053ab85763d2d1988\" width=\"480\"><br><br>\n"," - RPNの損失関数<br>\n","   - 合成損失関数で学習する<br><br>\n","$\\displaystyle\\mathcal{L}_{RPN} (\\{p_i\\},\\{{t}_i\\})  = \\frac{1}{N_{cls}}\\mathcal{L}_{cls}(p_i,p_i^*) + \\lambda \\frac{1}{N_{reg}} \\sum_i  p_i^* \\mathcal{L}_{reg} ({t}_i,{t}_i^*)$<br><br>\n","$\\mathcal{L}_{cls}$：<font color=\"silver\">log loss, $p_i^*\\log(p)$</font><br>\n","$\\mathcal{L}_{reg}$：<font color=\"silver\">smooth L1 loss</font><br>\n","$p_i$：<font color=\"silver\">アンカーボックスが物体である確率</font><br>\n","$p_i^*$：<font color=\"silver\">正解ラベル（アンカーボックスが正例（物体）の場合は 1、負例（背景）の場合は 0）</font><br>\n","${t}_i$：<font color=\"silver\">予測された座標ベクトル</font><br>\n","${t}_i^*$：<font color=\"silver\">真の座標ベクトル</font><br><br>\n"," - アンカーボックス方式の課題<br>\n","   -  アンカーボックスのアスペクト比や数が、予測精度に影響する<br>\n","   -  フィーチャーマップごとにアスペクト比を設定する必要がある<br>\n","   -  アンカーボックスのほとんどは、予測時に採用されない<br>\n","   -   全てのアンカーボックスの IoU を計算する必要あるため計算負荷が大きい<br>"],"metadata":{"id":"fjjwcsJfcSfN"}},{"cell_type":"markdown","source":["# <font color=\"silver\">物体│FPN</font>\n","> - FPN<br>\n","  - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://www.researchgate.net/figure/Illustration-of-the-feature-pyramid-network-FPN-The-FPN-consists-of-a-bottom-up_fig1_339006612)</font></font><br><br>\n","<img src=\"https://www.researchgate.net/publication/339006612/figure/fig1/AS:854656762789889@1580777459733/Illustration-of-the-feature-pyramid-network-FPN-The-FPN-consists-of-a-bottom-up.jpg\" width=\"640\"><br><br>\n","  - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/TaigaHasegawa/items/653abc81ac4ee1f0d7b8)</font></font><br><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F347235%2Fb8be118e-0de9-f23c-5fc1-d5c27862a05e.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=13b67d8fb621d25ca33c51eb0936f3fb\" width=\"640\"><br><br>\n","  - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/TaigaHasegawa/items/653abc81ac4ee1f0d7b8)</font></font><br><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F347235%2F57c457cb-c54d-8291-cca5-8008476a9909.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=6830565f22b49b5f439955eb0e556b18\" width=\"480\"><br><br>\n","  - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://cvml-expertguide.net/terms/dl/object-detection/fpn/)</font></font><br><br>\n","<img src=\"https://i0.wp.com/cvml-expertguide.net/wp-content/uploads/2022/10/fpn-detail.png?resize=768%2C446&ssl=1\" width=\"480\"><br><br>\n"],"metadata":{"id":"EOaOIwOl9Oj3"}},{"cell_type":"markdown","source":["# <font color=\"silver\">物体│ROI PoolingとROI align</font>\n","> - Description<br>\n",">    - ROI Pooling\n",">      - Selective Search又はRPNによって領域候補, Region Proposalとして縦横サイズが異なる関心領域, Region of Interestのバウンディングボックスが多数出力される。入力特徴マップを一定のサイズへと統一する目的\n"," - <font color=\"Blue\">$\\tiny{\\rm Link}$ […</font>](https://cvml-expertguide.net/terms/dl/object-detection/roi-pooling/)<br>\n"," <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://cvml-expertguide.net/terms/dl/layers/pooling-layer/spatial-pyramid-pooling/) [<font color=\"silver\">…](https://qiita.com/DeepTama/items/0cb9ca2d35c200deed37) [<font color=\"silver\">…](https://qiita.com/DeepTama/items/0cb9ca2d35c200deed37)</font></font></font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F1412761%2F617526e2-1986-976a-a2c1-3566712784dc.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=c20ceaa9d92c21c51eccf3b2b29bf6ab\" width=\"640\"><br>\n","<img src=\"https://i0.wp.com/cvml-expertguide.net/wp-content/uploads/2021/09/ROI_Pooling.png?resize=768%2C466&ssl=1\" width=\"480\">\n","<img src=\"https://user-images.githubusercontent.com/25688193/85193730-1e6f2e80-b306-11ea-931b-20a2c3326389.png\" width=\"480\">\n",">    - ROI align\n","       - <font color=\"Blue\">$\\tiny{\\rm Link}$ […</font>](https://qiita.com/yu4u/items/5cbe9db166a5d72f9eb8) […</font>](https://blog.negativemind.com/2019/04/27/general-object-detection-and-instance-segmentation-mask-r-cnn/)<br>\n"," - 候補領域座標の小数点以下の切り捨てを回避することで領域のズレを軽減<br>\n"," - RoI Poolingは、RoIを最大値や平均値で間引いており、多くの情報が欠落してしまうため、雑な離散化といえる。一方で、Roi Alignは、補間によってsub-pixelレベルの情報を考慮する形で固定サイズのベクトルを作成している<br>\n"," - RoI を N × N のグリッドに分割し,グリッドの各点の値を特徴マップの4 ヶ所からサンプリングした値をバイリニア補間して算出します.そして,その結果を max pooling あるいは average pooling して固定サイズの RoI 特徴ベクトルとします<br><br>\n"," <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://ivan-eng-murmur.medium.com/%E7%89%A9%E4%BB%B6%E5%81%B5%E6%B8%AC-s9-mask-r-cnn-%E7%B0%A1%E4%BB%8B-99370c98de28)</font></font><br>\n","<img src=\"https://user-images.githubusercontent.com/25688193/85193792-4c547300-b306-11ea-8fd3-927845519162.png\" width=\"640\"><br>\n","<img src=\"https://miro.medium.com/max/828/0*RTcInnhfoh0m9ItI\" width=\"640\"><br>"],"metadata":{"id":"O6UH1nw6-bMJ"}},{"cell_type":"markdown","source":["# <font color=\"silver\">物体│R-CNN family [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4894&cid=b0f01606242a6ed3&CT=1666717278455&OR=ItemsView)</font><br>\n","> - Description<br>\n"," - <font color=\"Blue\">$\\tiny{\\rm Link}$ […</font>](https://cvml-expertguide.net/terms/dl/object-detection/faster-r-cnn/) […</font>](https://qiita.com/amateur2020/items/14ae2d014c191e8c2360) <br>\n"," - R-CNN family</font><br>\n","<font color=\"black\">\n","\n",">|  R-CNN | SPP-NET  |Fast R-CNN  | Faster R-CNN |\n","| :---: | :---: | :---: | :---: |\n","|  入力画像  |  入力画像  |  入力画像 |入力画像 |\n","|  (SS)  |   (SS･CNN)  |   (SS･CNN) |（CNN）|\n","|  候補領域毎  |   候補領域･特徴マップ |   候補領域･特徴マップ  |特徴マップ|\n","|  (Crop･Resize)  |   (SPP)  |   (RoI Pooling) |（RPN）|\n","|  候補領域  |  空間サイズ固定特徴マップ |   空間サイズ固定特徴マップ  |特徴マップ･ 候補領域|\n","| （CNN） |   (Concate)  |   (Flatten) |（Flatten）|\n","|  特徴ベクトル  |   特徴ベクトル |   特徴ベクトル  |特徴ベクトル|\n","| （SVM･Regression） |  （SVM･Regression） |   (Multi-task Loss) |（Multi-task Loss）|\n","\n",">   - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/)</font></font><br>\n","<img src=\"https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/rcnn-family-summary.png\" width=\"800\"><br>\n","> - R-CNN</font>\n","     - 前半）Selective Search を使用し，後半ステージで処理する領域候補を検出\n","     - 後半）領域提案のBBox内画像から，R-CNNを順伝搬し特徴ベクトルを算出\n","     - 後半）CNN特徴ベクトルを入力として，以下2つを独立で予測する\n","       - [Classification] SVMで，物体クラスを識別\n","       - [Localization] 線形回帰により，領域候補に対するbboxの修正量を回帰<br><br>\n","<img src=\"https://koriavinash1.github.io/assets/images/blogs/ObDet/rcnn.png\" width=\"480\"><br>\n","> - Fast R-CNN</font><br>\n","     - 前半）Selective Search を使用し，後半ステージで処理する領域候補を検出\n","     - 後半）画像全体を先にCNNバックボーンに入力し，画像全体の特徴マップを計算\n","     - 後半）各番目の領域提案に対して，以下の計算を行う：\n","       - ROI Pooling層で，領域提案の特徴マップをプーリング    \n","       - 全結合層2層により，領域提案の特徴ベクトルを抽出\n","       - 全結合層のヘッド2つにより「softmax確率ベクトルをKクラス分」と「bboxの修正量 × Kクラス分」をそれぞれ推定\n","       - クラス確率最大のクラスと．そのクラスに対応するbbox修正量を出力（修正量のみ）\n","     - 後半）NMS非極大値抑制を行い，その結果を最終的な検出結果として出力\n","       - <img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/u/uiiurz1/20181223/20181223102316.png\" width=\"480\"><br>\n","> - Faster R-CNN</font><br>\n","     - 前半）アンカーボックス方式のRPNで，領域提案を検出\n","       - 特徴マップグリッド上の各グリッドの中心 =アンカー点に対して，領域候補の事前分布群に相当するアンカーボックスをK個用意する\n","       - 各アンカー位置において，K個のアンカーボックス群から「物体らしさを示すObjectness値 + バウンディングボックス修正量の回帰値 」を学習する\n","       - 序盤の特徴マップ抽出層であるVGGNet Pool5を 全画像の特徴マップを共有。これにより2モデルとも同一の特徴マップを用いて，残りの推定器ヘッドを学習できるようになった\n","     -後半）Fast R-CNNの各全結合層を畳み込み層に変更したネットワークに，RPNと同じ畳み込み層を前半に共有したネットワーク<br>\n","       - <img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/u/uiiurz1/20190101/20190101141054.png\" width=\"480\">\n","> - Mask R-CNN</font><br>\n","     - バウンディングボックスとセグメンテーションマスクを同時に予測するマルチタスク学習の寄与により精度が向上する.\n","     - 検出した bounding box に加えて物体かどうかの情報を 0 と 1 で持った Mask を用いて segmentation を行う.\n","     - Mask R-CNN では COCO(Common Objects in Context)データセットを用いた 3\n","つのタスクで上位の性能を発揮した.\n","       - <img src=\"https://www.researchgate.net/publication/355864197/figure/fig2/AS:1085873839976456@1635903906479/Architecture-of-Mask-R-CNN-for-instance-segmentation-adapted-from-8-and-27.ppm\" width=\"480\">\n","   - Multi-task loss<br>\n","       - モデルの出力は$p = (p_0,p_1,\\ldots, p_K)$と${t}^u = (t_x^u,t_y^u,t_w^u,t_h^u)$<br><br>\n","   $\\mathcal{L}({p},u,{t}^u,{v}) = \\mathcal{L}_{cls}({p},u) + \\lambda [ u \\geq 1]\\mathcal{L}_{loc}({t}^u, {v}) $<br><br>\n","   $u$：正解クラスのラベル<font color=\"silver\"></font><br>\n","   $p$：softmax出力のK+1次元クラス確率ベクトル<font color=\"silver\"></font><br>\n","   $v$：正解bboxの位置とサイズ<font color=\"silver\"></font><br>\n","   ${t}^u$：推定されたbboxの修正量<font color=\"silver\"></font><br><br>\n","       - クラス識別の損失には，事後確率の負の対数を用いる<br><br>\n","       $\\mathcal{L}_{cls}({p},u)  = – \\log p^u $<br><br>\n","       - bbox修正量回帰の損失には，外れ値に頑健な，Smooth L1損失を用いる<br><br>\n","$\\displaystyle\\mathcal{L}_{bbox} ({v}, {t} )= \\sum_{i \\in \\{ x,y,w,h \\}} L_1^{\\rm smooth}(t_i^u-v_i)$<br><br>\n","       - Smooth L1は原点付近はL2っぽく、それ以外はL1っぽくなる損失関数<br><br>\n","$L_1(x) = |x| \\quad L_2(x) = x^2$<br><br>\n","$\\displaystyle L_1^{\\rm smooth}(x)=\\begin{cases}0.5x^2 & if |x| < 1 \\\\|x| – 0.5 & {\\rm otherwise}\\end{cases}$"],"metadata":{"id":"WD_DPci-d9Gq"}},{"cell_type":"markdown","source":["# <font color=\"silver\">物体│YOLO  [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4900&cid=b0f01606242a6ed3&CT=1666717393959&OR=ItemsView)</font><br>\n","> - <font color=\"silver\">Description</font><br>\n","   - YOLO\n"," - 分割されたグリッドのサイズは固定で、グリッド内で識別クラスは1つまで、検出対象の物体は2つまで。そのため、グリッド内の物体が大量の場合は、精度が下がる<br>\n"," - サイズの小さな物体の検出を苦手としており、バウンディング・ボックスを個別に分析できるFaster R-CNNに比べて、識別の精度が低い。<br>\n"," - 領域探索とクラス分類を同時に実行し、物体検出を分類問題ではなく回帰問題推定を行う（Confidence Score）<br>\n"," - 検出と識別を同時に行うことで処理速度を早めた<br>\n"," - 画像全体に対して特徴マップを生成していくため、汎化制度が高い<br>\n"," - 小さい画像の検出は不得意な部分があり、密接した対象の識別には向いていない<br><br>\n","   - 1-stage Deterector\n"," - スライディングwindowやregion proposalといった領域スキャンのアプローチを使わずに、畳み込みニューラルネットワークで画像全体から直接物体らしさと位置を算出する。<br>\n"," - まず入力画像を正方形にリサイズし、それを畳み込みニューラルネットワークの入力とする（論文の例では448×448）。<br>\n","<br>\n","<img src=\"https://tech-swim-bike.info/wp-content/uploads/2022/02/YoLo-2.png\" width=\"480\"><br><br>\n","   - Network Design, Darknet<br><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F296108%2F53abd9cc-4633-44b6-7592-9854437bb64b.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=e6a4fcfaee4209aa78e21ae225b06921\" width=\"640\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://human-blog.com/yolocommentary3/)</font></font><br>\n","<img src=\"https://human-blog.com/wp-content/uploads/2022/05/Screen-Shot-2022-05-17-at-19.29.43-1024x457.png\" width=\"640\"><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://blog.negativemind.com/2019/02/21/general-object-recognition-yolo/)</font></font><br><br>\n","<img src=\"https://blog.negativemind.com/wp-content/uploads/2019/02/YOLO_detection.jpg\" width=\"640\"><br><br>\n","<img src=\"https://tech-swim-bike.info/wp-content/uploads/2022/02/YoLo-1.png\" width=\"480\"><br><br>\n","   - ネットワークの出力\n"," - 24層のCNNと４層のPooling層から特徴量を抽出し、２層の全結合層で物体のバウンディングボックスと物体の種類の確率を推定する。<br>\n"," - 畳み込み層の最終出力サイズ7×7はgrid cellの分割数と一致する。<br>\n"," - YOLOの出力は、1つのgrid cellにつきB × 5 + C個の出力となり、全体の出力はS × S × (B × 5 + C)個<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/#yolo-you-only-look-once)</font></font><br>\n","<img src=\"https://lilianweng.github.io/posts/2018-12-27-object-recognition-part-4/yolo-network-architecture.png\" width=\"640\"><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://human-blog.com/yolocommentary3/)</font></font><br>\n","<img src=\"https://human-blog.com/wp-content/uploads/2022/05/CE471F42-AA15-41CF-95F7-E5D31BF5FD84.png\" width=\"640\"><br><br>\n","   - Confidence Score\n"," - S × S の各 grid cell に対して、B個の Bounding Box を推定する。Sはユーザーが決める。論文では７×７。<br>\n"," - 1つのBounding Boxにつき、Bounding Boxの座標値(x, y, w, h)と、そのBounding Boxが物体である信頼度スコア（Confidence Score）の計5つの値を出力する。<br>\n"," - 座標値のx, yはgrid cellの境界を基準にしたBounding Boxの中心座標、幅wと高さhは画像全体のサイズに対する相対値。信頼度スコアはそのBounding Boxが物体か背景かの確率を表す。(物体なら1, 背景なら0)<br><br>\n","$\\rm Confidence = Pr(Object) * IoU$ <font color=\"silver\">物体が含まれている時の予測検出位置の正確性\n","<br><br><font color=\"black\">\n","$\\rm C = Pr(Class|Object)$<br><br>\n","$\\rm Confidence Score = C * Pr(Object) * IoU$<br><br>\n","   - YOLOの損失関数\n"," - 最終層のReLUを除いて活性化層はLeaky ReLUを利用する。<br>\n"," - 「(1)バウンディンボックスの損失」＋「(2)信頼度の損失」＋「(3)クラス識別損失（交差エントロピー）」の３つを重みづけした合成損失<br><br>\n","<img src=\"https://datahax.jp/wp-content/uploads/2019/01/YOLO_formula3.png\" width=\"480\"><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://ai-kenkyujo.com/artificial-intelligence/ai-architecture/)</font></font><br>\n","<img src=\"https://ai-kenkyujo.com/wp-content/uploads/2021/12/ai-img21-1.png.webp\" width=\"640\"><br>\n","<font color=\"black\">${ \\phi(x) = \\begin{cases} x, \\quad if \\quad x>0 \\\\ 0.1x, \\quad {\\rm otherwise} \\end{cases}}$\n","<br><br>\n","$\\mathcal{L}_{YOLOv1}= \\mathcal{L}_{BB} + \\lambda_1 \\mathcal{L}_{conf} +\\lambda_2  \\mathcal{L}_{CE}$<br><br>\n","$\\mathcal{L} = \\mathcal{L}_\\text{loc} + \\mathcal{L}_\\text{cls}$\n","<br><br>\n"," $\\begin{aligned}\n","\\mathcal{L}_\\text{loc} = \\lambda_\\text{coord} \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{ij}^\\text{obj} [(x_i - \\hat{x}_i)^2 + (y_i - \\hat{y}_i)^2 + (\\sqrt{w_i} - \\sqrt{\\hat{w}_i})^2 + (\\sqrt{h_i} - \\sqrt{\\hat{h}_i})^2 ] \n","\\end{aligned}$\n","<br><br>\n"," $\\begin{aligned}\n","\\mathcal{L}_\\text{cls}  = \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\big( \\mathbb{1}_{ij}^\\text{obj} + \\lambda_\\text{noobj} (1 - \\mathbb{1}_{ij}^\\text{obj})\\big) (C_{ij} - \\hat{C}_{ij})^2 + \\sum_{i=0}^{S^2} \\sum_{c \\in \\mathcal{C}} \\mathbb{1}_i^\\text{obj} (p_i(c) - \\hat{p}_i(c))^2\n","\\end{aligned}$\n","\n"],"metadata":{"id":"stFhSExy5xrC"}},{"cell_type":"markdown","source":["# <font color=\"silver\">物体│SSD</font><br>\n","<font color=\"silver\">SSD, Single Shot MultiBox Detector</font><br>\n","> - Description<br>\n"," - <font color=\"Blue\">$\\tiny{\\rm Link}$ […</font>](https://cvml-expertguide.net/terms/dl/object-detection/ssd/) […</font>](https://blog.negativemind.com/2019/02/26/general-object-recognition-single-shot-multibox-detector/)  […</font>](https://qiita.com/DeepTama/items/aab46729d2aa51a8954d)<br>\n","  - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://qiita.com/DeepTama/items/aab46729d2aa51a8954d)</font></font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F1412761%2Ff11ea69c-232c-68ba-d64d-6444f3308b5e.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=fb7d793bed5bed3bb7209cc42ea892c9\" width=\"640\">\n","  - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://cvml-expertguide.net/terms/dl/object-detection/ssd/)</font></font><br>\n","<img src=\"https://i0.wp.com/cvml-expertguide.net/wp-content/uploads/2021/10/SSD-e1644131334674.png?resize=780%2C629&ssl=1\" width=\"640\">\n","<img src=\"https://i0.wp.com/cvml-expertguide.net/wp-content/uploads/2022/11/dd4d85fe36651ade60d3c2415bf2f490.png?resize=768%2C365&ssl=1\" width=\"640\">\n"," - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://techplay.jp/event/846663)</font></font><br>\n","<img src=\"https://i.gyazo.com/4aa287bfec7dbe4102937a00dbacce81.png\" width=\"640\"><br><br>\n",">※ 信頼度(confidence)損失(3.2.1節)と領域検出(localization)損失 (3.2.2節)の，2タスクの損失を合成した，以下のMultibox損失関数</font><br><br>\n","$L(x, c, l, g) = \\cfrac{1}{N}(L_{CLS}(x, c) + \\alpha L_{LOC}(x, l, g))$<br><br>\n","${\\begin{aligned}\n","L_{\\mathrm{loc}}(x,l,g)\n","&= \\sum_{k=1}^p\\sum_{i\\in \\mathrm{Default Box}}\\sum_{j\\in \\mathrm{GT Box}} \\sum_{m\\in (cx, cy, w, h)} x_{ij}^k \\mathrm{smooth}_{\\mathrm{L1}}(l_i^m- \\hat g_j^m) \\\\\n","\\hat g_j^{\\mathrm{cx}} &= \\frac{g_j^{\\mathrm{cx}} - d_i^{\\mathrm{cx}}}{d_i^{\\mathrm y}} \\\\\n","\\hat g_j^{\\mathrm{cy}} &= \\frac{g_j^{\\mathrm{cy}} - d_i^{\\mathrm{cy}}}{d_i^{\\mathrm h}} \\\\\n","\\hat g_j^{\\mathrm w} &= \\log \\frac{g_j^{\\mathrm w} }{d_i^{\\mathrm w} } \\\\\n","\\hat g_j^{\\mathrm h} &= \\log \\frac{g_j^{\\mathrm h} }{d_i^{\\mathrm h} }\n","\\end{aligned}\n","}$<br><br>\n","${\\begin{aligned}\n","L_{\\mathrm{conf}}(x,c) &=\n","-\\sum_{k=1}^p\\sum_{i\\in \\mathrm{Default Box}}\\sum_{j\\in \\mathrm{GT Box}}  x_{ij}^k \\log(\\hat c_i^k) - \\sum_{i\\in \\mathrm{Default Box}}\\log(\\hat c_i^0)\\prod_{k=1}^p\\prod_{j\\in \\mathrm{GT Box}}(1-x_{ij}^k ) \\\\\n","\\hat c_i^k &= \\frac{\\exp(c_i^k)}{\\sum_k \\exp(c_i^k)}\n","\\end{aligned}\n","}$\n","<br><br>\n","$\\displaystyle {s_k = s_{\\mathrm{min}} + \\frac{s_{\\mathrm{max}} - s_{\\mathrm{min}}\n","}{m-1}(k-1), \\ k \\in [1,m]\n","}$\n","\n"],"metadata":{"id":"Qs3nVpVv5x5I"}},{"cell_type":"markdown","source":["# <font color=\"silver\">物体│FCOS [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4906&cid=b0f01606242a6ed3&CT=1666771227100&OR=ItemsView)</font><br>\n","<font color=\"silver\">FCOS, Fully Convolutional One-Stage Object Detection</font><br>\n","> - <font color=\"silver\">Description</font><br>\n"," - FCOS\n","   -  <font color=\"Blue\">$\\tiny{\\rm Link}$ […</font>](https://qiita.com/TaigaHasegawa/items/653abc81ac4ee1f0d7b8) […</font>](https://cvml-expertguide.net/terms/dl/object-detection/fpn/) <br>\n","   - アンカーボックスを用いない物体検出手法\n","     - アンカーボックスはポジティブサンプル数（検出物）とネガティブサンプル数（背景）が不均衡になるが、一方で、アンカーフリー法はポジティブ数とネガティブ数の不均衡を改善する。\n","     - アンカーボックスは特徴マップ上の各点につき複数個のバウンディングボックスを予測する。一方で、アンカーフリーは特徴マップ上の各点につき 1 つのバウンディングボックスを予測する。\n","     - 各特徴量マップの位置ijにおいて、正しいBounding boxとの上下左右の距離を回帰させる手法\n","   - FPN, Feature Pyramid Networks, 特徴ピラミッドネットワーク<br>\n","     - 異なるレベルの特徴マップを複数出力し、それぞれの特徴マップで、異なるサイズの物体を検出する\n","     - FCN と同様に、全結合層を持たないアーキテクチャ。<br>\n","     - head と呼ばれる拡張部分を各フィーチャーマップの後に追加し、Classification・Boundingbox regression・Center-nessを学習する。<br>\n","       - （上）クラスを出力。<br>\n","       - （中）バウンディングボックスの中心に対するズレを出力。<br>\n","       - （下）バウンディングボックスの各辺までの距離を出力。<br><br>\n","<img src=\"https://user-images.githubusercontent.com/24524018/88918756-f67bcf00-d2a4-11ea-9549-cb7a28a3c4b3.png\" width=\"800\"><br><br>\n","     - FPNを用いて複数のスケールで特徴量マップの抽出を行い、ピラミッドのレベルごと(P3〜P7)に予測することで、さまざまなスケールの特徴を捉える。（曖昧性を解消する）<br>\n","      -  例）テニスラケットと人を検出<br>\n","<img src=\"https://user-images.githubusercontent.com/24524018/88918758-f8459280-d2a4-11ea-9119-175abfa056b8.png\" width=\"320\"><br><br>\n","$\\begin{cases}l^{\\ast} = x - x^{(i)}_0, t^\\ast = y - y^{(i)}_0 \\\\ r^\\ast = x^{(i)}_1 - x, b^\\ast = y^{(i)}_1 - y  \\end{cases} $<br><br>\n","   - Center-ness for FCOS\n","     - FCOS ではフィーチャーマップ上の ground truth 内に入る全ての点はポジティブサンプル(物体が存在する領域)として扱う。そのため、物体の中心から離れた点を中心としたバウンディングボックスが予測されることがある。その様な状況を防ぐため、 FCOS では Center-ness というインデックスを学習に加え、低品質なバウンディングボックスが作成されることを抑制する。<br>\n","     - 正解バウンディングボックスから離れた位置に予測バウンディングボックスが出現することを抑制<br>\n","     - 学習時に、正解バウンディングボックスに対応する予測バウンディングボックス（正のサンプル）が多くなり、学習効率が上がる<br><br>\n","     - フィーチャーマップ上の点と ground truth の中心距離を数値化したインデックス。<br>\n","     - 特徴マップ上のある点が正解ボックスの中心からどれくらい離れているかを表す指標。<br>\n","     - 学習において損失は BCELoss:Binary Cross Entropyによって計算される。<br><br>\n","     - （ $l, t, r, b$ ）で計算する 0〜1の値で、値が大きいほど、正解ボックスの中心に位置する。<br>\n","     - 物体の中心付近では 1 に近く,物体の中心から離れるほど 0 に近くなる\n","     - 二乗根(√)をとるのは, Center-ness の減衰を抑制するため。\n","     - バウンディングボックス予測の際、センターネスの低い点はNMSによりフィルタリングする。<br>\n","<br><font color=\"black\">\n","$\\displaystyle {\\rm centerness}^\\ast = \\sqrt{\\frac{\\min(l^\\ast, r^\\ast)}{\\max(l^\\ast, r^\\ast)} \\times \\frac{\\min(t^\\ast, b^\\ast)}{\\max(t^\\ast, b^\\ast)}} $\n","<br><br>\n","（ $l^*, t^*, r^*, b^*$ ）：<font color=\"silver\">ある点からバウンディングボックスの各辺までの距離<br><br>\n","<img src=\"https://user-images.githubusercontent.com/24524018/88918747-f1b71b00-d2a4-11ea-8faa-1892f21c064b.png\" width=\"320\"></font>\n","   - 損失関数<br>\n","     - クラス分類に関する損失 + バウンディングボックスの座標に関する損失<br>\n","     - これによりアンカーが不要になる。<br><br>\n","<img src=\"https://user-images.githubusercontent.com/24524018/88918650-c7655d80-d2a4-11ea-8a04-32ade7894a88.png\" width=\"480\">\n","<br><font color=\"Black\">\n","<br>\n","第 1 項：<font color=\"silver\">クラス分類に関する損失<br></font>\n","第 2 項：<font color=\"silver\">バウンディングボックスの座標に関する損失<br></font>\n","<br>\n","$p_{x,y}$：<font color=\"silver\">クラス分類の予測確率<br></font>\n","$c_{x,y}^*$：<font color=\"silver\">正解のクラスラベル<br></font>\n","$t_{x,y}$：<font color=\"silver\">(l, r, t, b)の予測結果<br></font>\n","$t_{x,y}^*$：<font color=\"silver\">正解の$t_{x,y}$<br></font>\n","$1_{c_{x,y}^*>0}$：<font color=\"silver\">物体が存在するときは1<br></font>\n","$N_{pos}$：<font color=\"silver\">バウンディングボックスの個数<br></font>\n","<br>\n","$\\displaystyle L(\\{\\vec p_{x, y}\\}, \\{\\vec t_{x, y}\\}) = \\frac{1}{N_{pos}} \\sum_{x,y} L_{cls}(\\vec p_{x, y}, c^{\\ast}_{x, y}) + \\frac{\\lambda}{N_{pos}} \\sum_{x,y} I_{c^\\ast_{x,y} > 0} L_{reg} (\\vec t_{x, y}, \\vec t^\\ast_{x,y}) $<br><br>\n","\n"],"metadata":{"id":"LnJlphAlQKIw"}},{"cell_type":"markdown","source":["# <font color=\"silver\">物体│appendix, 損失関数比較\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://uiiurz1.hatenablog.com/entry/2018/12/26/204236)</font></font><br><br>\n","<img src=\"https://koriavinash1.github.io/assets/images/blogs/ObDet/rcnn.png\" width=\"480\"><br><br>\n","$\\displaystyle \\mathcal{L}_\\text{reg} = \\sum_{i \\in \\{x, y, w, h\\}} (t_i - d_i(\\mathbf{p}))^2 + \\lambda \\|\\mathbf{w}\\|^2$<br><br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/u/uiiurz1/20181223/20181223102316.png\" width=\"480\"><br><br>\n","$\\displaystyle \\mathcal{L}(p, u, t^u, v) = \\mathcal{L}_\\text{cls} (p, u) + \\mathbb{1} [u \\geq 1] \\mathcal{L}_\\text{box}(t^u, v) $<br><br>\n","$\\displaystyle \\mathbb{1} [u >= 1] = \\begin{cases}\n","    1  & \\text{if } u \\geq 1\\\\\n","    0  & \\text{otherwise}\n","\\end{cases}$<br><br>\n","$\\displaystyle \\mathcal{L}_\\text{cls}(p, u) = -\\log p_u $<br><br>\n","$\\displaystyle \\mathcal{L}_\\text{box}(t^u, v) = \\sum_{i \\in \\{x, y, w, h\\}} L_1^\\text{smooth} (t^u_i - v_i)$<br><br>\n","$\\displaystyle L_1^\\text{smooth}(x) = \\begin{cases}\n","    0.5 x^2             & \\text{if } \\vert x \\vert < 1\\\\\n","    \\vert x \\vert - 0.5 & \\text{otherwise}\n","\\end{cases}$<br><br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/u/uiiurz1/20190101/20190101141054.png\" width=\"480\"><br><br>\n","$\\displaystyle \\mathcal{L} = \\mathcal{L}_\\text{cls} + \\mathcal{L}_\\text{box}$<br><br>\n","$\\displaystyle\\mathcal{L}_{RPN} (\\{p_i\\},\\{{t}_i\\})  = \\frac{1}{N_{cls}}\\mathcal{L}_{cls}(p_i,p_i^*) + \\lambda \\frac{1}{N_{reg}} \\sum_i  p_i^* \\mathcal{L}_{reg} ({t}_i,{t}_i^*)$<br><br>\n","$\\displaystyle p^*_i \\cdot L_1^\\text{smooth}(t_i - t^*_i)$<br><br>\n","$\\displaystyle \\mathcal{L}_\\text{cls} (p_i, p^*_i) = - p^*_i \\log p_i - (1 - p^*_i) \\log (1 - p_i)$<br><br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/u/uiiurz1/20181231/20181231160724.png\" width=\"480\"><br><br>\n","$\\displaystyle \\mathcal{L}_\\text{mask} = - \\frac{1}{m^2} \\sum_{1 \\leq i, j \\leq m} \\big[ y_{ij} \\log \\hat{y}^k_{ij} + (1-y_{ij}) \\log (1- \\hat{y}^k_{ij}) \\big]$<br><br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/u/uiiurz1/20190101/20190101114445.png\" width=\"480\"><br><br>\n","$\\mathcal{L}_{YOLOv1}= \\mathcal{L}_{BB} + \\lambda_1 \\mathcal{L}_{conf} +\\lambda_2  \\mathcal{L}_{CE}$<br><br>\n","$\\displaystyle \\mathcal{L} = \\mathcal{L}_\\text{loc} + \\mathcal{L}_\\text{cls}$<br><br>\n","$\\displaystyle \\mathcal{L}_\\text{loc} = \\lambda_\\text{coord} \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\mathbb{1}_{ij}^\\text{obj} [(x_i - \\hat{x}_i)^2 + (y_i - \\hat{y}_i)^2 + (\\sqrt{w_i} - \\sqrt{\\hat{w}_i})^2 + (\\sqrt{h_i} - \\sqrt{\\hat{h}_i})^2 ] $<br><br>\n","$\\displaystyle \\mathcal{L}_\\text{cls}  = \\sum_{i=0}^{S^2} \\sum_{j=0}^B \\big( \\mathbb{1}_{ij}^\\text{obj} + \\lambda_\\text{noobj} (1 - \\mathbb{1}_{ij}^\\text{obj})\\big) (C_{ij} - \\hat{C}_{ij})^2 + \\sum_{i=0}^{S^2} \\sum_{c \\in \\mathcal{C}} \\mathbb{1}_i^\\text{obj} (p_i(c) - \\hat{p}_i(c))^2$<br><br>\n","<img src=\"https://ai-kenkyujo.com/wp-content/uploads/2021/12/ai-img21-1.png.webp\" width=\"640\"><br><br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/u/uiiurz1/20190101/20190101150346.png\" width=\"480\"><br>\n","$\\displaystyle \\mathcal{L}_{ \\text{multibox} }(x,c,l,g) = \\frac{1}{N}(\\mathcal{L}_{\\text{conf}}(x,c) + \\alpha \\mathcal{L}_{\\text{loc}}(x,\\hat{\\rm{l}},\\rm{g}))$<br><br>\n","$\\displaystyle \\mathcal{L}_{\\text{conf}}(x,c) = -\\sum_{i \\in Pos}^{N} x_{ij}^p \\log(\\hat{c}_i^p)- \\sum_{i \\in \\text{Neg}}^{N} x_{ij}^p \\log(\\hat{c}_i^p) $<br><br>\n","$\\displaystyle \\mathcal{L}_{loc}(x,\\hat{\\rm{l}},\\rm{g}) = \\sum_{i \\in {pos}}^{N} \\sum_{m \\in \\{c_x, c_y, w, h\\} } x^{k}_{ij} \\text{smooth}_{L_1}( \\hat{\\rm{l}}_j^m – \\rm{g}_i^m )$<br><br>\n"],"metadata":{"id":"q6uKqWAvQzff"}},{"cell_type":"markdown","source":["# <font color=\"silver\">物体│appendix, モデル構造比較\n","><font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://uiiurz1.hatenablog.com/entry/2018/12/26/204236)</font></font><br><br>\n","<img src=\"https://koriavinash1.github.io/assets/images/blogs/ObDet/rcnn.png\" width=\"320\">\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/u/uiiurz1/20181223/20181223102316.png\" width=\"320\"><br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/u/uiiurz1/20190101/20190101141054.png\" width=\"320\">\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/u/uiiurz1/20181231/20181231160724.png\" width=\"320\"><br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/u/uiiurz1/20190101/20190101114445.png\" width=\"320\">\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/u/uiiurz1/20190101/20190101150346.png\" width=\"320\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://www.flagly.org/project/projects/4/sections/42/)</font></font><br>\n","<img src=\"https://flagly.org/media/django-summernote/2020-08-12/85cbbe32-6292-4252-817e-2daf23432c7e.png\" width=\"800\"><br><br>"],"metadata":{"id":"4z9cHBraFjdV"}},{"cell_type":"markdown","source":["# <font color=\"silver\">分割 [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!6815&cid=b0f01606242a6ed3&CT=1673573221872&OR=ItemsView)</font><br>"],"metadata":{"id":"XxAQ__Yat94X"}},{"cell_type":"markdown","source":["# <font color=\"silver\">分割│Segmentation</font>\n","- Description<br>\n"," - <font color=\"Blue\">$\\tiny{\\rm Link}$ […</font>](https://cvml-expertguide.net/terms/dl/semantic-segmentation/) […</font>](https://cvml-expertguide.net/terms/dl/instance-segmentation/) <br>\n"," - <font color=\"silver\">$\\tiny\\text{画像引用元}$ [<font color=\"silver\">…](https://www.skillupai.com/blog/tech/segmentation1/) [<font color=\"silver\">…](https://cvml-expertguide.net/terms/dl/instance-segmentation/) [<font color=\"silver\">…](https://blog.albert2005.co.jp/2020/08/18/ssap/) [<font color=\"silver\">…](https://gigazine.net/news/20180208-capsnet/)</font></font><br></font>\n","<img src=\"https://www.skillupai.com/wp-content/uploads/2021/03/01-1.png\" width=\"320\"><img src=\"https://i0.wp.com/cvml-expertguide.net/wp-content/uploads/2021/10/instance_segmentation.png?resize=768%2C605&ssl=1\" width=\"320\"><br>\n","<img src=\"https://blog.albert2005.co.jp/wp-content/uploads/2020/07/%E5%9B%B33.png\" width=\"320\"><img src=\"https://i.gzn.jp/img/2018/02/08/capsnet/01.jpg\" width=\"320\"><br>\n"," - Semantic segmentation, 意味的分割\n","    - 画像の全てのピクセルについて、物体ならばその物体のクラス、それ以外は背景と分類\n","    - 車ならば車で同じラベルが振られ、重なりがある場合も考慮しない\n","    - 認識する物体のクラスと背景について全てのピクセルを塗り絵で表現\n","    - 同じクラスの物体が重なっていても、それらを区別しない\n","  - Instance segmentation, 実例分割\n","    - 物体に関してそれぞれの位置とその占める領域を認識するタスク\n","    - 物体でないもの、例えば、空やビルといった背景のような決まった形状をもたないオブジェクトは区別しない\n","    - 異なる物体が重なっていてもそれを区別\n","    - 同じクラスでも異なる物体は区別\n","   - 物体について、それらの個々が占める領域を推定\n","    - 同じクラスの物体が重なっていてもそれらの領域を別々に推定\n","  - Panoptic Segmentation (PS)\n","    - 数えられる物体(Thing)については IS数えられない背景(Stuff )については SSを実施する<br>\n",">\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://techplay.jp/event/846663)</font></font><br>\n","<img src=\"https://i.gyazo.com/c30bd55f4a2d8749305b1b96a63175d2.png\" width=\"480\"><br>"],"metadata":{"id":"f7Bfz_Z2qyV9"}},{"cell_type":"markdown","source":["# <font color=\"silver\">分割│FCN</font><br>\n","<font color=\"silver\">FCN, Fully Convolutional Networks, 全層畳み込み</font><br>\n","> - <font color=\"silver\">Description<br>\n","<font color=\"Blue\">$\\tiny{\\rm Link}$ […</font>](https://blog.negativemind.com/2019/03/11/semantic-segmentation-by-fully-convolutional-network/) […</font>](https://ai-kenkyujo.com/artificial-intelligence/ai-architecture-03/) […</font>](https://cvml-expertguide.net/terms/dl/semantic-segmentation/fcn/) <br>\n"," - <font color=\"Blue\">Fully Convolution</font>\n","   - 全結合層を1×1の畳み込み層に置き換えている。<br>\n","   - 全結合層を畳み込み層に置き換えることで、出力を分類クラスではなく二次元マップに変えることができる。<br>\n","   - 全結合層が存在することにより、固定サイズの画像しか扱うことができなかった。<br>\n","   - 全結合層が存在しないことにより、あらゆるサイズの画像でセグメンテーションマップが生成できる。<br>\n","   - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://blog.negativemind.com/2019/03/11/semantic-segmentation-by-fully-convolutional-network/)</font></font><br>\n","<img src=\"https://blog.negativemind.com/wp-content/uploads/2019/03/output_heatmap.jpg\" width=\"480\"><br><br>\n","<img src=\"https://i0.wp.com/cvml-expertguide.net/wp-content/uploads/2021/06/FCN.png?resize=768%2C399&ssl=1\" width=\"480\"><br><br>\n"," - <font color=\"Blue\">逆畳み込みによるアップサンプリング</font>\n","   - プーリング層の後、逆畳み込みを用いてアップサンプリングを施すことで、出力される画像サイズを大きくする。\n","   - 損失は、ピクセル毎のクロスエントロピー誤差の合計。\n"," - <font color=\"Blue\">Skip Connections-1</font>\n","   - ダウンサンプリングされた特徴マップに対して単純に逆畳み込みを適用するだけでは、出力結果が粗くなってしまうため、スキップ接続を用いて情報ロスが発生する前の情報をアップサンプリング処理に入力する。<br><br>\n","<img src=\"http://www.renom.jp/ja/notebooks/tutorial/image_processing/fcn-segmentation/fcn-segmentation.png\" width=\"640\"><br><br>\n"," - <font color=\"Blue\">Skip Connections-2</font>\n","   - 特徴抽出の最終層だけでなく、途中のpooling層で出力される大きいサイズの特徴マップも活用する。特徴マップのサイズは各層で異なるので、最終層の特徴マップから順にアップサンプリングで前の層と同サイズに拡大し、チャンネルごとに足し算する。\n","   - 足し算後の特徴マップに対して1×1の畳み込み処理を行う。\n","途中の層で出力されるサイズの大きい特徴マップを利用することで、物体の詳細な情報を捉えたsemantic segmentationが可能となる。<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://blog.negativemind.com/2019/03/11/semantic-segmentation-by-fully-convolutional-network/)</font></font><br>\n","<img src=\"https://blog.negativemind.com/wp-content/uploads/2019/03/upsampling.jpg\" width=\"800\">"],"metadata":{"id":"ctVpukfE5yZC"}},{"cell_type":"markdown","source":["# <font color=\"silver\">分割│SegNet</font><br>\n","> - Description<br>\n"," - <font color=\"Blue\">$\\tiny{\\rm Link}$ […</font>](https://blog.negativemind.com/2019/03/14/semantic-segmentation-by-segnet/) […</font>](https://ai-kenkyujo.com/artificial-intelligence/ai-architecture-03/) <br>\n"," -  Encoder-Decoder<br>\n","   -  FCNでは、途中の各pooling層の特徴マップをアップサンプリング・連結して利用していたが、そのために各特徴マップを一時的に保持する必要があり、メモリ効率が悪かった。\n","   -  SegNetでは、Encoderでpoolingした位置をmax-pooling indexとして記憶しておき、Decoderで特徴マップをそのindex位置にアップサンプリングすることでメモリ効率を高めている。<br><br>\n","    <img src=\"https://camo.qiitausercontent.com/015333dd8544c30001ab4960e539970793ad9ecb/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3236363433362f62386332666363372d316233372d366366322d363863372d3563653330623133333430622e706e67\" width=\"640\"><br><br>\n"," -  max-pooling index<br>\n","   - FCNのようにプーリング前の特徴をアップサンプリング層にコピーするのではなく、 エンコーダ部分のマックスプーリング層で採用した値の場所を記録しておき、デコーダ部分のアップサンプリング時にその場所を使って特徴マップを拡大する。これにより位置情報が保持される。<br>\n","   <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://cvml-expertguide.net/terms/dl/semantic-segmentation/segnet/)</font></font><br>\n","    <img src=\"https://blog.negativemind.com/wp-content/uploads/2019/03/upsampling_segnet.jpg\" width=\"320\"><img src=\"https://i0.wp.com/cvml-expertguide.net/wp-content/uploads/2021/09/34e77b6def4d419879648fec67ebfdf6.png?w=609&ssl=1\" width=\"400\"><br><br>\n"," -  Batch Normalization<br>\n","   -  <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://techplay.jp/event/846663)</font></font><br>\n","<img src=\"https://i.gyazo.com/ea4435c9c94153219a6c3586f182b611.png\" width=\"640\"><br>\n"],"metadata":{"id":"wSvThfSIHrPK"}},{"cell_type":"markdown","source":["# <font color=\"silver\">分割│U-Net</font></font><br>\n","> - Description<br>\n"," - <font color=\"Blue\">$\\tiny{\\rm Link}$ […</font>](https://blog.negativemind.com/2019/03/15/semantic-segmentation-by-u-ne) […</font>](https://ai-kenkyujo.com/artificial-intelligence/ai-architecture-03/) <br>\n"," -  Encoder-Decoder<br>\n","    -  <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://blog.negativemind.com/2019/03/15/semantic-segmentation-by-u-ne)</font></font><br>\n","<img src=\"https://blog.negativemind.com/wp-content/uploads/2019/03/u-net-architecture-1.jpg\" width=\"800\"><br>\n"," - Skip Connections\n"," -  Copy and Crop<br>\n"," -  Concatenate<br>\n","<br>\n","\n","\n","U-Net は、セマンティックセグメンテーションのタスクにおいて重要となる「局所的な特徴量と、画像全体の特徴の両方を捉えること」 を同時に実現できる。\n","より詳細には、Encoder 側の浅い層から、画像全体の大域的な特徴量を skip connection 経由で、Decoder 側に送り、Encoder 側の深い層からの、画像の局所的な特徴量を skip connection 経由で、Decoder 側に送る。\n","そして、Decoder 側で、これら skip connection で送られてきた大域的特徴量と局所的特徴量を保持したたま、アップサンプリングを行い、変換前と同じ解像度の画像を出力する。"],"metadata":{"id":"J2WZmk78HrhU"}},{"cell_type":"markdown","source":["# <font color=\"silver\">言語</font><br>"],"metadata":{"id":"t7Mbofm_uVCE"}},{"cell_type":"markdown","source":["# <font color=\"silver\">言語│タスク</font><br>\n","> - 回帰, one to one</font><br>\n","- 画像キャプション生成, one to many</font><br>\n","- 分類・感情分析, many to one</font><br>\n","- 翻訳, many to many</font><br>\n","- 文章生成, many to many</font><br><br>\n","<img src=\"http://karpathy.github.io/assets/rnn/diags.jpeg\" width=\"480\"><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>]((https://axa.biopapyrus.jp/deep-learning/rnn/)</font><br>\n","<img src=\"https://axa.biopapyrus.jp/media/rnn_network_structures.png\" width=\"480\">"],"metadata":{"id":"cQ1wtcLIVcPQ"}},{"cell_type":"markdown","source":["# <font color=\"silver\">言語│n-gram</font><br>\n","> <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>]((https://devopedia.org/n-gram-model)</font><br>\n","<img src=\"https://devopedia.org/images/article/219/7356.1569499094.png\" width=\"480\"><br>\n","> <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://resanaplaza.com/2022/07/31/%E3%80%90c%E3%80%91%E6%96%87%E5%AD%97%E5%88%97%E3%81%AE%E9%A1%9E%E4%BC%BC%E5%BA%A6%E3%82%92n-gram%E3%81%A8%E3%82%B3%E3%82%B5%E3%82%A4%E3%83%B3%E9%A1%9E%E4%BC%BC%E5%BA%A6%E3%81%A7%E6%B1%82%E3%82%81/)</font><br>\n","<img src=\"https://resanaplaza.com/wp-content/uploads/2022/07/unicos_001-1536x608.jpg\" width=\"640\">"],"metadata":{"id":"W4Nq_Z4zYjQk"}},{"cell_type":"markdown","source":["# <font color=\"silver\">言語│wordpieceとsentencepiece</font><br>\n","\n","> - Wordpieceとは、<br>\n","学習済みの Wordpiece モデルを用いて単語 (主に未知語) をサブワード (Subword Units) に分割する手法<br>\n","> - Sentencepieceとは、<br>\n","文をサブワードに分割する手法である．つまり形態素解析をする必要がない<br>"],"metadata":{"id":"pMtrD8IkYFv-"}},{"cell_type":"markdown","source":["# <font color=\"silver\">言語│形態要素解析</font><br>\n","> <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://udemy.benesse.co.jp/data-science/ai/morphological-analysis.html)</font><br>\n","<img src=\"https://udemy.benesse.co.jp/wp-content/uploads/f67805417f5968f3f2b5d07bc01f42c0-medium.jpg\" width=\"320\"><br>\n","\n","\n"],"metadata":{"id":"NdGKR2-daUjJ"}},{"cell_type":"markdown","source":["# <font color=\"silver\">言語│TF-IDF</font><br>\n","<font color=\"silver\">TF-IDF, Term Frequency - Inverse Document Frequency, 単語頻度と逆文書頻度</font><br>\n","> <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://atmarkit.itmedia.co.jp/ait/articles/2112/23/news028.html)</font><br>\n","<img src=\"https://image.itmedia.co.jp/ait/articles/2112/23/di-capture01.png\" width=\"480\"><br><br>\n","<img src=\"https://image.itmedia.co.jp/ait/articles/2112/23/di-capture02.png\" width=\"480\"><br><br>\n","<img src=\"https://image.itmedia.co.jp/ait/articles/2112/23/di-capture03.png\" width=\"480\"><br><br>"],"metadata":{"id":"O6Bv9sWnZTLl"}},{"cell_type":"markdown","source":["# <font color=\"silver\">言語│Bag-of-Words Models</font><br>\n","> ※ 文書における単語の出現頻度を 数えて頻度ベクトルで表現する方法。<br>\n","> ※ BOWの問題として単純な単語の計数としているため、その単語が文書集合の中でどのくらい重要であるのか、といった情報が欠録している。そのため単純に文書が長いほうが有利になると行った問題点がある。<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/part1.html)</font><br>\n","<img src=\"https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/img/pic201903-101.png\" width=\"640\">"],"metadata":{"id":"T1YecTbCGhTh"}},{"cell_type":"markdown","source":["# <font color=\"silver\">言語│Word2Vec  [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4625&cid=b0f01606242a6ed3&CT=1666544110776&OR=ItemsView)</font><br>\n","> - <font color=\"silver\">Description</font><br>\n"," - Word2Vec<br>\n","<font color=\"blue\">$\\tiny\\text{link}$ […</font>](https://lena-voita.github.io/nlp_course/word_embeddings.html) […</font>](https://shephexd.github.io/deep%20learning/2019/05/28/Deep_learning(14)-WordRepresentation.html)<br></font>\n","   - Skip-Gramのほうが計算コストが高いが精度が高い<br>\n","   - 構造は２層で$W_{in}$と$W_{out}$をパラメータとして持つ、$W_{in}$を単語埋め込み行列として用いる<br><br>\n","<img src=\"https://ainow.ai/wp-content/uploads/2021/04/image7-1.jpg\" width=\"640\"><br><br>\n","   - Word2Vecの課題<br></font>\n","     - Word2Vecは前後の単語から予測するといったことを行う。そのため、同じ文脈で使われる単語について同じと認識する。また同じ多義語として使われるような単語についてもあまり良い学習ができない。<br><br>\n","   - Negative Sampling<br></font>\n","     - １個の正例と$k$個の負例だけを対象として、シグモイド関数と交差エントロピー誤差関数を計算する。\n","     - 負例サンプリングを適用した場合は$k⁺1$個の２クラス分類問題を解いていることになる。\n","     - Sampling\n","       - コーパス中の各単語の単語出現頻度確率分布から、単語をサンプリングする。コーパス中で多く登場した単語は抽出されやすくなり、レアな単語は抽出されにくくなる。よく出現する単語ほど、きちんと予測できるようにしたいことによる。<br>\n","       - $k$はハイパラで小さなデータセットにおいては5～20、大きなデータセットにおいては2～5を推奨している\n","       - Negative Samplingでは元の確率分布に対して0.75を累乗することが経験的な推奨として提案されている。出現確率の低い単語を見捨てないようにするため<br>\n","       - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://medium.com/towards-datascience/word2vec-negative-sampling-made-easy-7a1a647e07a4)</font><br>\n","<img src=\"https://miro.medium.com/max/828/1*AH2nhhgon6ca8d_6TflGEA.webp\" width=\"480\"><br>\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"OA8e0l9WGqgF"}},{"cell_type":"markdown","source":["# <font color=\"silver\">言語│Language Models</font><br>\n","- <font color=\"silver\">Description</font><br>\n"," - <font color=\"Blue\">Language Models</font><br>\n"," <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)</font><br></font>\n","<img src=\"https://lena-voita.github.io/resources/lectures/seq2seq/general/lm_clm-min.png\" width=\"480\"><br><br>\n"," - <font color=\"Blue\">N-gram Language Models</font><br><br>\n","$\\displaystyle P(y_1, y_2, \\dots, y_n)=P(y_1)\\cdot P(y_2|y_1)\\cdot P(y_3|y_1, y_2)\\cdot\\dots\\cdot P(y_n|y_1, \\dots, y_{n-1})=  \\prod \\limits_{t=1}^n P(y_t|y_{\\mbox{<}t}).$\n","<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://lena-voita.github.io/nlp_course/language_modeling.html)</font><br></font>\n","<img src=\"https://lena-voita.github.io/resources/lectures/lang_models/ngram/example_cut_3gram-min.png\" width=\"640\"><br>\n"," - <font color=\"Blue\">DeepRNN</font><br>\n","前のn-gramには「n単語に分けた中に十分な情報がないと、正しい意味を読み取れない」という問題点があった。RNNLMは「それ以前の文脈を情報として渡す」ことによってn-gramの問題点を解決し高精度を出した。<br><br>\n","   - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://d2l.ai/chapter_recurrent-modern/deep-rnn.html)</font><br>\n","<img src=\"https://d2l.ai/_images/deep-rnn.svg\" width=\"240\"><br><br>\n","$\\mathbf{H}_t^{(l)} = \\phi_l(\\mathbf{H}_t^{(l-1)} \\mathbf{W}_{xh}^{(l)} + \\mathbf{H}_{t-1}^{(l)} \\mathbf{W}_{hh}^{(l)}  + \\mathbf{b}_h^{(l)})$<br><br>\n","$\\mathbf{O}_t = \\mathbf{H}_t^{(L)} \\mathbf{W}_{hq} + \\mathbf{b}_q$<br><br>\n"," - <font color=\"Blue\">BidirectionalRNN</font><br>\n","   - <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://d2l.ai/chapter_recurrent-modern/bi-rnn.html)</font><br>\n","<img src=\"https://d2l.ai/_images/birnn.svg\" width=\"320\"><br><br>\n","$\\overrightarrow{\\mathbf{H}}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh}^{(f)} + \\overrightarrow{\\mathbf{H}}_{t-1} \\mathbf{W}_{hh}^{(f)}  + \\mathbf{b}_h^{(f)})$<br><br>\n","$\\overleftarrow{\\mathbf{H}}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh}^{(b)} + \\overleftarrow{\\mathbf{H}}_{t+1} \\mathbf{W}_{hh}^{(b)}  + \\mathbf{b}_h^{(b)})$<br><br>\n","$\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q$<br><br>\n"," - <font color=\"blue\">Seq2Seq 学習時（上）と予測時（下）</font><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://d2l.ai/chapter_recurrent-modern/seq2seq.html)</font><br>\n","<img src=\"https://d2l.ai/_images/seq2seq.svg\" width=\"400\"><br>\n","<img src=\"https://d2l.ai/_images/seq2seq-predict.svg\" width=\"400\"><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/DeepTama/items/20b93ff8b8547428f662)</font><br>\n","<img src=\"https://camo.qiitausercontent.com/b46d7eff9ace234eaf02e91ca42f362c778526d9/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313431333235382f32303638316363622d663566622d386463382d303136352d3165363239353164666461372e706e67\" width=\"400\">\n","<img src=\"https://camo.qiitausercontent.com/0408f9a6724c7362b3887d7ccb0da44c602eaae9/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313431333235382f63636336386632362d353963652d383833362d663762312d3237646263306661636633302e706e67\" width=\"400\"><br><br>\n","<img src=\"https://camo.qiitausercontent.com/b46d7eff9ace234eaf02e91ca42f362c778526d9/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313431333235382f32303638316363622d663566622d386463382d303136352d3165363239353164666461372e706e67\" width=\"400\">\n","<img src=\"https://camo.qiitausercontent.com/7c8dca79ed07a107106c6fb861f0dbb64bd8eb6d/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313431333235382f32343864393233362d646666652d633631372d336664302d3239633434386165376561652e706e67\" width=\"400\"><br><br></font>\n"," - <font color=\"blue\">Teacher Forcing</font><br>\n","    - 学習時は、ある時刻の教師データを強制的に次の時刻のノードに入力させる。<br>\n"," 予測時は、計算によって求められた出力を次の時刻のノードに入れる。<br>\n"," 教師強制で学習させる場合、訓練時のみ、decoderの入力に正解データを入れる。<br>\n"," これにより、計算が安定し、収束までの時間が早くなることが期待できる。<br>\n"," ただし、テスト時におけるdecoderの入力は、予測された値になるため、訓練時と分布が異なってしまうことに注意。<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…](https://satopirka.com/2018/02/encoder-decoder%E3%83%A2%E3%83%87%E3%83%AB%E3%81%A8teacher-forcingscheduled-samplingprofessor-forcing/)</font><br>\n","<img src=\"https://satopirka.com/static/fb0e6efd51a0b36adc0ef6d7b8d8c74b/bb2fd/teacher-forcing.png\" width=\"480\"><br><br></font>\n"," - <font color=\"blue\">Reverse</font><br><br>\n"," - <font color=\"blue\">Peeky</font><br>\n"," <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/DeepTama/items/20b93ff8b8547428f662)</font><br>\n","<img src=\"https://camo.qiitausercontent.com/b46d7eff9ace234eaf02e91ca42f362c778526d9/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313431333235382f32303638316363622d663566622d386463382d303136352d3165363239353164666461372e706e67\" width=\"400\">\n","<img src=\"https://camo.qiitausercontent.com/5dac1d082ab2612085880d74b02495dc48bd69f0/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f313431333235382f39366430353961652d366234352d643735342d623565312d3766333832643331613639622e706e67\" width=\"400\"><br>"],"metadata":{"id":"8oCzJy4JfYHW"}},{"cell_type":"markdown","source":["# <font color=\"silver\">言語│GNMT [<font color=\"silver\">…](https://1drv.ms/w/s!AtNuKiQGFvCwpDqHErlaj63TplyF?e=NNelkC)</font><br></font>\n","> - Encoder and Decoder<br>\n",">  -  8層のエンコーダおよび8層のデコーダ\n","<img src=\"https://norman3.github.io/papers/images/gnmt/f01.png\" width=\"640\"><br><br>\n","> - GPU<br></font>\n",">  -  1層ごとに計16個の GPU を割り当てる。\n","> - 双方向LSTM<br></font>\n",">  -  エンコーダの1層目（1段目と2段目を合わせた部分）が 双方向LSTM\n",">  -  もし、全層を双方向LSTMにした場合、それぞれの層は前の層の順方向と逆方向 の計算が終わるのを待たなければならず、 2GPU (順方向に1、 逆方向に1) し か並列に動かすことができない。\n","> - Attention<br></font>\n",">  -  エンコーダ第8層の出力とデコーダ第1層の出力でAttentionを計算し、デコーダの全層に送る。\n",">  -  エンコーダの第8層とデコーダの第1層で計算するのは、並列化の効率を高めるため。 エンコーダの第8層とデコーダの第8層とでアテンションを計算すると待ち時間が発生する。<br><br>\n",">$s_t = \\mathrm{AttentionFunction}(\\boldsymbol{y_{i-1}}, \\boldsymbol{x_t})  \\quad ∀t,\\quad 1 ≤ t ≤ M$<br><br>\n","> $p_t = \\exp(s_t)/\\displaystyle\\sum_{t=1}^{M}\\exp(s_t) \\quad ∀t,\\quad 1 ≤ t ≤ M$<br><br>\n","> $\\boldsymbol{a_i} = \\displaystyle\\sum_{t=1}^{M}p_t \\boldsymbol{x_t}$<br><br>\n","> - Residual Connections<br></font>\n","> - 分散学習<br></font>\n",">  -  モデル並列 及び データ並列 を採用しており、パラメータの更新は 非同期型 で行う。<br>\n","> - Wordpiece と Sentencepiece<br></font>\n",">  -  サブワードの考え方を少し変えたのがSentencePiece\n",">  -  SentencePieceはテキストから文字や部分文字列に分解する。\n",">  -  SentencePieceを使うと未知語がなくなり、語彙数を小さくすることができる。\n","> - 強化学習<br></font>\n",">  -  Reward Objective, 強化学習による再学習<br>\n","> - 量子化<br></font>\n",">  -  Quantization, 量子化による高速化<br>"],"metadata":{"id":"4mjkeP09leae"}},{"cell_type":"markdown","source":["# <font color=\"silver\">言語│Transformer [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4673&cid=b0f01606242a6ed3&CT=1666551601096&OR=ItemsView)</font><br>\n","- <font color=\"blue\">Encoder and Decoder<br></font>\n","  -  <font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://nlpillustration.tech/?p=2171)</font><br>\n","<img src=\"https://nlpillustration.tech/wp-content/uploads/2022/08/Transformer6-770x770.jpg\" width=\"320\"><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://lilianweng.github.io/posts/2018-06-24-attention/)</font><br>\n","<img src=\"https://lilianweng.github.io/posts/2018-06-24-attention/transformer.png\" width=\"640\"><br><br></font>\n","- <font color=\"blue\">Input Encoding<br></font>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://qiita.com/jun40vn/items/35f6f0d26f9e58f01e4e)</font><br>\n","<img src=\"https://camo.qiitausercontent.com/32654fed87f53f2b82b773d50c36e94a97f9bec0/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3230393730352f31363033633765302d653532392d346237632d653936312d3039396561653661663433632e706e67\" width=\"400\"><br><br>\n","class Embedder(nn.Module):<br>\n","$\\qquad$def  __ init __ (self, text_embedding_vectors):<br>\n","$\\qquad$$\\qquad$super(Embedder, self). __ init __ ()<br>\n","$\\qquad$$\\qquad$self.embeddings = nn.Embedding.from_pretrained(text_embedding_vectors, freeze=True) <font color=\"silver\"># freeze=Trueで更新しない</font><br>\n","$\\qquad$def forward(self, x): <font color=\"silver\">(T, ) = (256, )</font><br>\n","$\\qquad$$\\qquad$x_vec = self.embeddings(x)<br>\n","$\\qquad$$\\qquad$return x_vec <font color=\"silver\">(T, D) = (256, 300)</font><br><br>\n","- <font color=\"blue\">Positional Encoding<br></font>\n","時系列を考慮するために、入力の埋め込み表現に「位置情報」を埋め込む。<br><br>\n","$\\rm \\displaystyle PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)$<br><br>\n","$\\rm \\displaystyle PE_{(pos, 2i + 1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)$<br><br><br>\n","$\\displaystyle P(\\text{pos},2d) = \\sin \\left( \\frac{ \\text{pos} }{ 10000^{2d/d_{model}} }\\right)$<br><br>\n","$\\displaystyle P(\\text{pos},2d+1) = \\cos \\left( \\frac{ \\text{pos} }{ 10000^{2d/d_{model}} }\\right)$<br><br><br>\n","$\\rm{PE}_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})$\n","<br><br>\n","$\\rm{PE}_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})$\n","<br><br>\n","$pos$：<font color=\"silver\">位置</font><br>\n","$i$：<font color=\"silver\">次元</font><br><br>\n","class PositionalEncoder(nn.Module):<br>\n","$\\qquad$def  __ init __ (self, d_model=300, max_seq_len=256): <font color=\"silver\"># 埋め込みベクトルの次元と入力長</font><br>\n","$\\qquad$$\\qquad$super(). __ init __ ()<br>\n","$\\qquad$$\\qquad$self.d_model = d_model<br>\n","$\\qquad$$\\qquad$pe = torch.zeros(max_seq_len, d_model) <font color=\"silver\"># PositionalEncodeを格納する受け皿, 零テンソル</font><br>\n","$\\qquad$$\\qquad$for pos in range(max_seq_len):<br>\n","$\\qquad$$\\qquad$$\\qquad$for i in range(0, d_model, 2):<br>\n","$\\qquad$$\\qquad$$\\qquad$$\\qquad$pe[pos, i] = math.sin(pos / (10000 ** ((2*i)/d_model)))<br>\n","$\\qquad$$\\qquad$$\\qquad$$\\qquad$pe[pos, i+1] = math.cos(pos / (10000 ** ((2*i)/d_model)))<br>\n","$\\qquad$$\\qquad$$\\qquad$self.pe = pe.unsqueeze(0)  <font color=\"silver\"># peの先頭に、ミニバッチを表す次元を追加</font><br>\n","$\\qquad$$\\qquad$$\\qquad$self.pe.requires_grad = False  <font color=\"silver\"># 勾配を計算しないようにする</font><br>\n","$\\qquad$def forward(self, x): <font color=\"silver\"># 入力xとpeを足し合わせる, xがpeよりも小さいため次元数の平方根を掛けて拡大</font><br>\n","$\\qquad$$\\qquad$$\\qquad$ret = math.sqrt(self.d_model)*x + self.pe<br>\n","$\\qquad$$\\qquad$$\\qquad$return ret  <font color=\"silver\">(T, D) = (256, 300)</font><br><br>\n","- <font color=\"blue\">Source Target AttentionとSelf-Attention</font><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://nlpillustration.tech/?p=2171)</font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F7f04542b-5d19-b76c-7951-737ecf4b1c3b.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=3d7cd0cb6b15e11675c112f77aa68e5b\" width=\"480\"><br><br>\n","- <font color=\"blue\">Multi-Head Attention</font><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://cvml-expertguide.net/terms/dl/seq2seq-translation/transformer/multi-head-attention/)</font><br>\n","<img src=\"https://i0.wp.com/cvml-expertguide.net/wp-content/uploads/2022/06/ffd86506f0c4eadb72d31f9927d898f5-1.png?resize=768%2C457&ssl=1\" width=\"480\"><br>\n","$\\begin{eqnarray} \\rm{MultiHead Attention}(\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V}) &=& \\rm{Concat}(head_1, head_2, \\cdots, head_h)\\boldsymbol{W}_o\\\\ \\rm{where}\\ head_i &=& \\rm{ScaledDotProductAttention}(\\boldsymbol{QW}^Q_i, \\boldsymbol{KW}^K_i, \\boldsymbol{VW}^V_i) \\end{eqnarray}$<br><br>\n","ただし、$W_i^Q, W_i^K \\in \\mathbb{R}^{d_{model} \\times d_k}, W_i^V \\in \\mathbb{R}^{d_{model} \\times d_v}, W^O \\in \\mathbb{R}^{hd_v \\times d_{model}}$<br><br>\n","- <font color=\"blue\">Scaled Dot-Product Attention</font><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://cvml-expertguide.net/terms/dl/seq2seq-translation/transformer/)</font><br>\n","<img src=\"https://i0.wp.com/cvml-expertguide.net/wp-content/uploads/2022/01/ffd86506f0c4eadb72d31f9927d898f5-1.png?resize=768%2C434&ssl=1\" width=\"640\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://nlpillustration.tech/?p=2171)</font><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F209705%2F68cb794b-92b2-9c4a-55e6-b4696545efd7.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=8042404aad0e06e993a9568404500a92\" width=\"480\"><br>\n","$\\rm{ScaledDotProductAttention}(\\boldsymbol{Q}, \\boldsymbol{K}, \\boldsymbol{V}) = \\rm{softmax}\\left(\\cfrac{\\boldsymbol{QK}^T}{\\sqrt{d_k}}\\right)\\boldsymbol{V}$<br><br>\n","$\\boldsymbol{Q}$：デコーダー, 検索文, 計算するターゲット<br>\n","$\\boldsymbol{K}$：エンコーダー, 辞書のインデックス, 類似度の計算に使う単語ベクトルの集まり<br>\n","$\\boldsymbol{V}$：エンコーダー, 辞書の本文, 重み付け和計算に使うベクトルの集まり<br>\n","$\\sqrt{d_k}$：query, keyの単語分散表現の次元数で論文では512<br><br>\n","$\\sqrt{d_k}$で割る理由は、確率が低い部分の勾配情報を保持するためで、$\\sqrt{d_k}$ が大きくなると逆伝播時のソフトマックス関数の勾配が小さくなるため、 学習が円滑に進まなくなる。</font><br><br>\n","- <font color=\"blue\">Masked Attention<br></font>\n","Scaled Dot-Product Attentionのsoftmaxの前の入力に対応する部分を$-\\infty$に置き換えてマスクする。<br>\n","${\\rm softmax}(x_i)=\\frac{\\exp(x_i)}{\\sum_j \\exp (x_j)}$ なので、$x_i \\to -\\infty$なら、maskされる。<br><br>\n","class Attention(nn.Module):<br>\n","$\\qquad$def  __ init __ (self, d_model=300):<br>\n","$\\qquad$$\\qquad$super(). __ init __ ()<br>\n","$\\qquad$$\\qquad$self.q_linear = nn.Linear(d_model, d_model)<br>\n","$\\qquad$$\\qquad$self.k_linear = nn.Linear(d_model, d_model)<br>\n","$\\qquad$$\\qquad$self.v_linear = nn.Linear(d_model, d_model)<br>\n","$\\qquad$$\\qquad$self.out = nn.Linear(d_model, d_model)<br>\n","$\\qquad$$\\qquad$self.d_k = d_model<br>\n","$\\qquad$def forward(self, q, k, v, mask):<br>\n","$\\qquad$$\\qquad$q = self.q_linear(q)<br>\n","$\\qquad$$\\qquad$k = self.k_linear(k)<br>\n","$\\qquad$$\\qquad$v = self.v_linear(v)<br>\n","$\\qquad$$\\qquad$weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.d_k)<br>\n","$\\qquad$$\\qquad$mask = mask.unsqueeze(1)<br>\n","$\\qquad$$\\qquad$weights = weights.masked_fill(mask == 0, -1e9)<br>\n","$\\qquad$$\\qquad$normlized_weights = F.softmax(weights, dim=-1)<br>\n","$\\qquad$$\\qquad$output = torch.matmul(normlized_weights, v)<br>\n","$\\qquad$$\\qquad$output = self.out(output)<br>\n","$\\qquad$$\\qquad$return output, normlized_weights<br><br>\n","- <font color=\"blue\">Position-wise Feed-Forward Networks<br></font>\n","Position-wiseというのはただ単に、各単語ごとに独立してニューラルネットワークがあるということ(ただし、重みは共有)。 ニューラルネットワーク内では他単語との干渉はない。2層のニューラルネットワークになっている。<br><br>\n","$\\rm{FFN}(x) = \\max(0, xW_1+b_1)W_2+b_2$<br><br>\n","class FeedForward(nn.Module):<br>\n","$\\qquad$def  __ init __ (self, d_model, d_ff=1024, dropout=0.1):<br>\n","$\\qquad$$\\qquad$super(). __ init __ ()<br>\n","$\\qquad$$\\qquad$self.linear_1 = nn.Linear(d_model, d_ff)<br>\n","$\\qquad$$\\qquad$self.dropout = nn.Dropout(dropout)<br>\n","$\\qquad$$\\qquad$self.linear_2 = nn.Linear(d_ff, d_model)<br>\n","$\\qquad$def forward(self, x):<br>\n","$\\qquad$$\\qquad$x = self.linear_1(x)<br>\n","$\\qquad$$\\qquad$x = self.dropout(F.relu(x))<br>\n","$\\qquad$$\\qquad$x = self.linear_2(x)<br>\n","$\\qquad$$\\qquad$return x<br><br>\n","- <font color=\"blue\">Layer Normalization<br></font>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)</font><br><br>\n","<img src=\"https://lena-voita.github.io/resources/lectures/seq2seq/transformer/layer_norm-min.png\" width=\"320\"><br>\n","class TransformerBlock(nn.Module):<br>\n","$\\qquad$def  __ init __ (self, d_model, dropout=0.1):<br>\n","$\\qquad$$\\qquad$super(). __ init __ ()<br>\n","$\\qquad$$\\qquad$self.norm_1 = nn.LayerNorm(d_model)<br>\n","$\\qquad$$\\qquad$self.norm_2 = nn.LayerNorm(d_model)<br>\n","$\\qquad$$\\qquad$self.attn = Attention(d_model)<br>\n","$\\qquad$$\\qquad$self.ff = FeedForward(d_model)<br>\n","$\\qquad$$\\qquad$self.dropout_1 = nn.Dropout(dropout)<br>\n","$\\qquad$$\\qquad$self.dropout_2 = nn.Dropout(dropout)<br>\n","$\\qquad$def forward(self, x, mask):<br>\n","$\\qquad$$\\qquad$x_normlized = self.norm_1(x)<br>\n","$\\qquad$$\\qquad$output, normlized_weights = self.attn(x_normlized, x_normlized, x_normlized, mask)<br>\n","$\\qquad$$\\qquad$x2 = x + self.dropout_1(output)<br>\n","$\\qquad$$\\qquad$x_normlized2 = self.norm_2(x2)<br>\n","$\\qquad$$\\qquad$output = x2 + self.dropout_2(self.ff(x_normlized2))<br>\n","$\\qquad$$\\qquad$return output, normlized_weights<br><br>\n","- <font color=\"blue\">Why Self-Attention<br></font>\n","計算量が小さい<br>\n","並列計算可能<br>\n","広範囲の依存関係を学習可能<br>\n","高い解釈可能性<br><br>\n","- <font color=\"blue\">計算コスト比較<br></font>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://zenn.dev/attentionplease/articles/4e09c41d7a85db)</font><br>\n","Complexity per Layer：1層あたりの計算複雑性<br>\n","Sequential Operations：系列を処理する最小の操作回数<br>\n","Maximum Path Length：入力から出力の経路の長さ、長期依存性の学習しやすさ<br>\n","$n$：シーケンスの長さ<br>\n","$d$：モデルの次元<br>\n","<img src=\"https://camo.qiitausercontent.com/493038d5bbfdd7e5a27858e84f288bf99c398d85/68747470733a2f2f696d6775722e636f6d2f667478666255682e706e67\" width=\"640\"><br>\n","-  翻訳性能比較\n"," -  Transformerは高い翻訳精度を出しつつ、かつ計算コストを削減できている。<br><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fimgur.com%2FNzfVUU1.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=09d6a8ddcccf9bfb54845cea29493f2c\" width=\"640\"><br>\n"],"metadata":{"id":"sk74S4ebXsZj"}},{"cell_type":"markdown","source":["# <font color=\"silver\">言語│BERT [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4675&cid=b0f01606242a6ed3&CT=1668010639719&OR=ItemsView)</font><br>\n","> - <font color=\"silver\">Description</font><br>\n"," - BERT\n","   - <font color=\"Blue\">深い双方向型</font><br>\n","$\\tiny\\text{link（ELMo）}$ […</font>](https://data-analytics.fun/2020/07/13/understanding-elmo/)</font><br>\n","      - ELMoは前向きと後ろ向きを別で計算してアウトプットを結合するため、浅い双方向LSTM\n","      - Transformerのエンコーダーのみで構成された構造<br>\n","      - 深い双方向型で前向きだけでなく後ろ向きにも学習させる<br>\n","      - TransformerやGPTは前向き、また、ELMoは前向きと後ろ向きを別で計算してアウトプットを結合するため、浅い双方向LSTM<br>\n","<img src=\"https://data-analytics.fun/wp-content/uploads/2020/04/bert3.png\" width=\"640\"><br>\n","   - <font color=\"Blue\">トークン</font><br>\n","      - 入力／出力表現<br>\n","      - sentenceの先頭に[CLS]トークンを持たせる。<br>\n","      - 間に[SEP]トークンを入れ 1文目か2文目かを表す埋め込み表現を加算 する。<br>\n","      - トークン＋セグメント＋ポジションの和。<br>\n","       - トークン、単語の違いを表す情報\n","       - セグメント、文の違いを表す情報\n","       - ポジション、単語の順序を表す情報\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fimgur.com%2F65BPFqu.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=1b61f96cff68acdd08e93cb7f0ca2d77\" width=\"800\"><br>\n","   - <font color=\"Blue\">教師なし事前学習</font><br>\n","     -  教師なし事前学習により言語分散表現を獲得する\n","     -  Task1: MLM<br>\n","       - Masked Language Modeling, 単語マスク問題, 多クラス分類, 局所的な特徴学習\n","       - 入力系列のうち、隠された単語がなにかを予測<br>\n","       - 通常言語モデルの学習には、過去の単語列から次の単語を予測する事前タスクが用いられますが、BERT はAttention 機構を用いて文章を一度にまとめて読み込む構造であるため、未来の単語のカンニングが発生しないようにするために単語マスク問題が用いる\n","       - 入力の15%のトークンを[Mask]トークンでマスクし、元のトークンを当てる。<br>選んだ15%のうち\n","          - 80%は[MASK]トークンに置き換える\n","          - 10%はランダムな単語に置き換える\n","          - 10%は置き換えない\n","     -  Task2: NSP<br>\n","       -  Next Sentence Prediction,  隣接文問題, ２クラス分類, 大域的な特徴学習\n","       -  2つの入力文が隣り合うものかどうかを判別<br>\n","       -  2文選んでそれらが連続した文かどうかを当てる<br>\n","   - <font color=\"Blue\">教師ありファインチューニング</font><br>\n","     -  教師ありファインチューニングによって特定のタスクへ適応させる\n","     -  最後に出力層を追加するだけで利用できる汎用性が高いモデル<br><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fimgur.com%2F1ol4NHO.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=e51ac8d5c804ad835a5d5c50a13eb5dc\" width=\"640\"><br>\n","   - <font color=\"Blue\">実験</font><br>\n","     - GLUE<br>\n","       - GLUEベンチマークとは8つの自然言語理解タスクを1つにまとめたもの。最終スコアは8つの平均をとる。<br>\n","  <img src=\"https://deepsquare.jp/wp-content/uploads/2020/09/GLUE.jpg\" width=\"640\"><br>\n","     - SQuAD v1.1<br>\n","       - 質問文と答えを含む文章が渡され、答えがどこにあるかを予測するタスク。<br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fimgur.com%2Fmln9RwP.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=e3cb1d304f93f44f615617f3bed82058\" width=\"280\">\n","     - SQuAD v2.0<br>\n","       - 質問文と答えを含む文章が渡され、答えがどこにあるかを予測するタスクに「答えが存在しない」という選択肢を加えたもの。<br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fimgur.com%2FSzApj96.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=7f8d8f886f755cfc8d51bfa880b3ecfb\" width=\"280\"><br>\n","     - SWAG<br>\n","       - 与えられた文に続く文としてもっともらしいものを4つの選択肢から選ぶという常識的な推論を行うタスク。<br>\n","<img src=\"https://camo.qiitausercontent.com/16252a4c2643241b5809a2af9d2230139bc74341/68747470733a2f2f696d6775722e636f6d2f7a32417a505a652e706e67\" width=\"320\"><br><br>\n","   - <font color=\"Blue\">アブレーションスタディ（部分をみていくような実験）</font><br>\n","      -  事前学習<br>\n","        -  隣接文問題と単語マスク問題<br>\n","          -  No NSP: 隣接文問題がなく、単語マスク問題のみで事前学習<br>\n","          -  LTR & No NSP: 単語マスク問題ではなく、通常使われるLeft-to-Righの言語モデルでのみ事前学習<br>\n","        -  実験結果<br>\n","          -  隣接文問題がないとQNLI, MNLIおよびSQuADにてかなり悪化（BERTベースとNoNSPの比較）<br>\n","          -  単語マスク問題の両方向性がないと、MRPCおよびSQuADにてかなり悪化（NoNSPとLTR&NoNSPの比較）<br>\n","          -  BiLSTMによる両方向性があるとSQuADでスコア向上ができるが、GLUEでは伸びない（LTR&NoNSPとLTR&NoNSP+BiLSTMの比較）<br><br>\n","<img src=\"https://deepsquare.jp/wp-content/uploads/2020/09/compare_traing.png\" width=\"320\"><br><br>\n","      -  モデルサイズ<br>\n","        -  層の数 ${\\rm L}$、隠れ層のサイズ ${\\rm H}$、アテンションヘッドの数 ${\\rm A}$を変えながら制度を確認<br>\n","        -  実験結果<br>\n","          -  モデルが十分に事前に訓練されていれば、小さなデータセットしかないタスクでも大きな改善が得られることを実証<br>\n","          -  機械翻訳と比べて小さなタスクにおいても大きいモデルを使うと精度も上がる。<br>\n","          -  下流タスクが小さくてもファインチューニングすることで事前学習が大きいため高い精度 を出せる。<br><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fimgur.com%2FV27JsNL.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=0a98771cf79e921b7cb6997ff61ac326\" width=\"320\"><br><br>\n","      -  特徴ベース<br>\n","        -  ファインチューニングの代わりにBERTに特徴量ベースの手法を適用<br>\n","        -  実験結果<br>\n","          - BERTはファインチューニングおよび特徴量ベースいずれの手法でも効果を発揮する<br><br>\n","<img src=\"https://deepsquare.jp/wp-content/uploads/2020/09/feature_approch.png\" width=\"320\"><br><br>"],"metadata":{"id":"pLqMfnRBknu1"}},{"cell_type":"markdown","source":["# <font color=\"silver\">言語│GPT-n[<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4677&cid=b0f01606242a6ed3&CT=1666551710079&OR=ItemsView)</font><br>\n","\n"," |    | Bert | GPT |\n","| ---- | ---- | ---- |\n","|  Transformer  |  Transformerエンコーダ  |  Transformerデコーダ  |\n","|  Attention  |  Self-Attention  |  Masked-Attention  |\n","|  Pretraining  |  マスク単語予測、次節文かどうか予測  |  次の単語を予測  |\n","\n","- <font color=\"silver\">Description</font><br>\n"," - <font color=\"Blue\">GPT-1</font><br>\n","<font color=\"blue\">$\\tiny\\text{link}$ […</font>](https://data-analytics.fun/2020/04/18/understanding-openai-gpt/)</font><br>\n","    - Transformerデコーダ<br></font>\n","    - Pretraining, 次の単語を予測<br></font>\n","    - Fine-tuning, 言語モデルのファインチューニングと分類器のファインチューニング<br></font><br>\n"," - <font color=\"Blue\">GPT-2</font><br>\n","<font color=\"blue\">$\\tiny\\text{link}$ […</font>](https://data-analytics.fun/2020/11/10/understanding-openai-gpt2/)</font><br>\n","    - ファインチューニングをしないが、ファインチューニングをすることで精度を上げることも可能。\n","    - モデルの仕組みはGPT-1とほぼ同じで、特定のタスクに特化した教師あり学習は行わず、より大きな言語コーパスを使って、より大きなモデルの言語モデルを事前学習させることにより、zero-shot、もしくはfew-shotのセッティングでも精度が出るような汎用的なモデルを目指している。<br><br>\n"," - <font color=\"Blue\">GPT-3</font><br>\n","<font color=\"blue\">$\\tiny\\text{link}$ […</font>](https://data-analytics.fun/2020/12/07/openai-gpt3/)</font><br>\n","    - タスクによらないパフォーマンスを実現するために、ファインチューニングを行わない。\n","    - タスクの例をモデルに与えたときに、パラメーターは更新しない、つまりその例を使って学習はしない。<br>\n","    - これまでの違いは、より大きなモデルをより大きな言語データ(コーパス)を使って学習させていること。<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://data-analytics.fun/2020/12/07/openai-gpt3/)</font><br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/g/gri-blog/20210805/20210805120830.png \" width=\"640\"><br><br>\n","    - 計算量比較<br>\n","      - 毎秒 1 ペタ回のニューラルネットワーク演算を90日行うと90 petaflop/s-days\n","      - GPT-3 175Bの計算量は、毎秒1ペタ回のニューラルネット演算を少なくとも2年以上行った計算量以上である\n","      - GPT-3 6.7B の学習に要した計算量は,100 petaflop/s-days<br><br>\n","<img src=\"https://data-analytics.fun/wp-content/uploads/2020/12/image-88.png \" width=\"640\"><br><br>\n","    - アーキテクチャ<br>\n","      - パラメーター数は、1億2500万個 ～ 1750億個\n","      - GPT-3 のモデルサイズ(パラメータの桁数)が大きいほど,バッチサイズを大きく,かつ学習率を小さくする工夫がなされている<br><br>\n","<img src=\"https://data-analytics.fun/wp-content/uploads/2020/11/image-25-1536x483.png \" width=\"640\"><br><br>\n","    - N-Shot Learning<br></font>\n"," - GPT-2 や GPT-3 は、ファインチューニングを必要としない学習が可能（One-Shot Learning, Zero-Shot Learning, Few-Shot Learning）\n"," - 学習データを一つも示さずどんなタスクを解くかを示すことでモデルにタスクを解かせることをZero-Shot Learningという<br>\n"," - 規模を大きくすることで、新しいタスクのラベル付きデータを1つだけ与えて予測させるOne-Shot Learningのような、ファインチューニングを使用しない学習が可能<br><br>\n","<img src=\"https://data-analytics.fun/wp-content/uploads/2020/11/image-20-768x480.png \" width=\"200\"><img src=\"https://data-analytics.fun/wp-content/uploads/2020/11/image-23-768x380.png \" width=\"200\"><img src=\"https://data-analytics.fun/wp-content/uploads/2020/11/image-24-768x338.png\" width=\"200\"><br>\n"," - 翻訳したいペアを与えずに翻訳する言語をまたいだ翻訳をZero-Shot Learning\n","\n","|    | 一般 | GPT |\n","| ---- | ---- | ---- |\n","|  One-Shot   |  少量の教師ありデータに基づく学習方法  |  タスク説明と少量のデモンストレーションを入力とした予測  |\n","|  Zero-Shot   | 学習時に存在しないクラスのデータを扱う枠組み |  タスク説明のみを入力とした予測  |\n"],"metadata":{"id":"JxbObIQSmBBh"}},{"cell_type":"markdown","source":["# <font color=\"silver\">言語│WaveNet [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!4718&cid=b0f01606242a6ed3&CT=1671861417244&OR=ItemsView)</font><br>\n",">- 言語モデル</font><br>\n","  - PixelCNNをベース\n","  - 音声波形を生成する</font><br><br>\n","${p(\\mathbf{x}) = \\displaystyle \\prod_{t=1}^{T}p(x_{t}\\ |\\ x_{1}, \\cdots, x_{t-1})}$<br><br>\n","> - Dilated Causal Convolution</font>\n",">   - Causal Convolution</font>\n","   - 過去の時間ステップだけを用いて畳み込みを行う\n","   - 予測分布 $p(x_{t+1} | x_{1}, \\cdots, x_{t})$は将来の時間ステップ $x_{t+1}, x_{t+2}, \\cdots, x_{T}$には依存しない<br>\n","   - 将来のデータにマスクしているというmasked convolutionと等価<br>\n","   - RNNより学習が速い<br>\n","   - 受容体を広くするには、多くの層又は大きなフィルタが必要<br><br>\n","<img src=\"https://storage.googleapis.com/zenn-user-upload/44262da54bfa-20220804.png\" width=\"480\"><br>\n",">   - Dilated Causal Convolution</font>\n","   - 層が深くなるにつれて, 畳み込むノードを離す(Dilation)<br>\n","   - 1,2,4,8,⋯,512と指数的に大きくしている\n","   - InputからOutputにかけて, Dilation=1,2,4,8となっている<br><br>\n","<img src=\"https://storage.googleapis.com/zenn-user-upload/d30785693173-20220804.png\" width=\"480\"><br>\n","> - Softmax Distribution</font>\n","   - $μ$-law companding transformation</font><br>\n","   - 音声データは16-bit/tであるため、65,536個の確率の計算が必要で計算量が多い。<br>\n","   - 計算量削減のためμ-lowアルゴリズムを用いて256通りに量子化してSoftmaxする<br>\n","   - 生成する音声がどのクラスに属するかという分類問題として生成音声の予測を行う。\n","<br><br>\n","$f(x_{t}) = \\rm{sign}(x_{t}) \\displaystyle \\frac{\\log \\{1 + \\mu |x_{t}|\\}}{\\log \\{1 + \\mu \\}}$\n","<br><br>\n","$-1 < x_{t} < 1, \\mu = 255$<br><br>\n","> - Gated Activation Units<br></font>\n","  - ゲートと出力候補には別の重みを用いる<br><br>\n","<font color=\"black\">$\\mathbf{z} = \\tanh (W_{f, k} * \\mathbf{x}) \\odot \\sigma (W_{g, k} * \\mathbf{x})$<br><br>\n","$*$：<font color=\"silver\">畳み込みの演算</font><br>\n","$k$：<font color=\"silver\">layer index</font><br>\n","$f$：<font color=\"silver\">filter</font><br>\n","$g$：<font color=\"silver\">gate</font><br>\n","$σ$：<font color=\"silver\">sigmoid関数</font>\n","<br><br>\n","<img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.amazonaws.com%2F0%2F218720%2F081f61bb-6bc2-f891-36b0-26f3b4df6e16.png?ixlib=rb-4.0.0&auto=format&gif-q=60&q=75&w=1400&fit=max&s=b093519e6fa1320dca152e8beaff1ad7\" width=\"480\"><br>\n","> - ResNet</font><br>\n","  - 1x1 Convolutionsでチャンネル数を調節する\n","> - Conditional</font>\n","  - 自己回帰モデルに生成された音声の特徴を特定することを目的とするインプット$\\mathbf{h}$を加える。<br>\n","  - 例えば, 複数の話し手の音声が含まれる音声データセットについてインプット$\\mathbf{h}$として話し手の特徴をモデルに加えることによって、その複数の話し手の中から特定の話し手を選択することができる<br><br>\n","<font color=\"black\">$p(\\mathbf{x}\\ |\\ \\mathbf{h}) = \\displaystyle \\prod_{t=1}^{T}p(x_{t}\\ | \\ x_{1}, \\cdots, x_{t-1}, \\mathbf{h})$<br><br>\n","   - Global conditioning<br></font>\n","発話者ＩＤのような共通の特徴をすべてのノードに入力する<br><br>\n",">$\\mathbf{z} = \\tanh (W_{f, k} * \\mathbf{x} + V_{f, k}^{T}\\mathbf{h}) \\odot \\sigma (W_{g, k} * \\mathbf{x} + V_{g, k}^{T}\\mathbf{h})$<br><br>\n","   - Local conditioning<br></font>\n","テキストから音声を生成するTTSモデルを作成する場合は、時系列言語情報のテキストなど、インプット$\\mathbf{h}$を畳み込みのノードに与える<br><br>\n","$\\mathbf{z} = \\tanh (W_{f, k} * \\mathbf{x} + V_{f, k}^{T} * f(\\mathbf{h})) \\odot \\sigma (W_{g, k} * \\mathbf{x} + V_{g, k}^{T} * f(\\mathbf{h}))$\n","\n"],"metadata":{"id":"vVx9gd7V3RfG"}},{"cell_type":"markdown","source":["# <font color=\"silver\">グラフ [<font color=\"silver\">…</font>](https://onedrive.live.com/edit.aspx?resid=B0F01606242A6ED3!6519&cid=b0f01606242a6ed3&CT=1671861765235&OR=ItemsView)</font><br>\n","\n"],"metadata":{"id":"T57O8DXwVYcm"}},{"cell_type":"markdown","source":["# <font color=\"silver\">グラフ│タスク</font><br>\n","><font color=\"silver\">リンク予測・ノード予測・グラフ予測<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://hackmd.io/0IwDJxeITPGLyq40EfvT1g)</font><br>\n","<img src=\"https://cpp-learning.com/wp-content/uploads/2020/04/amazon-graph.jpg\" width=\"160\">\n","<img src=\"https://cpp-learning.com/wp-content/uploads/2020/04/game-graph.jpg\" width=\"160\">\n","<img src=\"https://cpp-learning.com/wp-content/uploads/2020/04/Graph-Classification.jpg\" width=\"160\"><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://hackmd.io/0IwDJxeITPGLyq40EfvT1g)</font><br>\n","<img src=\"https://i.imgur.com/9xSduOk.pngg\" width=\"720\"><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://hackmd.io/@kkume/rkK3tmpHd#GCNg)</font><br>\n","<img src=\"https://i.imgur.com/19CXe2U.png\" width=\"720\">"],"metadata":{"id":"1Lj0NsjSPOip"}},{"cell_type":"markdown","source":["# <font color=\"silver\">グラフ│ラプラシアン行列</font><br>\n","> - 隣接行列</font><br>\n","  - 2つのノードが隣接しているかどうか（ノード間にエッジがあるか）を2値で表す行列\n","<br><br>\n","<font color=\"black\">$a_{i,j} = \\begin{cases}\n","    1 & ((i,j)\\in E) \\\\\n","    0 & (otherwise)\n","  \\end{cases}$\n","<br><br>\n","$a_{i,j}$：<font color=\"silver\">隣接行列の要素</font><br>\n","$E$：<font color=\"silver\">エッジ（edge）の集合</font><br><br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/d/daynap1204/20170425/20170425120416.png\" width=\"720\"><br><br>\n","- 次数行列</font><br>\n"," - ノードが含まれるエッジの数（結合次数）を対角成分に並べた行列<br><br>\n","$d_{i,j} = \\begin{cases}\n","    \\sum_{k}{a_{i,k}} & (i=j) \\\\\n","    0 & (otherwise)\n","  \\end{cases}$\n","<br><br>\n","$d_{i,j}$：<font color=\"silver\">次数行列の要素</font><br>\n","$a_{i,j}$：<font color=\"silver\">隣接行列の要素</font><br><br>\n","<img src=\"https://cdn-ak.f.st-hatena.com/images/fotolife/d/daynap1204/20170425/20170425120416.png\" width=\"720\"><br><br>\n","- ラプラシアン行列</font><br>\n"," - 次数行列から隣接行列を引いたもの<br><br>\n","$\\boldsymbol{L}=\\boldsymbol{D}-\\boldsymbol{A}$<br><br>\n","$\\boldsymbol{D}$：<font color=\"silver\">次数行列</font><br>\n","$\\boldsymbol{A}$：<font color=\"silver\">隣接行列</font><br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://hackmd.io/@kkume/rkK3tmpHd#GCN)</font><br>\n","<img src=\"https://i.imgur.com/iXTK0dT.png\" width=\"720\">\n","<br><br>\n","- 正規化ラプラシアン行列</font><br>\n"," - 次数行列から隣接行列を引いたもの<br><br>\n","$\\boldsymbol{\\mathcal{L}} = \\boldsymbol{D}^{- \\frac{1}{2}}\\boldsymbol{L}\\boldsymbol{D}^{- \\frac{1}{2}} = \\boldsymbol{I}-\\boldsymbol{D}^{- \\frac{1}{2}}\\boldsymbol{A}\\boldsymbol{D}^{- \\frac{1}{2}}$<br><br>\n","$\\boldsymbol{D}$：<font color=\"silver\">次数行列</font><br>\n","$\\boldsymbol{A}$：<font color=\"silver\">隣接行列</font>\n","<br><br>\n","- 正規化ラプラシアン行列の畳み込み</font><br>\n"," - ノードの次数で正規化された正規化<br><br>\n","$f(H^{(l)}, A) = \\sigma\\left( \\hat{D}^{-\\frac{1}{2}}\\hat{A}\\hat{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)}\\right)$\n","<br><br>\n","${\\bf H}^{(l+1)}=\\sigma({\\bf\\tilde D}^{-\\frac{1}{2}}{\\bf\\tilde A}{\\bf\\tilde D}^{-\\frac{1}{2}}{\\bf H}^{(l)}{\\bf W}^{(l)})$\n","<br><br>\n","$\\boldsymbol{D}$：<font color=\"silver\">次数行列</font><br>\n","$\\boldsymbol{A}$：<font color=\"silver\">隣接行列</font><br>\n","$\\boldsymbol{L}$：<font color=\"silver\">ラプラシアン行列</font>\n"],"metadata":{"id":"M9U4PehUUCxb"}},{"cell_type":"markdown","source":["# <font color=\"silver\">グラフ│グラフにおける畳み込み</font><br>\n","> <font color=\"red\">※ データをグラフフーリエ変換し、重みとの積を取ってから逆グラフフーリエ変換する<br>\n","※ 隣接したノードの特徴量の変動(変動大→高周波、変動小→低周波)<br><br></font>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://hackmd.io/@kkume/rkK3tmpHd#GCN)</font><br>\n","<img src=\"https://i.imgur.com/N6WloSI.png\" width=\"480\">\n","<br><br>\n","※ 隣接する画素値（０〜２５５）の変動(変動大→高周波、変動小→低周波)<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://hackmd.io/@kkume/rkK3tmpHd#GCN)</font><br>\n","<img src=\"https://i.imgur.com/CMYID1W.png\" width=\"640\"><br><br>\n"],"metadata":{"id":"_50krVWExO7I"}},{"cell_type":"markdown","source":["# <font color=\"silver\">グラフ│グラフにおけるフーリエ変換</font><br>\n","> - グラフフーリエ変換</font><br>\n"," - 正規化グラフラプラシアン行列の固有ベクトルを並べた行列の転置とデータの\n","積をとる<br><br>\n","$\\mathcal{F}_G[\\boldsymbol{x}] = \\boldsymbol{Q}^{\\top}\\boldsymbol{x}$\n","<br><br>\n","> - 逆グラフフーリエ変換</font><br>\n"," - 正規化グラフラプラシアン行列の固有ベクトルを並べた行列とデータの積をとる<br><br>\n","$\\mathcal{F}_G^{-1}[\\boldsymbol{x}] = \\boldsymbol{Q}\\boldsymbol{x}$\n","<br><br>\n","> - グラフフーリエ変換の逆グラフフーリエ変換</font><br>\n"," - <br>\n","$\\mathcal{F}_G^{-1}[\\mathcal{F}_G[\\boldsymbol{x}]] = \\boldsymbol{Q}\\boldsymbol{Q}^{\\rm{T}}\\boldsymbol{x} = \\boldsymbol{x}$\n","<br><br>\n","$\\boldsymbol{x}$ ：<font color=\"silver\">全ノードの特徴(各ノードの特徴が1要素)を並べた特徴ベクトル</font><br>\n","$\\boldsymbol{Q}$：<font color=\"silver\">正規化グラフラプラシアンの固有ベクトル行列</font><br><br>\n","> - 畳み込み定理</font><br>\n",">  - 2つの関数の畳み込みのフーリエ変換は、それらのフーリエ変換の各要素ごとの積である\n","<br><br>\n","$\\mathcal{F}_G[\\boldsymbol{x}_1*\\boldsymbol{x}_2] =\\mathcal{F}_G[\\boldsymbol{x}_1]\\odot \\mathcal{F}_G[\\boldsymbol{x}_2]$\n","<br><br>\n","$\\boldsymbol{x}_1*\\boldsymbol{x}_2=\\mathcal{F}_G^{-1}[\\mathcal{F}_G[\\boldsymbol{x}_1] \\odot \\mathcal{F}_G[\\boldsymbol{x}_2]]$<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://hackmd.io/@kkume/rkK3tmpHd#GCN)</font><br>\n","<img src=\"https://i.imgur.com/fqMSsCl.png\" width=\"480\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://hackmd.io/@kkume/rkK3tmpHd#GCN)</font><br>\n","<img src=\"https://i.imgur.com/qXUx9qW.gif\" width=\"480\">"],"metadata":{"id":"cVUvhqBboOQL"}},{"cell_type":"markdown","source":["# <font color=\"silver\">グラフ│Spectral Conv</font><br>\n","> - Description<br><font color=\"red\">\n"," -  Spectral Conv<br>\n","   - 正規化グラフラプラシアン行列の固有ベクトルとノード特徴量を使うグラフ畳み込み<br>\n","   - データをグラフフーリエ変換し、重みとの積を取ってから、逆グラフフーリエ変換する\n","   - 固有ベクトルは、ノードのまとまり具合を表わす<br><br></font>\n"," -  畳み込み定理</font><br><br>\n","$\\mathcal{F}_G[\\boldsymbol{x}] = \\boldsymbol{Q}^{\\top}\\boldsymbol{x}$<br><br>\n","$\\mathcal{F}_G^{-1}[\\boldsymbol{x}] = \\boldsymbol{Q}\\boldsymbol{x}$<br><br>\n","$\\mathcal{F}_G^{-1}[\\mathcal{F}_G[\\boldsymbol{x}]] = \\boldsymbol{Q}\\boldsymbol{Q}^{\\rm{T}}\\boldsymbol{x} = \\boldsymbol{x}$<br><br>\n","$\\mathcal{F}_G[\\boldsymbol{x}_1*\\boldsymbol{x}_2] =\\mathcal{F}_G[\\boldsymbol{x}_1]\\odot \\mathcal{F}_G[\\boldsymbol{x}_2]$<br><br>\n","$\\boldsymbol{x}_1*\\boldsymbol{x}_2=\\mathcal{F}_G^{-1}[\\mathcal{F}_G[\\boldsymbol{x}_1] \\odot \\mathcal{F}_G[\\boldsymbol{x}_2]]$<br><br>\n"," -  導出</font><br><br>\n",">$\\begin{align}\n","\\boldsymbol{x}*_G\\boldsymbol{g}_\\boldsymbol{θ}&=\\mathcal{F}_G^{-1}[\\mathcal{F}_G[\\boldsymbol{x}] \\odot \\mathcal{F}_G[\\boldsymbol{g}_\\boldsymbol{θ}]]\\\\\n","&=\\boldsymbol{Q}(\\boldsymbol{Q}^\\top\\boldsymbol{x}\\odot\\boldsymbol{Q}^\\top\\boldsymbol{g}_\\boldsymbol{θ})\\\\\n","&=\\boldsymbol{Q}(\\boldsymbol{Q}^\\top\\boldsymbol{g}_\\boldsymbol{θ}\\odot\\boldsymbol{Q}^\\top\\boldsymbol{x})\\\\\n","&=\\boldsymbol{Q}\\boldsymbol{Q}^\\top\\boldsymbol{g}_\\boldsymbol{θ}\\odot\\boldsymbol{Q}^\\top\\boldsymbol{x}\n","\\end{align}$\n","<br><br>\n","$\\theta=\\boldsymbol{Q}^\\top\\boldsymbol{g}_\\boldsymbol{θ}$<font color=\"silver\">をパラメータと考えて</font>\n","<br><br>\n","$\\boldsymbol{\\Theta}=\\mathrm{diag}(\\boldsymbol{Q}^\\top\\boldsymbol{g}_\\boldsymbol{θ})$<font color=\"silver\">をパラメータを対角に並べた行列とおくと、<br><br></font>\n","$\\begin{align}\n","\\boldsymbol{x}*_G\\boldsymbol{g}_\\boldsymbol{\\Theta}&=\\boldsymbol{Q}\\boldsymbol{Θ}\\boldsymbol{Q}^\\top\\boldsymbol{x}\\\\\n","&=(逆グラフフーリエ変換)(パラメータ)(グラフフーリエ変換)\n","\\end{align}$\n","<br><br>\n","$\\boldsymbol{x}$ ：<font color=\"silver\">全ノードの特徴(各ノードの特徴が1要素)を並べた特徴ベクトル</font><br>\n","$\\boldsymbol{g}_\\boldsymbol{θ}$ ：<font color=\"silver\">フィルタ</font><br>\n","$*_G$ ：<font color=\"silver\">グラフ畳み込み</font><br>\n","$\\boldsymbol{Q}$ ：<font color=\"silver\">正規化グラフラプラシアンの固有ベクトル行列</font><br><br>\n","   - グラフにおける畳み込みを多層かつマルチチャンネルに拡張<br><br>\n","$\\displaystyle\\boldsymbol{x}_j^{l+1} = \\rho \\Biggl( \\boldsymbol{Q} \\sum_{i=1}^{f_l}\\boldsymbol{\\Theta}_{i,j}^l\\boldsymbol{Q}^{\\rm{T}}\\boldsymbol{x}_i^l \\Biggr)= \\rho \\Biggl( \\sum_{i=1}^{f_l}\\boldsymbol{Q}\\boldsymbol{\\Theta}_{i,j}^l\\boldsymbol{Q}^{\\rm{T}}\\boldsymbol{x}_i^l \\Biggr),\n","~~~~~j=1,...,f_{l+1}$\n","<br><br>\n","$l$：<font color=\"silver\">層の番号(何番目か) </font><br>\n","$f_l$：<font color=\"silver\">$\\,l\\,$番目の層におけるフィルタの数</font><br>\n","$\\boldsymbol{x}_j^{l+1}$：<font color=\"silver\">$\\,l\\,$番目の層における $\\,j\\,$番目のチャンネルの特徴ベクトル</font><br>\n","$\\rho$：<font color=\"silver\">活性化関数</font><br><br>\n"," -  <font color=\"red\">Spectral Convの課題<br>\n","   - 計算量が膨大<br>\n","       - 固有値・固有ベクトルを求める固有値分解の計算量は行列の次元数（1行あたりの要素の個数）の 3 乗に比例し、最低でも<font color=\"blue\">$\\,O(N^2)\\,$</font>の計算量がかかる。\n","   - 形状が異なるグラフデータに対する学習・推論が NG<br>\n","       - 学習されるパラメータ$\\,Θ\\,$が$\\,Q\\,$に依存する形になっているため、異なる構造のグラフ間で同じパラメータを利用できない。<br>\n","       - 重み行列(フィルタ)は「正規化グラフラプラシアン行列の固有ベクトル」に合わせて学習\n","       - グラフの形状が変わると、正規化グラフラプラシアン行列も変化する。特にノード数が変わると固有ベクトルの次元数も変わる。<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://logmi.jp/tech/articles/321767)</font><br>\n","<img src=\"https://img.logmi.jp/article_images/NSYEB4vyNACq5wXybfFyfB.jpg\" width=\"720\">\n"],"metadata":{"id":"zPZfb-yTOkmg"}},{"cell_type":"markdown","source":["# <font color=\"silver\">グラフ│ChebNet</font><br>\n","> - ChebNet</font><br>\n","  - $K$次のChebyshev多項式近似により行列の固有値分解を回避する<br>\n","  - $\\boldsymbol{\\Theta} $をChebyshev多項式で近似することで固有値分解の計算を省略する。<br>\n","  - Spectral Convolution の計算量を$O(N^2)$から$O(N)$に削減する。$N$はノード数。<br>\n","<br>\n","$\\begin{align}\n","\\boldsymbol{x}*_G\\boldsymbol{g}_\\boldsymbol{\\Theta}&=\\boldsymbol{Q}\\boldsymbol{Θ}\\boldsymbol{Q}^\\top\\boldsymbol{x}\\\\\n","&=(逆グラフフーリエ変換)(パラメータ)(グラフフーリエ変換)\n","\\end{align}$\n","<br><br>\n","$\\boldsymbol{x}$ ：<font color=\"silver\">全ノードの特徴(各ノードの特徴が1要素)を並べた特徴ベクトル</font><br>\n","$\\boldsymbol{g}_\\boldsymbol{θ}$ ：<font color=\"silver\">フィルタ</font><br>\n","$*_G$ ：<font color=\"silver\">グラフ畳み込み</font><br>\n","$\\boldsymbol{Q}$ ：<font color=\"silver\">正規化グラフラプラシアンの固有ベクトル行列</font>\n","<br><br>\n","$\\displaystyle\\boldsymbol{\\Theta} = \\sum_{k=0}^{K}\\theta_k \\mathcal{T}_k(\\boldsymbol{\\tilde{\\Lambda}})$<br><br>\n","$\\displaystyle\\boldsymbol{\\tilde{\\Lambda}} = \\frac{2}{\\lambda_{max}}\\boldsymbol{\\Lambda} - \\boldsymbol{I}$\n","<br><br>\n","$\\mathcal{T}_k(x) = 2x\\mathcal{T}_{k-1}(x)-\\mathcal{T}_{k-2},~~~~\\mathcal{T}_0(x)=1,~~~~\\mathcal{T}_1(x)=x$\n","<br><br>\n","$\\displaystyle\\boldsymbol{x}*_G\\boldsymbol{g}_\\theta = \\boldsymbol{Q}\\Biggl(\\sum_{k=0}^{K}\\theta_k \\mathcal{T}_k(\\boldsymbol{\\tilde{\\Lambda}})\\Biggr)\\boldsymbol{Q}^{\\rm{T}}\\boldsymbol{x}=\\sum_{k=0}^{K}\\theta_k \\mathcal{T}_k(\\boldsymbol{\\tilde{\\mathcal{L}}})\\boldsymbol{x},~~~~\\boldsymbol{\\tilde{\\mathcal{L}}} = \\frac{2}{\\lambda_{max}}\\boldsymbol{\\mathcal{L}} - \\boldsymbol{I}$\n","<br><br>\n","$\\mathcal{T}_k(x)$ ：<font color=\"silver\">チェビシェフ多項式、漸化式で定義</font><br>\n","$k$ ：<font color=\"silver\">チェビシェフ多項式の次数、固有値をk個取るのに対応しているため、フィルターサイズと対応している</font><br>\n","$\\boldsymbol{\\tilde{\\Lambda}}$ ：<font color=\"silver\">固有値が$[−1,1]$に収まるるようにスケーリング</font><br>\n","$\\boldsymbol{x}$ ：<font color=\"silver\">全ノードの特徴(各ノードの特徴が1要素)を並べた特徴ベクトル</font><br>\n","$\\boldsymbol{g}$ ：<font color=\"silver\">関数</font><br>\n","$*_G$ ：<font color=\"silver\">グラフ畳み込み</font><br>\n","$\\boldsymbol{Q}$ ：<font color=\"silver\">正規化グラフラプラシアン行列の固有ベクトルを並べた行列</font><br>\n","$\\Theta$ ：<font color=\"silver\">パラメータを対角に並べた行列</font><br>\n","<img src=\"https://img.logmi.jp/article_images/HRu4MfHqdnQ1as97FXNDVz.jpg\" width=\"720\"><br>\n","\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://logmi.jp/tech/articles/321767)</font><br>"],"metadata":{"id":"poiW-zrj55pa"}},{"cell_type":"markdown","source":["# <font color=\"silver\">グラフ│GCN</font><br>\n","><font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://logmi.jp/tech/articles/321767)</font><br>\n","<img src=\"https://img.logmi.jp/article_images/LWBgEsG4jWXRSfcxL21D9E.jpg\" width=\"720\">"],"metadata":{"id":"kdFqJf_w8R_o"}},{"cell_type":"markdown","source":["# <font color=\"silver\">グラフ│Spatial Conv\n",">$\\displaystyle\\boldsymbol{h}_i^{l+1} = \\sigma \\Biggl( \\sum_{j \\in \\mathcal{N_i}} \\frac{1}{c_{ij}}\\boldsymbol{h}_j^l \\boldsymbol{W}^l \\Biggr)$<br><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://didi-universe.tistory.com/entry/GNN-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-2-Spatial-Convolutional-Network)</font><br>\n","<img src=\"https://velog.velcdn.com/images/whattsup_kim/post/6f9dce7a-2429-4fe4-8b9a-9c5eeb411c1d/image.png\" width=\"640\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://medium.com/programming-soda/graph-convolution%E3%82%92%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86%E3%81%AB%E5%BF%9C%E7%94%A8%E3%81%99%E3%82%8B-part1-b792d53c4c18)</font><br>\n","<img src=\"https://miro.medium.com/max/828/1*pHknauNv0i4CKhzs0CvckQ.webp\" width=\"720\"><br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://logmi.jp/tech/articles/321767)</font><br>\n","<img src=\"https://img.logmi.jp/article_images/2CXrwAxG6aD7RbSzHNwBQA.jpg\" width=\"720\"><br>\n"],"metadata":{"id":"GvCtLkoDmD45"}},{"cell_type":"markdown","source":["# <font color=\"silver\">グラフ│Spatial Convの派生\n","<img src=\"https://img-blog.csdnimg.cn/2020030519524986.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hvdENob2M=,size_16,color_FFFFFF,t_70\" width=\"800\"><br>"],"metadata":{"id":"-Tu2kEfMS4AB"}},{"cell_type":"markdown","source":["# <font color=\"silver\">グラフ│appendix, 1901.00596</font>\n","https://arxiv.org/pdf/1901.00596.pdf<br>\n","<font color=\"silver\">$\\tiny\\text{画像引用元}$[<font color=\"silver\">…</font>](https://blog.acolyer.org/2019/02/06/a-comprehensive-survey-on-graph-neural-networks/)</font><br>\n","<img src=\"https://blog.acolyer.org/wp-content/uploads/2019/01/Graph-Network-Survey-Fig-1.jpeg?w=480\" width=\"320\"><br>"],"metadata":{"id":"7wDvb57AL-ur"}},{"cell_type":"markdown","source":["# <font color=\"silver\">グラフ│appendix, スライド資料</font>\n","[<font color=\"black\">DL_appendix_for_E_ver_6_0](https://drive.google.com/drive/folders/1rS57y54mUYU-OSmCIPLDk6xsxAUT_Jb-)<br>\n","speakerdeck：はじめてのGraph Convolutional Network<br>\n","https://speakerdeck.com/yoichi7yamakawa/hazimetefalsegraph-convolutional-network<br>\n","speakerdeck：グラフ分析ナイト - グラフデータ分析 入門編<br>\n","https://speakerdeck.com/openjny/gurahufen-xi-naito-gurahudetafen-xi-ru-men-bian<br>\n","speakerdeck：Graph Neural Networksを完全に理解したい<br>\n","https://speakerdeck.com/shimacos/graph-neural-networkswowan-quan-nili-jie-sitai<br>\n","グラフニューラルネットワーク入門<br>\n","https://www.slideshare.net/ryosuke-kojima/ss-179423718<br>\n","英語：Graph Convolutional Neural Networks<br>\n","https://slideplayer.com/slide/17520741/<br>\n","英語：Graph Neural Networks: Models and Applications\n","https://docs.google.com/presentation/d/1rvm6Yq6-Ss4UmxLDIPTReJJkAdcXdhFb/edit#slide=id.p1<br>"],"metadata":{"id":"c7Xn6FNNDBTa"}}]}